{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "import codecs\n",
    "import argparse\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# very useful feature used to reload python modules\n",
    "from importlib import reload\n",
    "\n",
    "# import module that loads data, tokenises the tuples, initialises the embeddings matrix\n",
    "import crim_data\n",
    "\n",
    "import multiprojection_model\n",
    "# contains code to evaluate according to semeval2018 metrics\n",
    "import semeval_eval\n",
    "import crim_evaluator\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise embeddings and normalise to unit-norm\n",
    "#model = KeyedVectors.load_word2vec_format('embeddings/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "#model = KeyedVectors.load_word2vec_format('embeddings/glove.42B.300d.txt', binary=False)\n",
    "model = KeyedVectors.load_word2vec_format('embeddings/wiki-news-300d-1M.vec', binary=False)\n",
    "\n",
    "#model.save_word2vec_format('embeddings/GoogleNews-vectors-negative300.txt', binary=False)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tuples in entire set: 7377\n",
      "Unique hyponyms in set: 2247\n"
     ]
    }
   ],
   "source": [
    "# import Ustalov et al. prepare module\n",
    "import prepare\n",
    "\n",
    "sub_train = prepare.read_subsumptions('subsumptions-train.txt')\n",
    "sub_test = prepare.read_subsumptions('subsumptions-test.txt')\n",
    "sub_validation = prepare.read_subsumptions('subsumptions-validation.txt')\n",
    "synonyms = prepare.read_synonyms('synonyms.txt')  \n",
    "\n",
    "# remove vocab term having no vector in embeddings\n",
    "def get_terms_having_vectors(w2v, dataset):\n",
    "    return [(q,h) for q, h in dataset if q in w2v and h in w2v]\n",
    "\n",
    "sub_train = get_terms_having_vectors(model, sub_train)\n",
    "sub_test = get_terms_having_vectors(model, sub_test)\n",
    "sub_validation = get_terms_having_vectors(model, sub_validation)\n",
    "synonyms = prepare.get_synonymys_having_vectors(synonyms, model)\n",
    "synonyms.default_factory = None\n",
    "\n",
    "# create hypernym dictionary\n",
    "hyper_dict = defaultdict(list)\n",
    "for x, y in sub_train + sub_test + sub_validation:        \n",
    "    hyper_dict[x].append(y)\n",
    "    \n",
    "hyper_dict.default_factory = None\n",
    "\n",
    "print (\"Total number of tuples in entire set: %d\" % (len([x for (x,y) in sub_train + sub_test + sub_validation])))\n",
    "print (\"Unique hyponyms in set: %d\" % (len(set([x for (x,y) in sub_train + sub_test + sub_validation]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating limited vocabulary of 250000\n",
      "Dataset vocabulary size is 3232\n",
      "Truncated vocab length is 250000\n",
      "Vocab size is 250000 words\n",
      "Initialising negative sampler\n",
      "Tokenising all dataset tuples\n",
      "Creating embeddings matrix\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "args = {'w2v':model,\n",
    "        'train':sub_train, 'test':sub_test, 'validation':sub_validation, 'synonyms':synonyms, \n",
    "        'limited_vocab_n': 250000\n",
    "       }\n",
    "data = crim_data.CrimData(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tuples: 5901; test tuples: 1476\n",
      "Training tuples: 5901; test tuples: 1476\n",
      "Training tuples: 5902; test tuples: 1475\n",
      "Training tuples: 5902; test tuples: 1475\n",
      "Training tuples: 5902; test tuples: 1475\n"
     ]
    }
   ],
   "source": [
    "# convert full dataset to array\n",
    "all_data_tokens = np.asarray(data.all_data_token)\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(all_data_tokens[:,0])\n",
    "\n",
    "# split data into 5 different train-test folds\n",
    "train_data_split = []\n",
    "test_data_split = []\n",
    "for k in kf.split(all_data_tokens[:,0]):    \n",
    "    k_train_split = all_data_tokens[k[0]]\n",
    "    k_test_split = all_data_tokens[k[1]]\n",
    "    \n",
    "    train_data_split.append(k_train_split)\n",
    "    test_data_split.append(k_test_split)\n",
    "\n",
    "# output training-test split sizes    \n",
    "for tr, te in zip(train_data_split, test_data_split):\n",
    "    print (\"Training tuples: %d; test tuples: %d\" % (len(tr), len(te)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_1_fold(hyp_model, train_split, test_split):    \n",
    "    \n",
    "    # fit model\n",
    "    # the test split is only used to measure the test loss\n",
    "    hyp_model.fit(train_split, test_split)    \n",
    "    # this step should not be required since the model is dynamically linked to the evaluator\n",
    "    hyp_model.evaluator.set_model(hyp_model.model)\n",
    "    # generates predictions according to trained model\n",
    "    predictions = hyp_model.evaluator.predict(test_split)\n",
    "    # this converts the tokens back to words for evaluation\n",
    "    test_tuples = data.token_to_words(test_split)\n",
    "    # here we have a scorer that will mark our effort according to this particular test split\n",
    "    scorer = semeval_eval.HypernymEvaluation(test_tuples)\n",
    "    # get scores\n",
    "    score_names, all_scores = scorer.get_evaluation_scores(predictions)\n",
    "    # initialise scores (MRR, MAP, ...)\n",
    "    scores = {s:0.0 for s in score_names }\n",
    "    for k in range(len(score_names)):    \n",
    "        scores[score_names[k]] = float('%.5f' % (sum([score_list[k] for score_list in all_scores]) / len(all_scores)))    \n",
    "\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "# standard model parameters - we won't be changing these\n",
    "args['data'] = data\n",
    "args['epochs'] = 15\n",
    "args['batch_size']= 32\n",
    "args['synonym_sample_n']= 5\n",
    "args['phi_k'] = 1\n",
    "args['lambda_c'] = 0.\n",
    "args['negative_sample_n'] = 10\n",
    "\n",
    "# generate parameter combinations\n",
    "_clusters = [1, 5, 10]\n",
    "_lambda_c = [0, 0.1, 1]\n",
    "_neg_count = [1, 5, 10]\n",
    "\n",
    "parameters = [_clusters, _lambda_c, _neg_count]\n",
    "\n",
    "param_list = list(product(*parameters))\n",
    "\n",
    "# initialise hypernymy discovery model which we will reuse by resetting the model with new args\n",
    "hyp_model = multiprojection_model.MultiProjModel(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with following parameters: phi_k: 1; lambda_c: 0.00; neg_count: 1\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.00; epochs=15; negative_count=1; synonym_count=5\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch: 1; Training Loss: 0.67537; Test Loss: 0.65366\n",
      "Epoch: 2; Training Loss: 0.65613; Test Loss: 0.62789\n",
      "Epoch: 3; Training Loss: 0.63816; Test Loss: 0.60199\n",
      "Epoch: 4; Training Loss: 0.62051; Test Loss: 0.57895\n",
      "Epoch: 5; Training Loss: 0.60194; Test Loss: 0.55422\n",
      "Epoch: 6; Training Loss: 0.58548; Test Loss: 0.53395\n",
      "Epoch: 7; Training Loss: 0.56923; Test Loss: 0.51206\n",
      "Epoch: 8; Training Loss: 0.55311; Test Loss: 0.49182\n",
      "Epoch: 9; Training Loss: 0.53765; Test Loss: 0.47205\n",
      "Epoch: 10; Training Loss: 0.52256; Test Loss: 0.45305\n",
      "Epoch: 11; Training Loss: 0.50684; Test Loss: 0.43639\n",
      "Epoch: 12; Training Loss: 0.49405; Test Loss: 0.41891\n",
      "Epoch: 13; Training Loss: 0.48107; Test Loss: 0.40615\n",
      "Epoch: 14; Training Loss: 0.46626; Test Loss: 0.38739\n",
      "Epoch: 15; Training Loss: 0.45350; Test Loss: 0.37105\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.68395; Test Loss: 0.66604\n",
      "Epoch: 2; Training Loss: 0.66542; Test Loss: 0.64015\n",
      "Epoch: 3; Training Loss: 0.64729; Test Loss: 0.61524\n",
      "Epoch: 4; Training Loss: 0.62903; Test Loss: 0.59052\n",
      "Epoch: 5; Training Loss: 0.61154; Test Loss: 0.56736\n",
      "Epoch: 6; Training Loss: 0.59427; Test Loss: 0.54434\n",
      "Epoch: 7; Training Loss: 0.57757; Test Loss: 0.52280\n",
      "Epoch: 8; Training Loss: 0.56168; Test Loss: 0.50105\n",
      "Epoch: 9; Training Loss: 0.54674; Test Loss: 0.48062\n",
      "Epoch: 10; Training Loss: 0.52921; Test Loss: 0.46368\n",
      "Epoch: 11; Training Loss: 0.51456; Test Loss: 0.44496\n",
      "Epoch: 12; Training Loss: 0.50087; Test Loss: 0.42678\n",
      "Epoch: 13; Training Loss: 0.48668; Test Loss: 0.40988\n",
      "Epoch: 14; Training Loss: 0.47324; Test Loss: 0.39062\n",
      "Epoch: 15; Training Loss: 0.46055; Test Loss: 0.37822\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.67997; Test Loss: 0.66029\n",
      "Epoch: 2; Training Loss: 0.66115; Test Loss: 0.63541\n",
      "Epoch: 3; Training Loss: 0.64314; Test Loss: 0.61070\n",
      "Epoch: 4; Training Loss: 0.62485; Test Loss: 0.58616\n",
      "Epoch: 5; Training Loss: 0.60696; Test Loss: 0.56201\n",
      "Epoch: 6; Training Loss: 0.58959; Test Loss: 0.53919\n",
      "Epoch: 7; Training Loss: 0.57325; Test Loss: 0.51861\n",
      "Epoch: 8; Training Loss: 0.55682; Test Loss: 0.49793\n",
      "Epoch: 9; Training Loss: 0.54179; Test Loss: 0.47687\n",
      "Epoch: 10; Training Loss: 0.52689; Test Loss: 0.45912\n",
      "Epoch: 11; Training Loss: 0.51117; Test Loss: 0.44138\n",
      "Epoch: 12; Training Loss: 0.49772; Test Loss: 0.42682\n",
      "Epoch: 13; Training Loss: 0.48246; Test Loss: 0.41158\n",
      "Epoch: 14; Training Loss: 0.47003; Test Loss: 0.39351\n",
      "Epoch: 15; Training Loss: 0.45663; Test Loss: 0.37570\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.68115; Test Loss: 0.66223\n",
      "Epoch: 2; Training Loss: 0.66261; Test Loss: 0.63616\n",
      "Epoch: 3; Training Loss: 0.64427; Test Loss: 0.61105\n",
      "Epoch: 4; Training Loss: 0.62631; Test Loss: 0.58537\n",
      "Epoch: 5; Training Loss: 0.60914; Test Loss: 0.56387\n",
      "Epoch: 6; Training Loss: 0.59159; Test Loss: 0.54058\n",
      "Epoch: 7; Training Loss: 0.57518; Test Loss: 0.51915\n",
      "Epoch: 8; Training Loss: 0.55916; Test Loss: 0.49942\n",
      "Epoch: 9; Training Loss: 0.54271; Test Loss: 0.47886\n",
      "Epoch: 10; Training Loss: 0.52824; Test Loss: 0.45836\n",
      "Epoch: 11; Training Loss: 0.51281; Test Loss: 0.44026\n",
      "Epoch: 12; Training Loss: 0.49966; Test Loss: 0.42125\n",
      "Epoch: 13; Training Loss: 0.48448; Test Loss: 0.40779\n",
      "Epoch: 14; Training Loss: 0.46911; Test Loss: 0.38901\n",
      "Epoch: 15; Training Loss: 0.45846; Test Loss: 0.37407\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.68149; Test Loss: 0.66281\n",
      "Epoch: 2; Training Loss: 0.66278; Test Loss: 0.63646\n",
      "Epoch: 3; Training Loss: 0.64421; Test Loss: 0.61250\n",
      "Epoch: 4; Training Loss: 0.62682; Test Loss: 0.58687\n",
      "Epoch: 5; Training Loss: 0.60916; Test Loss: 0.56562\n",
      "Epoch: 6; Training Loss: 0.59207; Test Loss: 0.54207\n",
      "Epoch: 7; Training Loss: 0.57502; Test Loss: 0.51880\n",
      "Epoch: 8; Training Loss: 0.55918; Test Loss: 0.49890\n",
      "Epoch: 9; Training Loss: 0.54268; Test Loss: 0.47937\n",
      "Epoch: 10; Training Loss: 0.52846; Test Loss: 0.45825\n",
      "Epoch: 11; Training Loss: 0.51359; Test Loss: 0.44124\n",
      "Epoch: 12; Training Loss: 0.49924; Test Loss: 0.42446\n",
      "Epoch: 13; Training Loss: 0.48471; Test Loss: 0.40772\n",
      "Epoch: 14; Training Loss: 0.47059; Test Loss: 0.39179\n",
      "Epoch: 15; Training Loss: 0.45694; Test Loss: 0.37534\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 1; lambda_c: 0.00; neg_count: 5\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.65421; Test Loss: 0.60570\n",
      "Epoch: 2; Training Loss: 0.58391; Test Loss: 0.53463\n",
      "Epoch: 3; Training Loss: 0.52611; Test Loss: 0.47888\n",
      "Epoch: 4; Training Loss: 0.48094; Test Loss: 0.43597\n",
      "Epoch: 5; Training Loss: 0.44539; Test Loss: 0.40210\n",
      "Epoch: 6; Training Loss: 0.41650; Test Loss: 0.37639\n",
      "Epoch: 7; Training Loss: 0.39529; Test Loss: 0.35537\n",
      "Epoch: 8; Training Loss: 0.37859; Test Loss: 0.33939\n",
      "Epoch: 9; Training Loss: 0.36539; Test Loss: 0.32574\n",
      "Epoch: 10; Training Loss: 0.35601; Test Loss: 0.31413\n",
      "Epoch: 11; Training Loss: 0.34698; Test Loss: 0.30266\n",
      "Epoch: 12; Training Loss: 0.33874; Test Loss: 0.29149\n",
      "Epoch: 13; Training Loss: 0.32896; Test Loss: 0.28053\n",
      "Epoch: 14; Training Loss: 0.32032; Test Loss: 0.27095\n",
      "Epoch: 15; Training Loss: 0.31322; Test Loss: 0.26086\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.64617; Test Loss: 0.59262\n",
      "Epoch: 2; Training Loss: 0.57566; Test Loss: 0.52355\n",
      "Epoch: 3; Training Loss: 0.51978; Test Loss: 0.46896\n",
      "Epoch: 4; Training Loss: 0.47476; Test Loss: 0.42755\n",
      "Epoch: 5; Training Loss: 0.43952; Test Loss: 0.39546\n",
      "Epoch: 6; Training Loss: 0.41199; Test Loss: 0.36970\n",
      "Epoch: 7; Training Loss: 0.39092; Test Loss: 0.34990\n",
      "Epoch: 8; Training Loss: 0.37451; Test Loss: 0.33357\n",
      "Epoch: 9; Training Loss: 0.36224; Test Loss: 0.32098\n",
      "Epoch: 10; Training Loss: 0.35212; Test Loss: 0.30857\n",
      "Epoch: 11; Training Loss: 0.34324; Test Loss: 0.29807\n",
      "Epoch: 12; Training Loss: 0.33386; Test Loss: 0.28654\n",
      "Epoch: 13; Training Loss: 0.32573; Test Loss: 0.27637\n",
      "Epoch: 14; Training Loss: 0.31756; Test Loss: 0.26525\n",
      "Epoch: 15; Training Loss: 0.30928; Test Loss: 0.25503\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.64792; Test Loss: 0.59517\n",
      "Epoch: 2; Training Loss: 0.57753; Test Loss: 0.52592\n",
      "Epoch: 3; Training Loss: 0.52103; Test Loss: 0.47181\n",
      "Epoch: 4; Training Loss: 0.47564; Test Loss: 0.42958\n",
      "Epoch: 5; Training Loss: 0.44099; Test Loss: 0.39710\n",
      "Epoch: 6; Training Loss: 0.41303; Test Loss: 0.37159\n",
      "Epoch: 7; Training Loss: 0.39169; Test Loss: 0.35196\n",
      "Epoch: 8; Training Loss: 0.37491; Test Loss: 0.33626\n",
      "Epoch: 9; Training Loss: 0.36261; Test Loss: 0.32346\n",
      "Epoch: 10; Training Loss: 0.35281; Test Loss: 0.31131\n",
      "Epoch: 11; Training Loss: 0.34393; Test Loss: 0.30028\n",
      "Epoch: 12; Training Loss: 0.33481; Test Loss: 0.28952\n",
      "Epoch: 13; Training Loss: 0.32650; Test Loss: 0.27852\n",
      "Epoch: 14; Training Loss: 0.31850; Test Loss: 0.26955\n",
      "Epoch: 15; Training Loss: 0.30982; Test Loss: 0.25816\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.00; epochs=15; negative_count=5; synonym_count=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Training Loss: 0.65404; Test Loss: 0.60546\n",
      "Epoch: 2; Training Loss: 0.58332; Test Loss: 0.53480\n",
      "Epoch: 3; Training Loss: 0.52647; Test Loss: 0.47908\n",
      "Epoch: 4; Training Loss: 0.48100; Test Loss: 0.43532\n",
      "Epoch: 5; Training Loss: 0.44479; Test Loss: 0.40212\n",
      "Epoch: 6; Training Loss: 0.41746; Test Loss: 0.37580\n",
      "Epoch: 7; Training Loss: 0.39496; Test Loss: 0.35532\n",
      "Epoch: 8; Training Loss: 0.37822; Test Loss: 0.33854\n",
      "Epoch: 9; Training Loss: 0.36586; Test Loss: 0.32510\n",
      "Epoch: 10; Training Loss: 0.35650; Test Loss: 0.31268\n",
      "Epoch: 11; Training Loss: 0.34719; Test Loss: 0.30165\n",
      "Epoch: 12; Training Loss: 0.33797; Test Loss: 0.29031\n",
      "Epoch: 13; Training Loss: 0.32974; Test Loss: 0.27920\n",
      "Epoch: 14; Training Loss: 0.32129; Test Loss: 0.26875\n",
      "Epoch: 15; Training Loss: 0.31265; Test Loss: 0.25900\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.64901; Test Loss: 0.59729\n",
      "Epoch: 2; Training Loss: 0.57866; Test Loss: 0.52744\n",
      "Epoch: 3; Training Loss: 0.52179; Test Loss: 0.47283\n",
      "Epoch: 4; Training Loss: 0.47736; Test Loss: 0.43084\n",
      "Epoch: 5; Training Loss: 0.44227; Test Loss: 0.39810\n",
      "Epoch: 6; Training Loss: 0.41383; Test Loss: 0.37210\n",
      "Epoch: 7; Training Loss: 0.39182; Test Loss: 0.35181\n",
      "Epoch: 8; Training Loss: 0.37620; Test Loss: 0.33504\n",
      "Epoch: 9; Training Loss: 0.36370; Test Loss: 0.32315\n",
      "Epoch: 10; Training Loss: 0.35372; Test Loss: 0.31055\n",
      "Epoch: 11; Training Loss: 0.34483; Test Loss: 0.29803\n",
      "Epoch: 12; Training Loss: 0.33595; Test Loss: 0.28807\n",
      "Epoch: 13; Training Loss: 0.32655; Test Loss: 0.27752\n",
      "Epoch: 14; Training Loss: 0.31904; Test Loss: 0.26738\n",
      "Epoch: 15; Training Loss: 0.31062; Test Loss: 0.25667\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 1; lambda_c: 0.00; neg_count: 10\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.63993; Test Loss: 0.57713\n",
      "Epoch: 2; Training Loss: 0.55312; Test Loss: 0.49023\n",
      "Epoch: 3; Training Loss: 0.48158; Test Loss: 0.42073\n",
      "Epoch: 4; Training Loss: 0.42338; Test Loss: 0.36678\n",
      "Epoch: 5; Training Loss: 0.37668; Test Loss: 0.32457\n",
      "Epoch: 6; Training Loss: 0.33948; Test Loss: 0.29274\n",
      "Epoch: 7; Training Loss: 0.31002; Test Loss: 0.26848\n",
      "Epoch: 8; Training Loss: 0.28778; Test Loss: 0.24914\n",
      "Epoch: 9; Training Loss: 0.26920; Test Loss: 0.23434\n",
      "Epoch: 10; Training Loss: 0.25494; Test Loss: 0.22231\n",
      "Epoch: 11; Training Loss: 0.24435; Test Loss: 0.21240\n",
      "Epoch: 12; Training Loss: 0.23595; Test Loss: 0.20374\n",
      "Epoch: 13; Training Loss: 0.22918; Test Loss: 0.19668\n",
      "Epoch: 14; Training Loss: 0.22344; Test Loss: 0.19018\n",
      "Epoch: 15; Training Loss: 0.21838; Test Loss: 0.18343\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.63972; Test Loss: 0.57669\n",
      "Epoch: 2; Training Loss: 0.55306; Test Loss: 0.48984\n",
      "Epoch: 3; Training Loss: 0.48140; Test Loss: 0.42045\n",
      "Epoch: 4; Training Loss: 0.42323; Test Loss: 0.36616\n",
      "Epoch: 5; Training Loss: 0.37635; Test Loss: 0.32489\n",
      "Epoch: 6; Training Loss: 0.33919; Test Loss: 0.29309\n",
      "Epoch: 7; Training Loss: 0.31063; Test Loss: 0.26790\n",
      "Epoch: 8; Training Loss: 0.28744; Test Loss: 0.24967\n",
      "Epoch: 9; Training Loss: 0.26955; Test Loss: 0.23370\n",
      "Epoch: 10; Training Loss: 0.25474; Test Loss: 0.22189\n",
      "Epoch: 11; Training Loss: 0.24434; Test Loss: 0.21168\n",
      "Epoch: 12; Training Loss: 0.23549; Test Loss: 0.20341\n",
      "Epoch: 13; Training Loss: 0.22908; Test Loss: 0.19662\n",
      "Epoch: 14; Training Loss: 0.22360; Test Loss: 0.19036\n",
      "Epoch: 15; Training Loss: 0.21812; Test Loss: 0.18364\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.64087; Test Loss: 0.57895\n",
      "Epoch: 2; Training Loss: 0.55431; Test Loss: 0.49189\n",
      "Epoch: 3; Training Loss: 0.48275; Test Loss: 0.42197\n",
      "Epoch: 4; Training Loss: 0.42452; Test Loss: 0.36763\n",
      "Epoch: 5; Training Loss: 0.37705; Test Loss: 0.32524\n",
      "Epoch: 6; Training Loss: 0.34016; Test Loss: 0.29377\n",
      "Epoch: 7; Training Loss: 0.31160; Test Loss: 0.26929\n",
      "Epoch: 8; Training Loss: 0.28780; Test Loss: 0.25059\n",
      "Epoch: 9; Training Loss: 0.26982; Test Loss: 0.23499\n",
      "Epoch: 10; Training Loss: 0.25535; Test Loss: 0.22310\n",
      "Epoch: 11; Training Loss: 0.24448; Test Loss: 0.21333\n",
      "Epoch: 12; Training Loss: 0.23587; Test Loss: 0.20495\n",
      "Epoch: 13; Training Loss: 0.22907; Test Loss: 0.19860\n",
      "Epoch: 14; Training Loss: 0.22378; Test Loss: 0.19178\n",
      "Epoch: 15; Training Loss: 0.21853; Test Loss: 0.18541\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.64360; Test Loss: 0.58328\n",
      "Epoch: 2; Training Loss: 0.55680; Test Loss: 0.49537\n",
      "Epoch: 3; Training Loss: 0.48427; Test Loss: 0.42524\n",
      "Epoch: 4; Training Loss: 0.42571; Test Loss: 0.36983\n",
      "Epoch: 5; Training Loss: 0.37857; Test Loss: 0.32745\n",
      "Epoch: 6; Training Loss: 0.34129; Test Loss: 0.29483\n",
      "Epoch: 7; Training Loss: 0.31227; Test Loss: 0.27073\n",
      "Epoch: 8; Training Loss: 0.28883; Test Loss: 0.25112\n",
      "Epoch: 9; Training Loss: 0.27048; Test Loss: 0.23577\n",
      "Epoch: 10; Training Loss: 0.25671; Test Loss: 0.22320\n",
      "Epoch: 11; Training Loss: 0.24513; Test Loss: 0.21278\n",
      "Epoch: 12; Training Loss: 0.23631; Test Loss: 0.20418\n",
      "Epoch: 13; Training Loss: 0.22984; Test Loss: 0.19713\n",
      "Epoch: 14; Training Loss: 0.22452; Test Loss: 0.19026\n",
      "Epoch: 15; Training Loss: 0.21946; Test Loss: 0.18393\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.64192; Test Loss: 0.58023\n",
      "Epoch: 2; Training Loss: 0.55534; Test Loss: 0.49293\n",
      "Epoch: 3; Training Loss: 0.48330; Test Loss: 0.42257\n",
      "Epoch: 4; Training Loss: 0.42544; Test Loss: 0.36839\n",
      "Epoch: 5; Training Loss: 0.37702; Test Loss: 0.32633\n",
      "Epoch: 6; Training Loss: 0.34064; Test Loss: 0.29388\n",
      "Epoch: 7; Training Loss: 0.31114; Test Loss: 0.26930\n",
      "Epoch: 8; Training Loss: 0.28816; Test Loss: 0.25057\n",
      "Epoch: 9; Training Loss: 0.26991; Test Loss: 0.23516\n",
      "Epoch: 10; Training Loss: 0.25595; Test Loss: 0.22255\n",
      "Epoch: 11; Training Loss: 0.24495; Test Loss: 0.21215\n",
      "Epoch: 12; Training Loss: 0.23617; Test Loss: 0.20434\n",
      "Epoch: 13; Training Loss: 0.22960; Test Loss: 0.19727\n",
      "Epoch: 14; Training Loss: 0.22455; Test Loss: 0.19057\n",
      "Epoch: 15; Training Loss: 0.21895; Test Loss: 0.18380\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 1; lambda_c: 0.10; neg_count: 1\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.10; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.68426; Test Loss: 0.66780\n",
      "Epoch: 2; Training Loss: 0.66695; Test Loss: 0.64298\n",
      "Epoch: 3; Training Loss: 0.64914; Test Loss: 0.61955\n",
      "Epoch: 4; Training Loss: 0.63176; Test Loss: 0.59612\n",
      "Epoch: 5; Training Loss: 0.61564; Test Loss: 0.57428\n",
      "Epoch: 6; Training Loss: 0.59774; Test Loss: 0.55098\n",
      "Epoch: 7; Training Loss: 0.58237; Test Loss: 0.53011\n",
      "Epoch: 8; Training Loss: 0.56657; Test Loss: 0.50905\n",
      "Epoch: 9; Training Loss: 0.55060; Test Loss: 0.48939\n",
      "Epoch: 10; Training Loss: 0.53530; Test Loss: 0.47368\n",
      "Epoch: 11; Training Loss: 0.51915; Test Loss: 0.45599\n",
      "Epoch: 12; Training Loss: 0.50640; Test Loss: 0.43810\n",
      "Epoch: 13; Training Loss: 0.49347; Test Loss: 0.42233\n",
      "Epoch: 14; Training Loss: 0.47976; Test Loss: 0.40857\n",
      "Epoch: 15; Training Loss: 0.46635; Test Loss: 0.39068\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.10; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.68729; Test Loss: 0.67163\n",
      "Epoch: 2; Training Loss: 0.67008; Test Loss: 0.64668\n",
      "Epoch: 3; Training Loss: 0.65242; Test Loss: 0.62322\n",
      "Epoch: 4; Training Loss: 0.63492; Test Loss: 0.59937\n",
      "Epoch: 5; Training Loss: 0.61722; Test Loss: 0.57607\n",
      "Epoch: 6; Training Loss: 0.60152; Test Loss: 0.55621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7; Training Loss: 0.58430; Test Loss: 0.53290\n",
      "Epoch: 8; Training Loss: 0.56884; Test Loss: 0.51452\n",
      "Epoch: 9; Training Loss: 0.55305; Test Loss: 0.49436\n",
      "Epoch: 10; Training Loss: 0.53759; Test Loss: 0.47168\n",
      "Epoch: 11; Training Loss: 0.52281; Test Loss: 0.45773\n",
      "Epoch: 12; Training Loss: 0.50986; Test Loss: 0.43941\n",
      "Epoch: 13; Training Loss: 0.49546; Test Loss: 0.42296\n",
      "Epoch: 14; Training Loss: 0.48163; Test Loss: 0.40783\n",
      "Epoch: 15; Training Loss: 0.46788; Test Loss: 0.39082\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.10; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.68124; Test Loss: 0.66202\n",
      "Epoch: 2; Training Loss: 0.66342; Test Loss: 0.63776\n",
      "Epoch: 3; Training Loss: 0.64585; Test Loss: 0.61429\n",
      "Epoch: 4; Training Loss: 0.62887; Test Loss: 0.59081\n",
      "Epoch: 5; Training Loss: 0.61157; Test Loss: 0.57023\n",
      "Epoch: 6; Training Loss: 0.59432; Test Loss: 0.54852\n",
      "Epoch: 7; Training Loss: 0.57762; Test Loss: 0.52802\n",
      "Epoch: 8; Training Loss: 0.56227; Test Loss: 0.50792\n",
      "Epoch: 9; Training Loss: 0.54723; Test Loss: 0.48503\n",
      "Epoch: 10; Training Loss: 0.53208; Test Loss: 0.46890\n",
      "Epoch: 11; Training Loss: 0.51629; Test Loss: 0.44770\n",
      "Epoch: 12; Training Loss: 0.50340; Test Loss: 0.43485\n",
      "Epoch: 13; Training Loss: 0.48989; Test Loss: 0.41841\n",
      "Epoch: 14; Training Loss: 0.47586; Test Loss: 0.40475\n",
      "Epoch: 15; Training Loss: 0.46401; Test Loss: 0.38780\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.10; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.68192; Test Loss: 0.66402\n",
      "Epoch: 2; Training Loss: 0.66436; Test Loss: 0.63954\n",
      "Epoch: 3; Training Loss: 0.64716; Test Loss: 0.61426\n",
      "Epoch: 4; Training Loss: 0.62982; Test Loss: 0.59181\n",
      "Epoch: 5; Training Loss: 0.61225; Test Loss: 0.56942\n",
      "Epoch: 6; Training Loss: 0.59627; Test Loss: 0.54655\n",
      "Epoch: 7; Training Loss: 0.58036; Test Loss: 0.52745\n",
      "Epoch: 8; Training Loss: 0.56362; Test Loss: 0.50663\n",
      "Epoch: 9; Training Loss: 0.54762; Test Loss: 0.48496\n",
      "Epoch: 10; Training Loss: 0.53433; Test Loss: 0.46778\n",
      "Epoch: 11; Training Loss: 0.51871; Test Loss: 0.45042\n",
      "Epoch: 12; Training Loss: 0.50405; Test Loss: 0.43082\n",
      "Epoch: 13; Training Loss: 0.48971; Test Loss: 0.41363\n",
      "Epoch: 14; Training Loss: 0.47612; Test Loss: 0.39987\n",
      "Epoch: 15; Training Loss: 0.46452; Test Loss: 0.38301\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.10; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.68267; Test Loss: 0.66453\n",
      "Epoch: 2; Training Loss: 0.66484; Test Loss: 0.63998\n",
      "Epoch: 3; Training Loss: 0.64757; Test Loss: 0.61668\n",
      "Epoch: 4; Training Loss: 0.63003; Test Loss: 0.59171\n",
      "Epoch: 5; Training Loss: 0.61316; Test Loss: 0.56901\n",
      "Epoch: 6; Training Loss: 0.59621; Test Loss: 0.54804\n",
      "Epoch: 7; Training Loss: 0.57979; Test Loss: 0.52633\n",
      "Epoch: 8; Training Loss: 0.56330; Test Loss: 0.50586\n",
      "Epoch: 9; Training Loss: 0.54825; Test Loss: 0.48975\n",
      "Epoch: 10; Training Loss: 0.53308; Test Loss: 0.46836\n",
      "Epoch: 11; Training Loss: 0.51835; Test Loss: 0.44989\n",
      "Epoch: 12; Training Loss: 0.50420; Test Loss: 0.43456\n",
      "Epoch: 13; Training Loss: 0.49094; Test Loss: 0.41865\n",
      "Epoch: 14; Training Loss: 0.47753; Test Loss: 0.39909\n",
      "Epoch: 15; Training Loss: 0.46469; Test Loss: 0.38782\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 1; lambda_c: 0.10; neg_count: 5\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.10; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.65117; Test Loss: 0.59864\n",
      "Epoch: 2; Training Loss: 0.58160; Test Loss: 0.52844\n",
      "Epoch: 3; Training Loss: 0.52488; Test Loss: 0.47404\n",
      "Epoch: 4; Training Loss: 0.47950; Test Loss: 0.43090\n",
      "Epoch: 5; Training Loss: 0.44338; Test Loss: 0.39984\n",
      "Epoch: 6; Training Loss: 0.41649; Test Loss: 0.37548\n",
      "Epoch: 7; Training Loss: 0.39558; Test Loss: 0.35664\n",
      "Epoch: 8; Training Loss: 0.38030; Test Loss: 0.34320\n",
      "Epoch: 9; Training Loss: 0.36917; Test Loss: 0.33270\n",
      "Epoch: 10; Training Loss: 0.35972; Test Loss: 0.32141\n",
      "Epoch: 11; Training Loss: 0.35040; Test Loss: 0.31041\n",
      "Epoch: 12; Training Loss: 0.34226; Test Loss: 0.29836\n",
      "Epoch: 13; Training Loss: 0.33341; Test Loss: 0.28979\n",
      "Epoch: 14; Training Loss: 0.32537; Test Loss: 0.27905\n",
      "Epoch: 15; Training Loss: 0.31660; Test Loss: 0.27019\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.10; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.65251; Test Loss: 0.60153\n",
      "Epoch: 2; Training Loss: 0.58288; Test Loss: 0.53087\n",
      "Epoch: 3; Training Loss: 0.52618; Test Loss: 0.47601\n",
      "Epoch: 4; Training Loss: 0.48027; Test Loss: 0.43251\n",
      "Epoch: 5; Training Loss: 0.44496; Test Loss: 0.40086\n",
      "Epoch: 6; Training Loss: 0.41788; Test Loss: 0.37610\n",
      "Epoch: 7; Training Loss: 0.39693; Test Loss: 0.35765\n",
      "Epoch: 8; Training Loss: 0.38096; Test Loss: 0.34310\n",
      "Epoch: 9; Training Loss: 0.36921; Test Loss: 0.33246\n",
      "Epoch: 10; Training Loss: 0.36082; Test Loss: 0.32133\n",
      "Epoch: 11; Training Loss: 0.35125; Test Loss: 0.31113\n",
      "Epoch: 12; Training Loss: 0.34288; Test Loss: 0.29976\n",
      "Epoch: 13; Training Loss: 0.33405; Test Loss: 0.28911\n",
      "Epoch: 14; Training Loss: 0.32563; Test Loss: 0.27928\n",
      "Epoch: 15; Training Loss: 0.31791; Test Loss: 0.26960\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.10; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.65732; Test Loss: 0.60935\n",
      "Epoch: 2; Training Loss: 0.58786; Test Loss: 0.53791\n",
      "Epoch: 3; Training Loss: 0.53003; Test Loss: 0.48109\n",
      "Epoch: 4; Training Loss: 0.48420; Test Loss: 0.43791\n",
      "Epoch: 5; Training Loss: 0.44826; Test Loss: 0.40492\n",
      "Epoch: 6; Training Loss: 0.41986; Test Loss: 0.38023\n",
      "Epoch: 7; Training Loss: 0.39928; Test Loss: 0.36175\n",
      "Epoch: 8; Training Loss: 0.38346; Test Loss: 0.34839\n",
      "Epoch: 9; Training Loss: 0.37193; Test Loss: 0.33693\n",
      "Epoch: 10; Training Loss: 0.36241; Test Loss: 0.32496\n",
      "Epoch: 11; Training Loss: 0.35357; Test Loss: 0.31414\n",
      "Epoch: 12; Training Loss: 0.34501; Test Loss: 0.30407\n",
      "Epoch: 13; Training Loss: 0.33649; Test Loss: 0.29343\n",
      "Epoch: 14; Training Loss: 0.32810; Test Loss: 0.28436\n",
      "Epoch: 15; Training Loss: 0.31990; Test Loss: 0.27397\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.10; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.65237; Test Loss: 0.60120\n",
      "Epoch: 2; Training Loss: 0.58261; Test Loss: 0.53129\n",
      "Epoch: 3; Training Loss: 0.52611; Test Loss: 0.47484\n",
      "Epoch: 4; Training Loss: 0.48062; Test Loss: 0.43257\n",
      "Epoch: 5; Training Loss: 0.44498; Test Loss: 0.40013\n",
      "Epoch: 6; Training Loss: 0.41771; Test Loss: 0.37605\n",
      "Epoch: 7; Training Loss: 0.39607; Test Loss: 0.35721\n",
      "Epoch: 8; Training Loss: 0.38078; Test Loss: 0.34367\n",
      "Epoch: 9; Training Loss: 0.36964; Test Loss: 0.33229\n",
      "Epoch: 10; Training Loss: 0.36023; Test Loss: 0.32058\n",
      "Epoch: 11; Training Loss: 0.35098; Test Loss: 0.30962\n",
      "Epoch: 12; Training Loss: 0.34250; Test Loss: 0.29926\n",
      "Epoch: 13; Training Loss: 0.33391; Test Loss: 0.28930\n",
      "Epoch: 14; Training Loss: 0.32636; Test Loss: 0.27914\n",
      "Epoch: 15; Training Loss: 0.31777; Test Loss: 0.26874\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.10; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.65563; Test Loss: 0.60657\n",
      "Epoch: 2; Training Loss: 0.58600; Test Loss: 0.53528\n",
      "Epoch: 3; Training Loss: 0.52867; Test Loss: 0.47940\n",
      "Epoch: 4; Training Loss: 0.48243; Test Loss: 0.43557\n",
      "Epoch: 5; Training Loss: 0.44732; Test Loss: 0.40309\n",
      "Epoch: 6; Training Loss: 0.41955; Test Loss: 0.37811\n",
      "Epoch: 7; Training Loss: 0.39899; Test Loss: 0.35999\n",
      "Epoch: 8; Training Loss: 0.38276; Test Loss: 0.34601\n",
      "Epoch: 9; Training Loss: 0.37100; Test Loss: 0.33496\n",
      "Epoch: 10; Training Loss: 0.36163; Test Loss: 0.32349\n",
      "Epoch: 11; Training Loss: 0.35323; Test Loss: 0.31252\n",
      "Epoch: 12; Training Loss: 0.34377; Test Loss: 0.30163\n",
      "Epoch: 13; Training Loss: 0.33644; Test Loss: 0.29082\n",
      "Epoch: 14; Training Loss: 0.32759; Test Loss: 0.28087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15; Training Loss: 0.31937; Test Loss: 0.27079\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 1; lambda_c: 0.10; neg_count: 10\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.10; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.64787; Test Loss: 0.58874\n",
      "Epoch: 2; Training Loss: 0.56214; Test Loss: 0.50053\n",
      "Epoch: 3; Training Loss: 0.49013; Test Loss: 0.42965\n",
      "Epoch: 4; Training Loss: 0.43039; Test Loss: 0.37358\n",
      "Epoch: 5; Training Loss: 0.38276; Test Loss: 0.33004\n",
      "Epoch: 6; Training Loss: 0.34469; Test Loss: 0.29697\n",
      "Epoch: 7; Training Loss: 0.31489; Test Loss: 0.27225\n",
      "Epoch: 8; Training Loss: 0.29207; Test Loss: 0.25381\n",
      "Epoch: 9; Training Loss: 0.27386; Test Loss: 0.23979\n",
      "Epoch: 10; Training Loss: 0.25939; Test Loss: 0.22875\n",
      "Epoch: 11; Training Loss: 0.24928; Test Loss: 0.22105\n",
      "Epoch: 12; Training Loss: 0.24165; Test Loss: 0.21495\n",
      "Epoch: 13; Training Loss: 0.23627; Test Loss: 0.20834\n",
      "Epoch: 14; Training Loss: 0.23049; Test Loss: 0.20189\n",
      "Epoch: 15; Training Loss: 0.22557; Test Loss: 0.19614\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.10; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.64712; Test Loss: 0.58812\n",
      "Epoch: 2; Training Loss: 0.56133; Test Loss: 0.50010\n",
      "Epoch: 3; Training Loss: 0.48941; Test Loss: 0.42885\n",
      "Epoch: 4; Training Loss: 0.43007; Test Loss: 0.37286\n",
      "Epoch: 5; Training Loss: 0.38232; Test Loss: 0.32977\n",
      "Epoch: 6; Training Loss: 0.34465; Test Loss: 0.29659\n",
      "Epoch: 7; Training Loss: 0.31444; Test Loss: 0.27179\n",
      "Epoch: 8; Training Loss: 0.29130; Test Loss: 0.25304\n",
      "Epoch: 9; Training Loss: 0.27384; Test Loss: 0.23896\n",
      "Epoch: 10; Training Loss: 0.25996; Test Loss: 0.22847\n",
      "Epoch: 11; Training Loss: 0.24944; Test Loss: 0.22044\n",
      "Epoch: 12; Training Loss: 0.24206; Test Loss: 0.21420\n",
      "Epoch: 13; Training Loss: 0.23570; Test Loss: 0.20808\n",
      "Epoch: 14; Training Loss: 0.23057; Test Loss: 0.20137\n",
      "Epoch: 15; Training Loss: 0.22572; Test Loss: 0.19516\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.10; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.64862; Test Loss: 0.59028\n",
      "Epoch: 2; Training Loss: 0.56316; Test Loss: 0.50229\n",
      "Epoch: 3; Training Loss: 0.49082; Test Loss: 0.43091\n",
      "Epoch: 4; Training Loss: 0.43121; Test Loss: 0.37429\n",
      "Epoch: 5; Training Loss: 0.38370; Test Loss: 0.33139\n",
      "Epoch: 6; Training Loss: 0.34476; Test Loss: 0.29794\n",
      "Epoch: 7; Training Loss: 0.31523; Test Loss: 0.27245\n",
      "Epoch: 8; Training Loss: 0.29189; Test Loss: 0.25394\n",
      "Epoch: 9; Training Loss: 0.27387; Test Loss: 0.24037\n",
      "Epoch: 10; Training Loss: 0.26015; Test Loss: 0.22970\n",
      "Epoch: 11; Training Loss: 0.25030; Test Loss: 0.22160\n",
      "Epoch: 12; Training Loss: 0.24198; Test Loss: 0.21535\n",
      "Epoch: 13; Training Loss: 0.23631; Test Loss: 0.20913\n",
      "Epoch: 14; Training Loss: 0.23083; Test Loss: 0.20291\n",
      "Epoch: 15; Training Loss: 0.22584; Test Loss: 0.19617\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.10; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.64013; Test Loss: 0.57566\n",
      "Epoch: 2; Training Loss: 0.55442; Test Loss: 0.49013\n",
      "Epoch: 3; Training Loss: 0.48355; Test Loss: 0.42061\n",
      "Epoch: 4; Training Loss: 0.42492; Test Loss: 0.36624\n",
      "Epoch: 5; Training Loss: 0.37849; Test Loss: 0.32423\n",
      "Epoch: 6; Training Loss: 0.34122; Test Loss: 0.29213\n",
      "Epoch: 7; Training Loss: 0.31186; Test Loss: 0.26813\n",
      "Epoch: 8; Training Loss: 0.28927; Test Loss: 0.25007\n",
      "Epoch: 9; Training Loss: 0.27112; Test Loss: 0.23582\n",
      "Epoch: 10; Training Loss: 0.25763; Test Loss: 0.22591\n",
      "Epoch: 11; Training Loss: 0.24736; Test Loss: 0.21778\n",
      "Epoch: 12; Training Loss: 0.23962; Test Loss: 0.21083\n",
      "Epoch: 13; Training Loss: 0.23366; Test Loss: 0.20488\n",
      "Epoch: 14; Training Loss: 0.22844; Test Loss: 0.19838\n",
      "Epoch: 15; Training Loss: 0.22360; Test Loss: 0.19198\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=0.10; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.64547; Test Loss: 0.58483\n",
      "Epoch: 2; Training Loss: 0.55963; Test Loss: 0.49765\n",
      "Epoch: 3; Training Loss: 0.48771; Test Loss: 0.42690\n",
      "Epoch: 4; Training Loss: 0.42889; Test Loss: 0.37157\n",
      "Epoch: 5; Training Loss: 0.38111; Test Loss: 0.32796\n",
      "Epoch: 6; Training Loss: 0.34330; Test Loss: 0.29556\n",
      "Epoch: 7; Training Loss: 0.31403; Test Loss: 0.27094\n",
      "Epoch: 8; Training Loss: 0.29081; Test Loss: 0.25249\n",
      "Epoch: 9; Training Loss: 0.27310; Test Loss: 0.23839\n",
      "Epoch: 10; Training Loss: 0.25949; Test Loss: 0.22817\n",
      "Epoch: 11; Training Loss: 0.24918; Test Loss: 0.21967\n",
      "Epoch: 12; Training Loss: 0.24103; Test Loss: 0.21364\n",
      "Epoch: 13; Training Loss: 0.23556; Test Loss: 0.20692\n",
      "Epoch: 14; Training Loss: 0.23011; Test Loss: 0.20011\n",
      "Epoch: 15; Training Loss: 0.22512; Test Loss: 0.19403\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 1; lambda_c: 1.00; neg_count: 1\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=1.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.69200; Test Loss: 0.67802\n",
      "Epoch: 2; Training Loss: 0.67624; Test Loss: 0.65728\n",
      "Epoch: 3; Training Loss: 0.66137; Test Loss: 0.63701\n",
      "Epoch: 4; Training Loss: 0.64598; Test Loss: 0.61511\n",
      "Epoch: 5; Training Loss: 0.63128; Test Loss: 0.59553\n",
      "Epoch: 6; Training Loss: 0.61586; Test Loss: 0.57604\n",
      "Epoch: 7; Training Loss: 0.60091; Test Loss: 0.55622\n",
      "Epoch: 8; Training Loss: 0.58679; Test Loss: 0.53522\n",
      "Epoch: 9; Training Loss: 0.57289; Test Loss: 0.52194\n",
      "Epoch: 10; Training Loss: 0.55931; Test Loss: 0.50196\n",
      "Epoch: 11; Training Loss: 0.54366; Test Loss: 0.48600\n",
      "Epoch: 12; Training Loss: 0.53047; Test Loss: 0.47257\n",
      "Epoch: 13; Training Loss: 0.51820; Test Loss: 0.45391\n",
      "Epoch: 14; Training Loss: 0.50671; Test Loss: 0.44694\n",
      "Epoch: 15; Training Loss: 0.49364; Test Loss: 0.42932\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=1.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.69465; Test Loss: 0.68525\n",
      "Epoch: 2; Training Loss: 0.68121; Test Loss: 0.66352\n",
      "Epoch: 3; Training Loss: 0.66603; Test Loss: 0.64229\n",
      "Epoch: 4; Training Loss: 0.65066; Test Loss: 0.62143\n",
      "Epoch: 5; Training Loss: 0.63535; Test Loss: 0.60089\n",
      "Epoch: 6; Training Loss: 0.62028; Test Loss: 0.58025\n",
      "Epoch: 7; Training Loss: 0.60518; Test Loss: 0.56034\n",
      "Epoch: 8; Training Loss: 0.59133; Test Loss: 0.54107\n",
      "Epoch: 9; Training Loss: 0.57701; Test Loss: 0.52390\n",
      "Epoch: 10; Training Loss: 0.56129; Test Loss: 0.50430\n",
      "Epoch: 11; Training Loss: 0.55030; Test Loss: 0.48907\n",
      "Epoch: 12; Training Loss: 0.53605; Test Loss: 0.47402\n",
      "Epoch: 13; Training Loss: 0.52318; Test Loss: 0.45936\n",
      "Epoch: 14; Training Loss: 0.51105; Test Loss: 0.44731\n",
      "Epoch: 15; Training Loss: 0.49888; Test Loss: 0.43003\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=1.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.69419; Test Loss: 0.68448\n",
      "Epoch: 2; Training Loss: 0.68126; Test Loss: 0.66303\n",
      "Epoch: 3; Training Loss: 0.66626; Test Loss: 0.64085\n",
      "Epoch: 4; Training Loss: 0.65103; Test Loss: 0.62000\n",
      "Epoch: 5; Training Loss: 0.63578; Test Loss: 0.59930\n",
      "Epoch: 6; Training Loss: 0.62153; Test Loss: 0.58079\n",
      "Epoch: 7; Training Loss: 0.60526; Test Loss: 0.55967\n",
      "Epoch: 8; Training Loss: 0.59178; Test Loss: 0.54012\n",
      "Epoch: 9; Training Loss: 0.57716; Test Loss: 0.52413\n",
      "Epoch: 10; Training Loss: 0.56262; Test Loss: 0.50677\n",
      "Epoch: 11; Training Loss: 0.54927; Test Loss: 0.48816\n",
      "Epoch: 12; Training Loss: 0.53421; Test Loss: 0.46990\n",
      "Epoch: 13; Training Loss: 0.52453; Test Loss: 0.45844\n",
      "Epoch: 14; Training Loss: 0.51012; Test Loss: 0.44316\n",
      "Epoch: 15; Training Loss: 0.49993; Test Loss: 0.43192\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=1.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.69081; Test Loss: 0.67652\n",
      "Epoch: 2; Training Loss: 0.67508; Test Loss: 0.65488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3; Training Loss: 0.65976; Test Loss: 0.63511\n",
      "Epoch: 4; Training Loss: 0.64460; Test Loss: 0.61379\n",
      "Epoch: 5; Training Loss: 0.63000; Test Loss: 0.59359\n",
      "Epoch: 6; Training Loss: 0.61523; Test Loss: 0.57400\n",
      "Epoch: 7; Training Loss: 0.59964; Test Loss: 0.55610\n",
      "Epoch: 8; Training Loss: 0.58540; Test Loss: 0.53496\n",
      "Epoch: 9; Training Loss: 0.56993; Test Loss: 0.51742\n",
      "Epoch: 10; Training Loss: 0.55782; Test Loss: 0.49903\n",
      "Epoch: 11; Training Loss: 0.54474; Test Loss: 0.48627\n",
      "Epoch: 12; Training Loss: 0.53134; Test Loss: 0.46878\n",
      "Epoch: 13; Training Loss: 0.51852; Test Loss: 0.45412\n",
      "Epoch: 14; Training Loss: 0.50822; Test Loss: 0.43880\n",
      "Epoch: 15; Training Loss: 0.49470; Test Loss: 0.42511\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=1.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.69204; Test Loss: 0.67819\n",
      "Epoch: 2; Training Loss: 0.67698; Test Loss: 0.65731\n",
      "Epoch: 3; Training Loss: 0.66190; Test Loss: 0.63649\n",
      "Epoch: 4; Training Loss: 0.64637; Test Loss: 0.61468\n",
      "Epoch: 5; Training Loss: 0.63137; Test Loss: 0.59412\n",
      "Epoch: 6; Training Loss: 0.61635; Test Loss: 0.57622\n",
      "Epoch: 7; Training Loss: 0.60255; Test Loss: 0.55633\n",
      "Epoch: 8; Training Loss: 0.58699; Test Loss: 0.53762\n",
      "Epoch: 9; Training Loss: 0.57367; Test Loss: 0.51789\n",
      "Epoch: 10; Training Loss: 0.55887; Test Loss: 0.50132\n",
      "Epoch: 11; Training Loss: 0.54489; Test Loss: 0.48458\n",
      "Epoch: 12; Training Loss: 0.53066; Test Loss: 0.46672\n",
      "Epoch: 13; Training Loss: 0.51829; Test Loss: 0.45228\n",
      "Epoch: 14; Training Loss: 0.50802; Test Loss: 0.43825\n",
      "Epoch: 15; Training Loss: 0.49459; Test Loss: 0.42361\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 1; lambda_c: 1.00; neg_count: 5\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=1.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.65653; Test Loss: 0.60830\n",
      "Epoch: 2; Training Loss: 0.58994; Test Loss: 0.53985\n",
      "Epoch: 3; Training Loss: 0.53489; Test Loss: 0.48437\n",
      "Epoch: 4; Training Loss: 0.48983; Test Loss: 0.44160\n",
      "Epoch: 5; Training Loss: 0.45440; Test Loss: 0.41007\n",
      "Epoch: 6; Training Loss: 0.42724; Test Loss: 0.38691\n",
      "Epoch: 7; Training Loss: 0.40651; Test Loss: 0.36978\n",
      "Epoch: 8; Training Loss: 0.39135; Test Loss: 0.35817\n",
      "Epoch: 9; Training Loss: 0.38155; Test Loss: 0.34929\n",
      "Epoch: 10; Training Loss: 0.37340; Test Loss: 0.33893\n",
      "Epoch: 11; Training Loss: 0.36557; Test Loss: 0.32924\n",
      "Epoch: 12; Training Loss: 0.35744; Test Loss: 0.32105\n",
      "Epoch: 13; Training Loss: 0.35023; Test Loss: 0.31197\n",
      "Epoch: 14; Training Loss: 0.34242; Test Loss: 0.30313\n",
      "Epoch: 15; Training Loss: 0.33515; Test Loss: 0.29467\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=1.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.65478; Test Loss: 0.60398\n",
      "Epoch: 2; Training Loss: 0.58777; Test Loss: 0.53693\n",
      "Epoch: 3; Training Loss: 0.53291; Test Loss: 0.48229\n",
      "Epoch: 4; Training Loss: 0.48837; Test Loss: 0.43945\n",
      "Epoch: 5; Training Loss: 0.45307; Test Loss: 0.40693\n",
      "Epoch: 6; Training Loss: 0.42566; Test Loss: 0.38371\n",
      "Epoch: 7; Training Loss: 0.40471; Test Loss: 0.36714\n",
      "Epoch: 8; Training Loss: 0.39070; Test Loss: 0.35536\n",
      "Epoch: 9; Training Loss: 0.37980; Test Loss: 0.34622\n",
      "Epoch: 10; Training Loss: 0.37239; Test Loss: 0.33660\n",
      "Epoch: 11; Training Loss: 0.36410; Test Loss: 0.32694\n",
      "Epoch: 12; Training Loss: 0.35619; Test Loss: 0.31764\n",
      "Epoch: 13; Training Loss: 0.34841; Test Loss: 0.30828\n",
      "Epoch: 14; Training Loss: 0.34077; Test Loss: 0.29907\n",
      "Epoch: 15; Training Loss: 0.33419; Test Loss: 0.29138\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=1.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.66167; Test Loss: 0.61654\n",
      "Epoch: 2; Training Loss: 0.59485; Test Loss: 0.54864\n",
      "Epoch: 3; Training Loss: 0.53969; Test Loss: 0.49280\n",
      "Epoch: 4; Training Loss: 0.49442; Test Loss: 0.44883\n",
      "Epoch: 5; Training Loss: 0.45815; Test Loss: 0.41454\n",
      "Epoch: 6; Training Loss: 0.43034; Test Loss: 0.39051\n",
      "Epoch: 7; Training Loss: 0.40944; Test Loss: 0.37292\n",
      "Epoch: 8; Training Loss: 0.39457; Test Loss: 0.36132\n",
      "Epoch: 9; Training Loss: 0.38490; Test Loss: 0.35163\n",
      "Epoch: 10; Training Loss: 0.37670; Test Loss: 0.34132\n",
      "Epoch: 11; Training Loss: 0.36852; Test Loss: 0.33141\n",
      "Epoch: 12; Training Loss: 0.36039; Test Loss: 0.32230\n",
      "Epoch: 13; Training Loss: 0.35324; Test Loss: 0.31323\n",
      "Epoch: 14; Training Loss: 0.34539; Test Loss: 0.30380\n",
      "Epoch: 15; Training Loss: 0.33794; Test Loss: 0.29567\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=1.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.66247; Test Loss: 0.61797\n",
      "Epoch: 2; Training Loss: 0.59557; Test Loss: 0.55043\n",
      "Epoch: 3; Training Loss: 0.54027; Test Loss: 0.49367\n",
      "Epoch: 4; Training Loss: 0.49505; Test Loss: 0.45016\n",
      "Epoch: 5; Training Loss: 0.45905; Test Loss: 0.41656\n",
      "Epoch: 6; Training Loss: 0.43097; Test Loss: 0.39303\n",
      "Epoch: 7; Training Loss: 0.41018; Test Loss: 0.37485\n",
      "Epoch: 8; Training Loss: 0.39538; Test Loss: 0.36312\n",
      "Epoch: 9; Training Loss: 0.38484; Test Loss: 0.35424\n",
      "Epoch: 10; Training Loss: 0.37695; Test Loss: 0.34444\n",
      "Epoch: 11; Training Loss: 0.36852; Test Loss: 0.33450\n",
      "Epoch: 12; Training Loss: 0.36071; Test Loss: 0.32336\n",
      "Epoch: 13; Training Loss: 0.35352; Test Loss: 0.31613\n",
      "Epoch: 14; Training Loss: 0.34533; Test Loss: 0.30548\n",
      "Epoch: 15; Training Loss: 0.33817; Test Loss: 0.29654\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=1.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.66625; Test Loss: 0.63066\n",
      "Epoch: 2; Training Loss: 0.60226; Test Loss: 0.56167\n",
      "Epoch: 3; Training Loss: 0.54634; Test Loss: 0.50357\n",
      "Epoch: 4; Training Loss: 0.50061; Test Loss: 0.45801\n",
      "Epoch: 5; Training Loss: 0.46419; Test Loss: 0.42406\n",
      "Epoch: 6; Training Loss: 0.43573; Test Loss: 0.39823\n",
      "Epoch: 7; Training Loss: 0.41448; Test Loss: 0.38018\n",
      "Epoch: 8; Training Loss: 0.39975; Test Loss: 0.36824\n",
      "Epoch: 9; Training Loss: 0.38910; Test Loss: 0.35838\n",
      "Epoch: 10; Training Loss: 0.38088; Test Loss: 0.34754\n",
      "Epoch: 11; Training Loss: 0.37258; Test Loss: 0.33747\n",
      "Epoch: 12; Training Loss: 0.36476; Test Loss: 0.32792\n",
      "Epoch: 13; Training Loss: 0.35686; Test Loss: 0.31780\n",
      "Epoch: 14; Training Loss: 0.34951; Test Loss: 0.30959\n",
      "Epoch: 15; Training Loss: 0.34158; Test Loss: 0.29970\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 1; lambda_c: 1.00; neg_count: 10\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=1.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.65228; Test Loss: 0.59609\n",
      "Epoch: 2; Training Loss: 0.57030; Test Loss: 0.51300\n",
      "Epoch: 3; Training Loss: 0.50127; Test Loss: 0.44323\n",
      "Epoch: 4; Training Loss: 0.44305; Test Loss: 0.38654\n",
      "Epoch: 5; Training Loss: 0.39523; Test Loss: 0.34246\n",
      "Epoch: 6; Training Loss: 0.35648; Test Loss: 0.30844\n",
      "Epoch: 7; Training Loss: 0.32580; Test Loss: 0.28272\n",
      "Epoch: 8; Training Loss: 0.30185; Test Loss: 0.26411\n",
      "Epoch: 9; Training Loss: 0.28315; Test Loss: 0.25018\n",
      "Epoch: 10; Training Loss: 0.26895; Test Loss: 0.24003\n",
      "Epoch: 11; Training Loss: 0.25917; Test Loss: 0.23358\n",
      "Epoch: 12; Training Loss: 0.25227; Test Loss: 0.22892\n",
      "Epoch: 13; Training Loss: 0.24719; Test Loss: 0.22354\n",
      "Epoch: 14; Training Loss: 0.24249; Test Loss: 0.21763\n",
      "Epoch: 15; Training Loss: 0.23815; Test Loss: 0.21271\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=1.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.65329; Test Loss: 0.59968\n",
      "Epoch: 2; Training Loss: 0.57208; Test Loss: 0.51632\n",
      "Epoch: 3; Training Loss: 0.50311; Test Loss: 0.44643\n",
      "Epoch: 4; Training Loss: 0.44454; Test Loss: 0.38940\n",
      "Epoch: 5; Training Loss: 0.39628; Test Loss: 0.34511\n",
      "Epoch: 6; Training Loss: 0.35747; Test Loss: 0.31087\n",
      "Epoch: 7; Training Loss: 0.32650; Test Loss: 0.28420\n",
      "Epoch: 8; Training Loss: 0.30251; Test Loss: 0.26489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9; Training Loss: 0.28409; Test Loss: 0.25030\n",
      "Epoch: 10; Training Loss: 0.27040; Test Loss: 0.24056\n",
      "Epoch: 11; Training Loss: 0.25985; Test Loss: 0.23396\n",
      "Epoch: 12; Training Loss: 0.25306; Test Loss: 0.22881\n",
      "Epoch: 13; Training Loss: 0.24814; Test Loss: 0.22359\n",
      "Epoch: 14; Training Loss: 0.24350; Test Loss: 0.21798\n",
      "Epoch: 15; Training Loss: 0.23884; Test Loss: 0.21229\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=1.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.65697; Test Loss: 0.60798\n",
      "Epoch: 2; Training Loss: 0.57681; Test Loss: 0.52413\n",
      "Epoch: 3; Training Loss: 0.50723; Test Loss: 0.45283\n",
      "Epoch: 4; Training Loss: 0.44845; Test Loss: 0.39517\n",
      "Epoch: 5; Training Loss: 0.39999; Test Loss: 0.35005\n",
      "Epoch: 6; Training Loss: 0.36069; Test Loss: 0.31432\n",
      "Epoch: 7; Training Loss: 0.32924; Test Loss: 0.28774\n",
      "Epoch: 8; Training Loss: 0.30435; Test Loss: 0.26798\n",
      "Epoch: 9; Training Loss: 0.28630; Test Loss: 0.25350\n",
      "Epoch: 10; Training Loss: 0.27200; Test Loss: 0.24256\n",
      "Epoch: 11; Training Loss: 0.26179; Test Loss: 0.23570\n",
      "Epoch: 12; Training Loss: 0.25502; Test Loss: 0.23079\n",
      "Epoch: 13; Training Loss: 0.24963; Test Loss: 0.22544\n",
      "Epoch: 14; Training Loss: 0.24467; Test Loss: 0.21931\n",
      "Epoch: 15; Training Loss: 0.24063; Test Loss: 0.21389\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=1.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.65676; Test Loss: 0.60676\n",
      "Epoch: 2; Training Loss: 0.57616; Test Loss: 0.52306\n",
      "Epoch: 3; Training Loss: 0.50660; Test Loss: 0.45251\n",
      "Epoch: 4; Training Loss: 0.44796; Test Loss: 0.39471\n",
      "Epoch: 5; Training Loss: 0.39919; Test Loss: 0.34869\n",
      "Epoch: 6; Training Loss: 0.35994; Test Loss: 0.31476\n",
      "Epoch: 7; Training Loss: 0.32817; Test Loss: 0.28690\n",
      "Epoch: 8; Training Loss: 0.30433; Test Loss: 0.26819\n",
      "Epoch: 9; Training Loss: 0.28580; Test Loss: 0.25339\n",
      "Epoch: 10; Training Loss: 0.27162; Test Loss: 0.24308\n",
      "Epoch: 11; Training Loss: 0.26151; Test Loss: 0.23620\n",
      "Epoch: 12; Training Loss: 0.25451; Test Loss: 0.23138\n",
      "Epoch: 13; Training Loss: 0.24948; Test Loss: 0.22605\n",
      "Epoch: 14; Training Loss: 0.24457; Test Loss: 0.22029\n",
      "Epoch: 15; Training Loss: 0.23992; Test Loss: 0.21497\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=1; lambda_c=1.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.65734; Test Loss: 0.60918\n",
      "Epoch: 2; Training Loss: 0.57727; Test Loss: 0.52550\n",
      "Epoch: 3; Training Loss: 0.50767; Test Loss: 0.45461\n",
      "Epoch: 4; Training Loss: 0.44907; Test Loss: 0.39644\n",
      "Epoch: 5; Training Loss: 0.40008; Test Loss: 0.35031\n",
      "Epoch: 6; Training Loss: 0.36074; Test Loss: 0.31553\n",
      "Epoch: 7; Training Loss: 0.32933; Test Loss: 0.28799\n",
      "Epoch: 8; Training Loss: 0.30497; Test Loss: 0.26876\n",
      "Epoch: 9; Training Loss: 0.28626; Test Loss: 0.25381\n",
      "Epoch: 10; Training Loss: 0.27180; Test Loss: 0.24328\n",
      "Epoch: 11; Training Loss: 0.26204; Test Loss: 0.23594\n",
      "Epoch: 12; Training Loss: 0.25508; Test Loss: 0.23142\n",
      "Epoch: 13; Training Loss: 0.25000; Test Loss: 0.22571\n",
      "Epoch: 14; Training Loss: 0.24533; Test Loss: 0.22033\n",
      "Epoch: 15; Training Loss: 0.24027; Test Loss: 0.21405\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 5; lambda_c: 0.00; neg_count: 1\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.63079; Test Loss: 0.54415\n",
      "Epoch: 2; Training Loss: 0.54809; Test Loss: 0.44578\n",
      "Epoch: 3; Training Loss: 0.47514; Test Loss: 0.36274\n",
      "Epoch: 4; Training Loss: 0.41213; Test Loss: 0.30174\n",
      "Epoch: 5; Training Loss: 0.35708; Test Loss: 0.25223\n",
      "Epoch: 6; Training Loss: 0.31346; Test Loss: 0.21342\n",
      "Epoch: 7; Training Loss: 0.27154; Test Loss: 0.18600\n",
      "Epoch: 8; Training Loss: 0.23723; Test Loss: 0.15881\n",
      "Epoch: 9; Training Loss: 0.20918; Test Loss: 0.14274\n",
      "Epoch: 10; Training Loss: 0.18583; Test Loss: 0.12176\n",
      "Epoch: 11; Training Loss: 0.16436; Test Loss: 0.10386\n",
      "Epoch: 12; Training Loss: 0.14810; Test Loss: 0.09528\n",
      "Epoch: 13; Training Loss: 0.13069; Test Loss: 0.09035\n",
      "Epoch: 14; Training Loss: 0.12086; Test Loss: 0.07810\n",
      "Epoch: 15; Training Loss: 0.10747; Test Loss: 0.07355\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.63928; Test Loss: 0.55421\n",
      "Epoch: 2; Training Loss: 0.55601; Test Loss: 0.45183\n",
      "Epoch: 3; Training Loss: 0.48175; Test Loss: 0.36949\n",
      "Epoch: 4; Training Loss: 0.41965; Test Loss: 0.30658\n",
      "Epoch: 5; Training Loss: 0.36559; Test Loss: 0.25046\n",
      "Epoch: 6; Training Loss: 0.31686; Test Loss: 0.20875\n",
      "Epoch: 7; Training Loss: 0.27699; Test Loss: 0.17969\n",
      "Epoch: 8; Training Loss: 0.24174; Test Loss: 0.15647\n",
      "Epoch: 9; Training Loss: 0.21588; Test Loss: 0.13708\n",
      "Epoch: 10; Training Loss: 0.19019; Test Loss: 0.11531\n",
      "Epoch: 11; Training Loss: 0.16649; Test Loss: 0.10448\n",
      "Epoch: 12; Training Loss: 0.15159; Test Loss: 0.09840\n",
      "Epoch: 13; Training Loss: 0.13231; Test Loss: 0.08959\n",
      "Epoch: 14; Training Loss: 0.11928; Test Loss: 0.08043\n",
      "Epoch: 15; Training Loss: 0.11113; Test Loss: 0.07182\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.62596; Test Loss: 0.53805\n",
      "Epoch: 2; Training Loss: 0.54262; Test Loss: 0.44406\n",
      "Epoch: 3; Training Loss: 0.47092; Test Loss: 0.36507\n",
      "Epoch: 4; Training Loss: 0.40715; Test Loss: 0.29997\n",
      "Epoch: 5; Training Loss: 0.35483; Test Loss: 0.25244\n",
      "Epoch: 6; Training Loss: 0.30996; Test Loss: 0.20970\n",
      "Epoch: 7; Training Loss: 0.27238; Test Loss: 0.17704\n",
      "Epoch: 8; Training Loss: 0.23760; Test Loss: 0.15447\n",
      "Epoch: 9; Training Loss: 0.20953; Test Loss: 0.13093\n",
      "Epoch: 10; Training Loss: 0.18592; Test Loss: 0.11981\n",
      "Epoch: 11; Training Loss: 0.16637; Test Loss: 0.10765\n",
      "Epoch: 12; Training Loss: 0.14745; Test Loss: 0.10259\n",
      "Epoch: 13; Training Loss: 0.13015; Test Loss: 0.09076\n",
      "Epoch: 14; Training Loss: 0.11799; Test Loss: 0.08811\n",
      "Epoch: 15; Training Loss: 0.10543; Test Loss: 0.08091\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.63970; Test Loss: 0.55697\n",
      "Epoch: 2; Training Loss: 0.55607; Test Loss: 0.45181\n",
      "Epoch: 3; Training Loss: 0.48315; Test Loss: 0.36792\n",
      "Epoch: 4; Training Loss: 0.41913; Test Loss: 0.30516\n",
      "Epoch: 5; Training Loss: 0.36601; Test Loss: 0.25462\n",
      "Epoch: 6; Training Loss: 0.31652; Test Loss: 0.20975\n",
      "Epoch: 7; Training Loss: 0.27864; Test Loss: 0.18276\n",
      "Epoch: 8; Training Loss: 0.24690; Test Loss: 0.15363\n",
      "Epoch: 9; Training Loss: 0.21393; Test Loss: 0.14035\n",
      "Epoch: 10; Training Loss: 0.19014; Test Loss: 0.11815\n",
      "Epoch: 11; Training Loss: 0.16901; Test Loss: 0.11103\n",
      "Epoch: 12; Training Loss: 0.14929; Test Loss: 0.10330\n",
      "Epoch: 13; Training Loss: 0.13488; Test Loss: 0.09371\n",
      "Epoch: 14; Training Loss: 0.11870; Test Loss: 0.07609\n",
      "Epoch: 15; Training Loss: 0.11036; Test Loss: 0.06685\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.63309; Test Loss: 0.54650\n",
      "Epoch: 2; Training Loss: 0.54966; Test Loss: 0.44721\n",
      "Epoch: 3; Training Loss: 0.47696; Test Loss: 0.36506\n",
      "Epoch: 4; Training Loss: 0.41523; Test Loss: 0.30265\n",
      "Epoch: 5; Training Loss: 0.36046; Test Loss: 0.25000\n",
      "Epoch: 6; Training Loss: 0.31280; Test Loss: 0.21022\n",
      "Epoch: 7; Training Loss: 0.27414; Test Loss: 0.17942\n",
      "Epoch: 8; Training Loss: 0.24261; Test Loss: 0.15340\n",
      "Epoch: 9; Training Loss: 0.21227; Test Loss: 0.14130\n",
      "Epoch: 10; Training Loss: 0.18771; Test Loss: 0.11910\n",
      "Epoch: 11; Training Loss: 0.16424; Test Loss: 0.11531\n",
      "Epoch: 12; Training Loss: 0.15071; Test Loss: 0.09682\n",
      "Epoch: 13; Training Loss: 0.13357; Test Loss: 0.09159\n",
      "Epoch: 14; Training Loss: 0.11818; Test Loss: 0.08444\n",
      "Epoch: 15; Training Loss: 0.10769; Test Loss: 0.07308\n",
      "Done 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 5; lambda_c: 0.00; neg_count: 5\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.60109; Test Loss: 0.47768\n",
      "Epoch: 2; Training Loss: 0.47827; Test Loss: 0.36507\n",
      "Epoch: 3; Training Loss: 0.39552; Test Loss: 0.28637\n",
      "Epoch: 4; Training Loss: 0.33241; Test Loss: 0.23128\n",
      "Epoch: 5; Training Loss: 0.28288; Test Loss: 0.18970\n",
      "Epoch: 6; Training Loss: 0.24391; Test Loss: 0.15835\n",
      "Epoch: 7; Training Loss: 0.21187; Test Loss: 0.13170\n",
      "Epoch: 8; Training Loss: 0.18438; Test Loss: 0.11579\n",
      "Epoch: 9; Training Loss: 0.16131; Test Loss: 0.09846\n",
      "Epoch: 10; Training Loss: 0.14307; Test Loss: 0.08596\n",
      "Epoch: 11; Training Loss: 0.12610; Test Loss: 0.07983\n",
      "Epoch: 12; Training Loss: 0.11196; Test Loss: 0.06879\n",
      "Epoch: 13; Training Loss: 0.09906; Test Loss: 0.06637\n",
      "Epoch: 14; Training Loss: 0.08863; Test Loss: 0.06046\n",
      "Epoch: 15; Training Loss: 0.08016; Test Loss: 0.05638\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.60389; Test Loss: 0.48301\n",
      "Epoch: 2; Training Loss: 0.48069; Test Loss: 0.36562\n",
      "Epoch: 3; Training Loss: 0.39625; Test Loss: 0.28714\n",
      "Epoch: 4; Training Loss: 0.33403; Test Loss: 0.23274\n",
      "Epoch: 5; Training Loss: 0.28509; Test Loss: 0.18944\n",
      "Epoch: 6; Training Loss: 0.24549; Test Loss: 0.15615\n",
      "Epoch: 7; Training Loss: 0.21178; Test Loss: 0.13263\n",
      "Epoch: 8; Training Loss: 0.18417; Test Loss: 0.11310\n",
      "Epoch: 9; Training Loss: 0.16117; Test Loss: 0.09904\n",
      "Epoch: 10; Training Loss: 0.14256; Test Loss: 0.08513\n",
      "Epoch: 11; Training Loss: 0.12605; Test Loss: 0.07871\n",
      "Epoch: 12; Training Loss: 0.11295; Test Loss: 0.07143\n",
      "Epoch: 13; Training Loss: 0.10099; Test Loss: 0.05999\n",
      "Epoch: 14; Training Loss: 0.09060; Test Loss: 0.05599\n",
      "Epoch: 15; Training Loss: 0.08202; Test Loss: 0.05590\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.61232; Test Loss: 0.49363\n",
      "Epoch: 2; Training Loss: 0.48834; Test Loss: 0.37476\n",
      "Epoch: 3; Training Loss: 0.40172; Test Loss: 0.29503\n",
      "Epoch: 4; Training Loss: 0.33721; Test Loss: 0.23641\n",
      "Epoch: 5; Training Loss: 0.28789; Test Loss: 0.19335\n",
      "Epoch: 6; Training Loss: 0.24709; Test Loss: 0.16110\n",
      "Epoch: 7; Training Loss: 0.21484; Test Loss: 0.13547\n",
      "Epoch: 8; Training Loss: 0.18698; Test Loss: 0.11786\n",
      "Epoch: 9; Training Loss: 0.16377; Test Loss: 0.10358\n",
      "Epoch: 10; Training Loss: 0.14380; Test Loss: 0.08853\n",
      "Epoch: 11; Training Loss: 0.12677; Test Loss: 0.08146\n",
      "Epoch: 12; Training Loss: 0.11223; Test Loss: 0.07312\n",
      "Epoch: 13; Training Loss: 0.10115; Test Loss: 0.06459\n",
      "Epoch: 14; Training Loss: 0.08967; Test Loss: 0.06598\n",
      "Epoch: 15; Training Loss: 0.08143; Test Loss: 0.05903\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.59858; Test Loss: 0.47501\n",
      "Epoch: 2; Training Loss: 0.47632; Test Loss: 0.36047\n",
      "Epoch: 3; Training Loss: 0.39345; Test Loss: 0.28538\n",
      "Epoch: 4; Training Loss: 0.33106; Test Loss: 0.22964\n",
      "Epoch: 5; Training Loss: 0.28267; Test Loss: 0.18692\n",
      "Epoch: 6; Training Loss: 0.24332; Test Loss: 0.15532\n",
      "Epoch: 7; Training Loss: 0.21038; Test Loss: 0.13045\n",
      "Epoch: 8; Training Loss: 0.18422; Test Loss: 0.11328\n",
      "Epoch: 9; Training Loss: 0.16129; Test Loss: 0.09557\n",
      "Epoch: 10; Training Loss: 0.14186; Test Loss: 0.08655\n",
      "Epoch: 11; Training Loss: 0.12579; Test Loss: 0.07686\n",
      "Epoch: 12; Training Loss: 0.11162; Test Loss: 0.06935\n",
      "Epoch: 13; Training Loss: 0.10043; Test Loss: 0.06570\n",
      "Epoch: 14; Training Loss: 0.09003; Test Loss: 0.06101\n",
      "Epoch: 15; Training Loss: 0.08115; Test Loss: 0.05463\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.60746; Test Loss: 0.48620\n",
      "Epoch: 2; Training Loss: 0.48433; Test Loss: 0.36928\n",
      "Epoch: 3; Training Loss: 0.39816; Test Loss: 0.28965\n",
      "Epoch: 4; Training Loss: 0.33567; Test Loss: 0.23384\n",
      "Epoch: 5; Training Loss: 0.28678; Test Loss: 0.19114\n",
      "Epoch: 6; Training Loss: 0.24597; Test Loss: 0.15743\n",
      "Epoch: 7; Training Loss: 0.21282; Test Loss: 0.13247\n",
      "Epoch: 8; Training Loss: 0.18559; Test Loss: 0.11373\n",
      "Epoch: 9; Training Loss: 0.16279; Test Loss: 0.09945\n",
      "Epoch: 10; Training Loss: 0.14313; Test Loss: 0.08924\n",
      "Epoch: 11; Training Loss: 0.12647; Test Loss: 0.07979\n",
      "Epoch: 12; Training Loss: 0.11320; Test Loss: 0.07062\n",
      "Epoch: 13; Training Loss: 0.10115; Test Loss: 0.06433\n",
      "Epoch: 14; Training Loss: 0.08952; Test Loss: 0.06144\n",
      "Epoch: 15; Training Loss: 0.08147; Test Loss: 0.05577\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 5; lambda_c: 0.00; neg_count: 10\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.59176; Test Loss: 0.44993\n",
      "Epoch: 2; Training Loss: 0.44722; Test Loss: 0.31705\n",
      "Epoch: 3; Training Loss: 0.35143; Test Loss: 0.23997\n",
      "Epoch: 4; Training Loss: 0.28654; Test Loss: 0.19041\n",
      "Epoch: 5; Training Loss: 0.23941; Test Loss: 0.15334\n",
      "Epoch: 6; Training Loss: 0.20348; Test Loss: 0.12543\n",
      "Epoch: 7; Training Loss: 0.17489; Test Loss: 0.10351\n",
      "Epoch: 8; Training Loss: 0.15109; Test Loss: 0.08842\n",
      "Epoch: 9; Training Loss: 0.13210; Test Loss: 0.07723\n",
      "Epoch: 10; Training Loss: 0.11488; Test Loss: 0.06780\n",
      "Epoch: 11; Training Loss: 0.10176; Test Loss: 0.05888\n",
      "Epoch: 12; Training Loss: 0.09078; Test Loss: 0.05398\n",
      "Epoch: 13; Training Loss: 0.08038; Test Loss: 0.04968\n",
      "Epoch: 14; Training Loss: 0.07160; Test Loss: 0.04688\n",
      "Epoch: 15; Training Loss: 0.06461; Test Loss: 0.04430\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.58899; Test Loss: 0.44608\n",
      "Epoch: 2; Training Loss: 0.44503; Test Loss: 0.31399\n",
      "Epoch: 3; Training Loss: 0.34974; Test Loss: 0.23881\n",
      "Epoch: 4; Training Loss: 0.28528; Test Loss: 0.18803\n",
      "Epoch: 5; Training Loss: 0.23982; Test Loss: 0.15297\n",
      "Epoch: 6; Training Loss: 0.20251; Test Loss: 0.12555\n",
      "Epoch: 7; Training Loss: 0.17422; Test Loss: 0.10488\n",
      "Epoch: 8; Training Loss: 0.15068; Test Loss: 0.08744\n",
      "Epoch: 9; Training Loss: 0.13226; Test Loss: 0.07566\n",
      "Epoch: 10; Training Loss: 0.11515; Test Loss: 0.06688\n",
      "Epoch: 11; Training Loss: 0.10149; Test Loss: 0.05884\n",
      "Epoch: 12; Training Loss: 0.09046; Test Loss: 0.05252\n",
      "Epoch: 13; Training Loss: 0.08111; Test Loss: 0.04857\n",
      "Epoch: 14; Training Loss: 0.07231; Test Loss: 0.04461\n",
      "Epoch: 15; Training Loss: 0.06447; Test Loss: 0.04033\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.58490; Test Loss: 0.44046\n",
      "Epoch: 2; Training Loss: 0.44173; Test Loss: 0.31174\n",
      "Epoch: 3; Training Loss: 0.34731; Test Loss: 0.23654\n",
      "Epoch: 4; Training Loss: 0.28300; Test Loss: 0.18758\n",
      "Epoch: 5; Training Loss: 0.23760; Test Loss: 0.15266\n",
      "Epoch: 6; Training Loss: 0.20139; Test Loss: 0.12362\n",
      "Epoch: 7; Training Loss: 0.17333; Test Loss: 0.10487\n",
      "Epoch: 8; Training Loss: 0.15077; Test Loss: 0.09036\n",
      "Epoch: 9; Training Loss: 0.13135; Test Loss: 0.07647\n",
      "Epoch: 10; Training Loss: 0.11440; Test Loss: 0.06829\n",
      "Epoch: 11; Training Loss: 0.10140; Test Loss: 0.05963\n",
      "Epoch: 12; Training Loss: 0.08960; Test Loss: 0.05303\n",
      "Epoch: 13; Training Loss: 0.08012; Test Loss: 0.04948\n",
      "Epoch: 14; Training Loss: 0.07131; Test Loss: 0.04472\n",
      "Epoch: 15; Training Loss: 0.06385; Test Loss: 0.04364\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.58656; Test Loss: 0.44293\n",
      "Epoch: 2; Training Loss: 0.44293; Test Loss: 0.31345\n",
      "Epoch: 3; Training Loss: 0.34836; Test Loss: 0.23817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4; Training Loss: 0.28404; Test Loss: 0.18785\n",
      "Epoch: 5; Training Loss: 0.23767; Test Loss: 0.15204\n",
      "Epoch: 6; Training Loss: 0.20237; Test Loss: 0.12395\n",
      "Epoch: 7; Training Loss: 0.17363; Test Loss: 0.10474\n",
      "Epoch: 8; Training Loss: 0.15063; Test Loss: 0.08770\n",
      "Epoch: 9; Training Loss: 0.13179; Test Loss: 0.07405\n",
      "Epoch: 10; Training Loss: 0.11539; Test Loss: 0.06590\n",
      "Epoch: 11; Training Loss: 0.10157; Test Loss: 0.05840\n",
      "Epoch: 12; Training Loss: 0.09064; Test Loss: 0.05055\n",
      "Epoch: 13; Training Loss: 0.08044; Test Loss: 0.04758\n",
      "Epoch: 14; Training Loss: 0.07191; Test Loss: 0.04322\n",
      "Epoch: 15; Training Loss: 0.06550; Test Loss: 0.04203\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.59267; Test Loss: 0.45108\n",
      "Epoch: 2; Training Loss: 0.44801; Test Loss: 0.31698\n",
      "Epoch: 3; Training Loss: 0.35151; Test Loss: 0.24062\n",
      "Epoch: 4; Training Loss: 0.28647; Test Loss: 0.19021\n",
      "Epoch: 5; Training Loss: 0.23976; Test Loss: 0.15409\n",
      "Epoch: 6; Training Loss: 0.20413; Test Loss: 0.12607\n",
      "Epoch: 7; Training Loss: 0.17467; Test Loss: 0.10459\n",
      "Epoch: 8; Training Loss: 0.15128; Test Loss: 0.08861\n",
      "Epoch: 9; Training Loss: 0.13187; Test Loss: 0.07552\n",
      "Epoch: 10; Training Loss: 0.11603; Test Loss: 0.06654\n",
      "Epoch: 11; Training Loss: 0.10201; Test Loss: 0.05933\n",
      "Epoch: 12; Training Loss: 0.08999; Test Loss: 0.05326\n",
      "Epoch: 13; Training Loss: 0.08067; Test Loss: 0.04872\n",
      "Epoch: 14; Training Loss: 0.07236; Test Loss: 0.04408\n",
      "Epoch: 15; Training Loss: 0.06420; Test Loss: 0.04174\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 5; lambda_c: 0.10; neg_count: 1\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.10; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.63915; Test Loss: 0.55540\n",
      "Epoch: 2; Training Loss: 0.55508; Test Loss: 0.45385\n",
      "Epoch: 3; Training Loss: 0.48092; Test Loss: 0.37251\n",
      "Epoch: 4; Training Loss: 0.41989; Test Loss: 0.30674\n",
      "Epoch: 5; Training Loss: 0.36301; Test Loss: 0.26013\n",
      "Epoch: 6; Training Loss: 0.31586; Test Loss: 0.21360\n",
      "Epoch: 7; Training Loss: 0.27879; Test Loss: 0.17903\n",
      "Epoch: 8; Training Loss: 0.24261; Test Loss: 0.16164\n",
      "Epoch: 9; Training Loss: 0.21756; Test Loss: 0.13926\n",
      "Epoch: 10; Training Loss: 0.18914; Test Loss: 0.12277\n",
      "Epoch: 11; Training Loss: 0.16810; Test Loss: 0.10431\n",
      "Epoch: 12; Training Loss: 0.14905; Test Loss: 0.09647\n",
      "Epoch: 13; Training Loss: 0.13603; Test Loss: 0.09164\n",
      "Epoch: 14; Training Loss: 0.11947; Test Loss: 0.07516\n",
      "Epoch: 15; Training Loss: 0.10860; Test Loss: 0.07853\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.10; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.63989; Test Loss: 0.55835\n",
      "Epoch: 2; Training Loss: 0.55907; Test Loss: 0.45928\n",
      "Epoch: 3; Training Loss: 0.48677; Test Loss: 0.37593\n",
      "Epoch: 4; Training Loss: 0.42358; Test Loss: 0.31777\n",
      "Epoch: 5; Training Loss: 0.36974; Test Loss: 0.26489\n",
      "Epoch: 6; Training Loss: 0.32382; Test Loss: 0.22180\n",
      "Epoch: 7; Training Loss: 0.28027; Test Loss: 0.19095\n",
      "Epoch: 8; Training Loss: 0.25067; Test Loss: 0.16849\n",
      "Epoch: 9; Training Loss: 0.22059; Test Loss: 0.14633\n",
      "Epoch: 10; Training Loss: 0.19734; Test Loss: 0.13046\n",
      "Epoch: 11; Training Loss: 0.17632; Test Loss: 0.11779\n",
      "Epoch: 12; Training Loss: 0.15686; Test Loss: 0.10548\n",
      "Epoch: 13; Training Loss: 0.14356; Test Loss: 0.10229\n",
      "Epoch: 14; Training Loss: 0.12915; Test Loss: 0.08929\n",
      "Epoch: 15; Training Loss: 0.11814; Test Loss: 0.08219\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.10; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.63119; Test Loss: 0.54531\n",
      "Epoch: 2; Training Loss: 0.54839; Test Loss: 0.44646\n",
      "Epoch: 3; Training Loss: 0.47581; Test Loss: 0.36764\n",
      "Epoch: 4; Training Loss: 0.41251; Test Loss: 0.30466\n",
      "Epoch: 5; Training Loss: 0.36003; Test Loss: 0.25678\n",
      "Epoch: 6; Training Loss: 0.31334; Test Loss: 0.21216\n",
      "Epoch: 7; Training Loss: 0.27324; Test Loss: 0.18328\n",
      "Epoch: 8; Training Loss: 0.24003; Test Loss: 0.16024\n",
      "Epoch: 9; Training Loss: 0.21153; Test Loss: 0.13740\n",
      "Epoch: 10; Training Loss: 0.18690; Test Loss: 0.11252\n",
      "Epoch: 11; Training Loss: 0.16342; Test Loss: 0.10961\n",
      "Epoch: 12; Training Loss: 0.14847; Test Loss: 0.09960\n",
      "Epoch: 13; Training Loss: 0.13149; Test Loss: 0.08885\n",
      "Epoch: 14; Training Loss: 0.11909; Test Loss: 0.07758\n",
      "Epoch: 15; Training Loss: 0.10716; Test Loss: 0.07841\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.10; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.63307; Test Loss: 0.54671\n",
      "Epoch: 2; Training Loss: 0.54975; Test Loss: 0.44712\n",
      "Epoch: 3; Training Loss: 0.47845; Test Loss: 0.36407\n",
      "Epoch: 4; Training Loss: 0.41615; Test Loss: 0.29781\n",
      "Epoch: 5; Training Loss: 0.36116; Test Loss: 0.24717\n",
      "Epoch: 6; Training Loss: 0.31345; Test Loss: 0.21025\n",
      "Epoch: 7; Training Loss: 0.27599; Test Loss: 0.17835\n",
      "Epoch: 8; Training Loss: 0.24212; Test Loss: 0.15356\n",
      "Epoch: 9; Training Loss: 0.21148; Test Loss: 0.12772\n",
      "Epoch: 10; Training Loss: 0.18830; Test Loss: 0.12228\n",
      "Epoch: 11; Training Loss: 0.16730; Test Loss: 0.10757\n",
      "Epoch: 12; Training Loss: 0.14845; Test Loss: 0.10234\n",
      "Epoch: 13; Training Loss: 0.13468; Test Loss: 0.08972\n",
      "Epoch: 14; Training Loss: 0.11775; Test Loss: 0.07865\n",
      "Epoch: 15; Training Loss: 0.10988; Test Loss: 0.06963\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.10; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.63469; Test Loss: 0.54813\n",
      "Epoch: 2; Training Loss: 0.55124; Test Loss: 0.44625\n",
      "Epoch: 3; Training Loss: 0.47842; Test Loss: 0.37002\n",
      "Epoch: 4; Training Loss: 0.41749; Test Loss: 0.30131\n",
      "Epoch: 5; Training Loss: 0.36108; Test Loss: 0.25228\n",
      "Epoch: 6; Training Loss: 0.31412; Test Loss: 0.21357\n",
      "Epoch: 7; Training Loss: 0.27465; Test Loss: 0.18847\n",
      "Epoch: 8; Training Loss: 0.24159; Test Loss: 0.15606\n",
      "Epoch: 9; Training Loss: 0.21421; Test Loss: 0.13244\n",
      "Epoch: 10; Training Loss: 0.18875; Test Loss: 0.12256\n",
      "Epoch: 11; Training Loss: 0.16500; Test Loss: 0.10744\n",
      "Epoch: 12; Training Loss: 0.14930; Test Loss: 0.09778\n",
      "Epoch: 13; Training Loss: 0.13644; Test Loss: 0.09102\n",
      "Epoch: 14; Training Loss: 0.12130; Test Loss: 0.07919\n",
      "Epoch: 15; Training Loss: 0.10816; Test Loss: 0.07731\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 5; lambda_c: 0.10; neg_count: 5\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.10; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.59833; Test Loss: 0.47575\n",
      "Epoch: 2; Training Loss: 0.47748; Test Loss: 0.36100\n",
      "Epoch: 3; Training Loss: 0.39277; Test Loss: 0.28592\n",
      "Epoch: 4; Training Loss: 0.33224; Test Loss: 0.22995\n",
      "Epoch: 5; Training Loss: 0.28266; Test Loss: 0.18769\n",
      "Epoch: 6; Training Loss: 0.24311; Test Loss: 0.15722\n",
      "Epoch: 7; Training Loss: 0.21179; Test Loss: 0.13144\n",
      "Epoch: 8; Training Loss: 0.18359; Test Loss: 0.11334\n",
      "Epoch: 9; Training Loss: 0.16178; Test Loss: 0.09749\n",
      "Epoch: 10; Training Loss: 0.14186; Test Loss: 0.08669\n",
      "Epoch: 11; Training Loss: 0.12643; Test Loss: 0.07729\n",
      "Epoch: 12; Training Loss: 0.11246; Test Loss: 0.07402\n",
      "Epoch: 13; Training Loss: 0.09899; Test Loss: 0.06732\n",
      "Epoch: 14; Training Loss: 0.08916; Test Loss: 0.06000\n",
      "Epoch: 15; Training Loss: 0.08141; Test Loss: 0.05766\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.10; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.59706; Test Loss: 0.47224\n",
      "Epoch: 2; Training Loss: 0.47642; Test Loss: 0.36084\n",
      "Epoch: 3; Training Loss: 0.39271; Test Loss: 0.28291\n",
      "Epoch: 4; Training Loss: 0.33109; Test Loss: 0.22762\n",
      "Epoch: 5; Training Loss: 0.28343; Test Loss: 0.18654\n",
      "Epoch: 6; Training Loss: 0.24341; Test Loss: 0.15536\n",
      "Epoch: 7; Training Loss: 0.21187; Test Loss: 0.13207\n",
      "Epoch: 8; Training Loss: 0.18419; Test Loss: 0.10938\n",
      "Epoch: 9; Training Loss: 0.15990; Test Loss: 0.09510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10; Training Loss: 0.14139; Test Loss: 0.08417\n",
      "Epoch: 11; Training Loss: 0.12586; Test Loss: 0.07578\n",
      "Epoch: 12; Training Loss: 0.11183; Test Loss: 0.07066\n",
      "Epoch: 13; Training Loss: 0.10133; Test Loss: 0.06429\n",
      "Epoch: 14; Training Loss: 0.08978; Test Loss: 0.06009\n",
      "Epoch: 15; Training Loss: 0.08093; Test Loss: 0.05683\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.10; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.60481; Test Loss: 0.48478\n",
      "Epoch: 2; Training Loss: 0.48235; Test Loss: 0.36654\n",
      "Epoch: 3; Training Loss: 0.39748; Test Loss: 0.29083\n",
      "Epoch: 4; Training Loss: 0.33342; Test Loss: 0.23567\n",
      "Epoch: 5; Training Loss: 0.28461; Test Loss: 0.19128\n",
      "Epoch: 6; Training Loss: 0.24651; Test Loss: 0.16257\n",
      "Epoch: 7; Training Loss: 0.21208; Test Loss: 0.13195\n",
      "Epoch: 8; Training Loss: 0.18515; Test Loss: 0.11685\n",
      "Epoch: 9; Training Loss: 0.16223; Test Loss: 0.09848\n",
      "Epoch: 10; Training Loss: 0.14328; Test Loss: 0.08829\n",
      "Epoch: 11; Training Loss: 0.12567; Test Loss: 0.08179\n",
      "Epoch: 12; Training Loss: 0.11126; Test Loss: 0.07379\n",
      "Epoch: 13; Training Loss: 0.09959; Test Loss: 0.06528\n",
      "Epoch: 14; Training Loss: 0.08969; Test Loss: 0.06350\n",
      "Epoch: 15; Training Loss: 0.08156; Test Loss: 0.05756\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.10; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.60873; Test Loss: 0.48827\n",
      "Epoch: 2; Training Loss: 0.48528; Test Loss: 0.37099\n",
      "Epoch: 3; Training Loss: 0.40038; Test Loss: 0.29082\n",
      "Epoch: 4; Training Loss: 0.33765; Test Loss: 0.23561\n",
      "Epoch: 5; Training Loss: 0.28662; Test Loss: 0.19145\n",
      "Epoch: 6; Training Loss: 0.24862; Test Loss: 0.16171\n",
      "Epoch: 7; Training Loss: 0.21513; Test Loss: 0.13472\n",
      "Epoch: 8; Training Loss: 0.18825; Test Loss: 0.11700\n",
      "Epoch: 9; Training Loss: 0.16424; Test Loss: 0.10121\n",
      "Epoch: 10; Training Loss: 0.14555; Test Loss: 0.08786\n",
      "Epoch: 11; Training Loss: 0.12941; Test Loss: 0.08353\n",
      "Epoch: 12; Training Loss: 0.11515; Test Loss: 0.07196\n",
      "Epoch: 13; Training Loss: 0.10313; Test Loss: 0.06675\n",
      "Epoch: 14; Training Loss: 0.09282; Test Loss: 0.06134\n",
      "Epoch: 15; Training Loss: 0.08458; Test Loss: 0.05989\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.10; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.60429; Test Loss: 0.48207\n",
      "Epoch: 2; Training Loss: 0.48118; Test Loss: 0.36580\n",
      "Epoch: 3; Training Loss: 0.39795; Test Loss: 0.28812\n",
      "Epoch: 4; Training Loss: 0.33347; Test Loss: 0.23222\n",
      "Epoch: 5; Training Loss: 0.28545; Test Loss: 0.19083\n",
      "Epoch: 6; Training Loss: 0.24624; Test Loss: 0.15784\n",
      "Epoch: 7; Training Loss: 0.21450; Test Loss: 0.13470\n",
      "Epoch: 8; Training Loss: 0.18528; Test Loss: 0.11565\n",
      "Epoch: 9; Training Loss: 0.16430; Test Loss: 0.10108\n",
      "Epoch: 10; Training Loss: 0.14418; Test Loss: 0.09092\n",
      "Epoch: 11; Training Loss: 0.12835; Test Loss: 0.07987\n",
      "Epoch: 12; Training Loss: 0.11373; Test Loss: 0.07424\n",
      "Epoch: 13; Training Loss: 0.10283; Test Loss: 0.06904\n",
      "Epoch: 14; Training Loss: 0.09174; Test Loss: 0.06314\n",
      "Epoch: 15; Training Loss: 0.08346; Test Loss: 0.06193\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 5; lambda_c: 0.10; neg_count: 10\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.10; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.59711; Test Loss: 0.45692\n",
      "Epoch: 2; Training Loss: 0.45184; Test Loss: 0.32151\n",
      "Epoch: 3; Training Loss: 0.35540; Test Loss: 0.24248\n",
      "Epoch: 4; Training Loss: 0.28865; Test Loss: 0.19160\n",
      "Epoch: 5; Training Loss: 0.23986; Test Loss: 0.15560\n",
      "Epoch: 6; Training Loss: 0.20593; Test Loss: 0.12799\n",
      "Epoch: 7; Training Loss: 0.17688; Test Loss: 0.10880\n",
      "Epoch: 8; Training Loss: 0.15265; Test Loss: 0.09177\n",
      "Epoch: 9; Training Loss: 0.13307; Test Loss: 0.07902\n",
      "Epoch: 10; Training Loss: 0.11826; Test Loss: 0.06770\n",
      "Epoch: 11; Training Loss: 0.10453; Test Loss: 0.06253\n",
      "Epoch: 12; Training Loss: 0.09200; Test Loss: 0.05777\n",
      "Epoch: 13; Training Loss: 0.08280; Test Loss: 0.05019\n",
      "Epoch: 14; Training Loss: 0.07388; Test Loss: 0.04835\n",
      "Epoch: 15; Training Loss: 0.06646; Test Loss: 0.04646\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.10; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.59655; Test Loss: 0.45588\n",
      "Epoch: 2; Training Loss: 0.45108; Test Loss: 0.32129\n",
      "Epoch: 3; Training Loss: 0.35386; Test Loss: 0.24247\n",
      "Epoch: 4; Training Loss: 0.28879; Test Loss: 0.19147\n",
      "Epoch: 5; Training Loss: 0.24131; Test Loss: 0.15380\n",
      "Epoch: 6; Training Loss: 0.20460; Test Loss: 0.12756\n",
      "Epoch: 7; Training Loss: 0.17593; Test Loss: 0.10627\n",
      "Epoch: 8; Training Loss: 0.15236; Test Loss: 0.08906\n",
      "Epoch: 9; Training Loss: 0.13257; Test Loss: 0.07712\n",
      "Epoch: 10; Training Loss: 0.11679; Test Loss: 0.06615\n",
      "Epoch: 11; Training Loss: 0.10290; Test Loss: 0.05870\n",
      "Epoch: 12; Training Loss: 0.09056; Test Loss: 0.05468\n",
      "Epoch: 13; Training Loss: 0.08189; Test Loss: 0.04985\n",
      "Epoch: 14; Training Loss: 0.07288; Test Loss: 0.04475\n",
      "Epoch: 15; Training Loss: 0.06583; Test Loss: 0.04206\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.10; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.59026; Test Loss: 0.44741\n",
      "Epoch: 2; Training Loss: 0.44600; Test Loss: 0.31604\n",
      "Epoch: 3; Training Loss: 0.35094; Test Loss: 0.23910\n",
      "Epoch: 4; Training Loss: 0.28608; Test Loss: 0.18956\n",
      "Epoch: 5; Training Loss: 0.23944; Test Loss: 0.15445\n",
      "Epoch: 6; Training Loss: 0.20367; Test Loss: 0.12685\n",
      "Epoch: 7; Training Loss: 0.17428; Test Loss: 0.10560\n",
      "Epoch: 8; Training Loss: 0.15186; Test Loss: 0.08972\n",
      "Epoch: 9; Training Loss: 0.13134; Test Loss: 0.07781\n",
      "Epoch: 10; Training Loss: 0.11611; Test Loss: 0.06855\n",
      "Epoch: 11; Training Loss: 0.10104; Test Loss: 0.05945\n",
      "Epoch: 12; Training Loss: 0.09030; Test Loss: 0.05398\n",
      "Epoch: 13; Training Loss: 0.08052; Test Loss: 0.05149\n",
      "Epoch: 14; Training Loss: 0.07197; Test Loss: 0.04637\n",
      "Epoch: 15; Training Loss: 0.06544; Test Loss: 0.04219\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.10; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.58624; Test Loss: 0.44285\n",
      "Epoch: 2; Training Loss: 0.44294; Test Loss: 0.31325\n",
      "Epoch: 3; Training Loss: 0.34841; Test Loss: 0.23654\n",
      "Epoch: 4; Training Loss: 0.28359; Test Loss: 0.18738\n",
      "Epoch: 5; Training Loss: 0.23868; Test Loss: 0.15193\n",
      "Epoch: 6; Training Loss: 0.20262; Test Loss: 0.12471\n",
      "Epoch: 7; Training Loss: 0.17475; Test Loss: 0.10468\n",
      "Epoch: 8; Training Loss: 0.15150; Test Loss: 0.08904\n",
      "Epoch: 9; Training Loss: 0.13231; Test Loss: 0.07620\n",
      "Epoch: 10; Training Loss: 0.11605; Test Loss: 0.06740\n",
      "Epoch: 11; Training Loss: 0.10330; Test Loss: 0.06044\n",
      "Epoch: 12; Training Loss: 0.09121; Test Loss: 0.05508\n",
      "Epoch: 13; Training Loss: 0.08191; Test Loss: 0.04979\n",
      "Epoch: 14; Training Loss: 0.07359; Test Loss: 0.04631\n",
      "Epoch: 15; Training Loss: 0.06682; Test Loss: 0.04317\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.10; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.57328; Test Loss: 0.42461\n",
      "Epoch: 2; Training Loss: 0.43234; Test Loss: 0.30262\n",
      "Epoch: 3; Training Loss: 0.34152; Test Loss: 0.23033\n",
      "Epoch: 4; Training Loss: 0.28030; Test Loss: 0.18267\n",
      "Epoch: 5; Training Loss: 0.23440; Test Loss: 0.14837\n",
      "Epoch: 6; Training Loss: 0.19927; Test Loss: 0.12270\n",
      "Epoch: 7; Training Loss: 0.17206; Test Loss: 0.10297\n",
      "Epoch: 8; Training Loss: 0.14916; Test Loss: 0.08609\n",
      "Epoch: 9; Training Loss: 0.13057; Test Loss: 0.07530\n",
      "Epoch: 10; Training Loss: 0.11483; Test Loss: 0.06588\n",
      "Epoch: 11; Training Loss: 0.10170; Test Loss: 0.05971\n",
      "Epoch: 12; Training Loss: 0.08966; Test Loss: 0.05575\n",
      "Epoch: 13; Training Loss: 0.08106; Test Loss: 0.05002\n",
      "Epoch: 14; Training Loss: 0.07252; Test Loss: 0.04804\n",
      "Epoch: 15; Training Loss: 0.06564; Test Loss: 0.04367\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 5; lambda_c: 1.00; neg_count: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=1.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.64345; Test Loss: 0.56048\n",
      "Epoch: 2; Training Loss: 0.55905; Test Loss: 0.46094\n",
      "Epoch: 3; Training Loss: 0.48636; Test Loss: 0.37467\n",
      "Epoch: 4; Training Loss: 0.42118; Test Loss: 0.31016\n",
      "Epoch: 5; Training Loss: 0.36659; Test Loss: 0.25821\n",
      "Epoch: 6; Training Loss: 0.32079; Test Loss: 0.21697\n",
      "Epoch: 7; Training Loss: 0.28054; Test Loss: 0.18430\n",
      "Epoch: 8; Training Loss: 0.24474; Test Loss: 0.16214\n",
      "Epoch: 9; Training Loss: 0.21648; Test Loss: 0.14371\n",
      "Epoch: 10; Training Loss: 0.19046; Test Loss: 0.12307\n",
      "Epoch: 11; Training Loss: 0.17030; Test Loss: 0.12117\n",
      "Epoch: 12; Training Loss: 0.15023; Test Loss: 0.09650\n",
      "Epoch: 13; Training Loss: 0.13620; Test Loss: 0.08988\n",
      "Epoch: 14; Training Loss: 0.12320; Test Loss: 0.08415\n",
      "Epoch: 15; Training Loss: 0.11296; Test Loss: 0.08189\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=1.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.64407; Test Loss: 0.56127\n",
      "Epoch: 2; Training Loss: 0.56221; Test Loss: 0.46461\n",
      "Epoch: 3; Training Loss: 0.49309; Test Loss: 0.38853\n",
      "Epoch: 4; Training Loss: 0.43073; Test Loss: 0.32110\n",
      "Epoch: 5; Training Loss: 0.37537; Test Loss: 0.26875\n",
      "Epoch: 6; Training Loss: 0.33167; Test Loss: 0.22763\n",
      "Epoch: 7; Training Loss: 0.29153; Test Loss: 0.20110\n",
      "Epoch: 8; Training Loss: 0.25769; Test Loss: 0.16654\n",
      "Epoch: 9; Training Loss: 0.22804; Test Loss: 0.15048\n",
      "Epoch: 10; Training Loss: 0.20706; Test Loss: 0.13344\n",
      "Epoch: 11; Training Loss: 0.18550; Test Loss: 0.12585\n",
      "Epoch: 12; Training Loss: 0.16472; Test Loss: 0.10970\n",
      "Epoch: 13; Training Loss: 0.14959; Test Loss: 0.09949\n",
      "Epoch: 14; Training Loss: 0.13363; Test Loss: 0.08902\n",
      "Epoch: 15; Training Loss: 0.12182; Test Loss: 0.08036\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=1.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.63447; Test Loss: 0.54641\n",
      "Epoch: 2; Training Loss: 0.54947; Test Loss: 0.44931\n",
      "Epoch: 3; Training Loss: 0.47818; Test Loss: 0.36661\n",
      "Epoch: 4; Training Loss: 0.41465; Test Loss: 0.30702\n",
      "Epoch: 5; Training Loss: 0.36081; Test Loss: 0.25529\n",
      "Epoch: 6; Training Loss: 0.31428; Test Loss: 0.21322\n",
      "Epoch: 7; Training Loss: 0.27474; Test Loss: 0.18716\n",
      "Epoch: 8; Training Loss: 0.23950; Test Loss: 0.15148\n",
      "Epoch: 9; Training Loss: 0.21066; Test Loss: 0.13743\n",
      "Epoch: 10; Training Loss: 0.18801; Test Loss: 0.12012\n",
      "Epoch: 11; Training Loss: 0.16654; Test Loss: 0.11302\n",
      "Epoch: 12; Training Loss: 0.14872; Test Loss: 0.10114\n",
      "Epoch: 13; Training Loss: 0.13423; Test Loss: 0.09403\n",
      "Epoch: 14; Training Loss: 0.12223; Test Loss: 0.08786\n",
      "Epoch: 15; Training Loss: 0.10788; Test Loss: 0.07869\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=1.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.64157; Test Loss: 0.55412\n",
      "Epoch: 2; Training Loss: 0.55739; Test Loss: 0.45286\n",
      "Epoch: 3; Training Loss: 0.48480; Test Loss: 0.37328\n",
      "Epoch: 4; Training Loss: 0.42111; Test Loss: 0.30247\n",
      "Epoch: 5; Training Loss: 0.36633; Test Loss: 0.25330\n",
      "Epoch: 6; Training Loss: 0.32028; Test Loss: 0.21858\n",
      "Epoch: 7; Training Loss: 0.27902; Test Loss: 0.18114\n",
      "Epoch: 8; Training Loss: 0.24475; Test Loss: 0.15863\n",
      "Epoch: 9; Training Loss: 0.21460; Test Loss: 0.13634\n",
      "Epoch: 10; Training Loss: 0.19059; Test Loss: 0.12189\n",
      "Epoch: 11; Training Loss: 0.16845; Test Loss: 0.10900\n",
      "Epoch: 12; Training Loss: 0.15430; Test Loss: 0.09793\n",
      "Epoch: 13; Training Loss: 0.13508; Test Loss: 0.09430\n",
      "Epoch: 14; Training Loss: 0.12367; Test Loss: 0.08660\n",
      "Epoch: 15; Training Loss: 0.11240; Test Loss: 0.07910\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=1.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.63625; Test Loss: 0.54922\n",
      "Epoch: 2; Training Loss: 0.55596; Test Loss: 0.45484\n",
      "Epoch: 3; Training Loss: 0.48539; Test Loss: 0.37561\n",
      "Epoch: 4; Training Loss: 0.42490; Test Loss: 0.31688\n",
      "Epoch: 5; Training Loss: 0.37325; Test Loss: 0.26939\n",
      "Epoch: 6; Training Loss: 0.32769; Test Loss: 0.22126\n",
      "Epoch: 7; Training Loss: 0.28887; Test Loss: 0.19594\n",
      "Epoch: 8; Training Loss: 0.25436; Test Loss: 0.17125\n",
      "Epoch: 9; Training Loss: 0.22648; Test Loss: 0.15120\n",
      "Epoch: 10; Training Loss: 0.20088; Test Loss: 0.13273\n",
      "Epoch: 11; Training Loss: 0.18109; Test Loss: 0.11781\n",
      "Epoch: 12; Training Loss: 0.16018; Test Loss: 0.11019\n",
      "Epoch: 13; Training Loss: 0.14703; Test Loss: 0.09882\n",
      "Epoch: 14; Training Loss: 0.13315; Test Loss: 0.10878\n",
      "Epoch: 15; Training Loss: 0.12139; Test Loss: 0.09763\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 5; lambda_c: 1.00; neg_count: 5\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=1.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.60164; Test Loss: 0.47117\n",
      "Epoch: 2; Training Loss: 0.48112; Test Loss: 0.35958\n",
      "Epoch: 3; Training Loss: 0.39666; Test Loss: 0.28504\n",
      "Epoch: 4; Training Loss: 0.33526; Test Loss: 0.23306\n",
      "Epoch: 5; Training Loss: 0.28634; Test Loss: 0.19314\n",
      "Epoch: 6; Training Loss: 0.24815; Test Loss: 0.16212\n",
      "Epoch: 7; Training Loss: 0.21727; Test Loss: 0.14034\n",
      "Epoch: 8; Training Loss: 0.18950; Test Loss: 0.12187\n",
      "Epoch: 9; Training Loss: 0.16814; Test Loss: 0.10825\n",
      "Epoch: 10; Training Loss: 0.14965; Test Loss: 0.09641\n",
      "Epoch: 11; Training Loss: 0.13425; Test Loss: 0.08767\n",
      "Epoch: 12; Training Loss: 0.12064; Test Loss: 0.08041\n",
      "Epoch: 13; Training Loss: 0.10699; Test Loss: 0.07903\n",
      "Epoch: 14; Training Loss: 0.09740; Test Loss: 0.07177\n",
      "Epoch: 15; Training Loss: 0.09109; Test Loss: 0.06452\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=1.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.60007; Test Loss: 0.47247\n",
      "Epoch: 2; Training Loss: 0.47863; Test Loss: 0.36063\n",
      "Epoch: 3; Training Loss: 0.39520; Test Loss: 0.28402\n",
      "Epoch: 4; Training Loss: 0.33209; Test Loss: 0.22833\n",
      "Epoch: 5; Training Loss: 0.28401; Test Loss: 0.18850\n",
      "Epoch: 6; Training Loss: 0.24426; Test Loss: 0.15557\n",
      "Epoch: 7; Training Loss: 0.21100; Test Loss: 0.13310\n",
      "Epoch: 8; Training Loss: 0.18465; Test Loss: 0.11317\n",
      "Epoch: 9; Training Loss: 0.16246; Test Loss: 0.09781\n",
      "Epoch: 10; Training Loss: 0.14221; Test Loss: 0.08450\n",
      "Epoch: 11; Training Loss: 0.12778; Test Loss: 0.08224\n",
      "Epoch: 12; Training Loss: 0.11292; Test Loss: 0.07147\n",
      "Epoch: 13; Training Loss: 0.10075; Test Loss: 0.06070\n",
      "Epoch: 14; Training Loss: 0.09077; Test Loss: 0.06320\n",
      "Epoch: 15; Training Loss: 0.08059; Test Loss: 0.05710\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=1.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.62302; Test Loss: 0.49680\n",
      "Epoch: 2; Training Loss: 0.49267; Test Loss: 0.37627\n",
      "Epoch: 3; Training Loss: 0.40596; Test Loss: 0.29746\n",
      "Epoch: 4; Training Loss: 0.34102; Test Loss: 0.23847\n",
      "Epoch: 5; Training Loss: 0.29086; Test Loss: 0.19517\n",
      "Epoch: 6; Training Loss: 0.25014; Test Loss: 0.16289\n",
      "Epoch: 7; Training Loss: 0.21659; Test Loss: 0.13888\n",
      "Epoch: 8; Training Loss: 0.18841; Test Loss: 0.12104\n",
      "Epoch: 9; Training Loss: 0.16597; Test Loss: 0.10511\n",
      "Epoch: 10; Training Loss: 0.14674; Test Loss: 0.09198\n",
      "Epoch: 11; Training Loss: 0.12819; Test Loss: 0.08122\n",
      "Epoch: 12; Training Loss: 0.11422; Test Loss: 0.07548\n",
      "Epoch: 13; Training Loss: 0.10206; Test Loss: 0.06609\n",
      "Epoch: 14; Training Loss: 0.09150; Test Loss: 0.06407\n",
      "Epoch: 15; Training Loss: 0.08341; Test Loss: 0.05919\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=1.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.61585; Test Loss: 0.49219\n",
      "Epoch: 2; Training Loss: 0.49201; Test Loss: 0.37340\n",
      "Epoch: 3; Training Loss: 0.40556; Test Loss: 0.29589\n",
      "Epoch: 4; Training Loss: 0.34190; Test Loss: 0.24050\n",
      "Epoch: 5; Training Loss: 0.29258; Test Loss: 0.19883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6; Training Loss: 0.25278; Test Loss: 0.16429\n",
      "Epoch: 7; Training Loss: 0.22200; Test Loss: 0.14304\n",
      "Epoch: 8; Training Loss: 0.19381; Test Loss: 0.12451\n",
      "Epoch: 9; Training Loss: 0.17206; Test Loss: 0.11065\n",
      "Epoch: 10; Training Loss: 0.15282; Test Loss: 0.09841\n",
      "Epoch: 11; Training Loss: 0.13499; Test Loss: 0.08644\n",
      "Epoch: 12; Training Loss: 0.12172; Test Loss: 0.08329\n",
      "Epoch: 13; Training Loss: 0.11065; Test Loss: 0.07335\n",
      "Epoch: 14; Training Loss: 0.10156; Test Loss: 0.06920\n",
      "Epoch: 15; Training Loss: 0.09197; Test Loss: 0.06517\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=1.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.59611; Test Loss: 0.46673\n",
      "Epoch: 2; Training Loss: 0.47415; Test Loss: 0.35441\n",
      "Epoch: 3; Training Loss: 0.39271; Test Loss: 0.28145\n",
      "Epoch: 4; Training Loss: 0.33041; Test Loss: 0.22784\n",
      "Epoch: 5; Training Loss: 0.28272; Test Loss: 0.18521\n",
      "Epoch: 6; Training Loss: 0.24230; Test Loss: 0.15460\n",
      "Epoch: 7; Training Loss: 0.20976; Test Loss: 0.13055\n",
      "Epoch: 8; Training Loss: 0.18478; Test Loss: 0.11214\n",
      "Epoch: 9; Training Loss: 0.16060; Test Loss: 0.09770\n",
      "Epoch: 10; Training Loss: 0.14217; Test Loss: 0.08220\n",
      "Epoch: 11; Training Loss: 0.12561; Test Loss: 0.07877\n",
      "Epoch: 12; Training Loss: 0.11135; Test Loss: 0.07311\n",
      "Epoch: 13; Training Loss: 0.09984; Test Loss: 0.06579\n",
      "Epoch: 14; Training Loss: 0.09098; Test Loss: 0.06217\n",
      "Epoch: 15; Training Loss: 0.08199; Test Loss: 0.05420\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 5; lambda_c: 1.00; neg_count: 10\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=1.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.60502; Test Loss: 0.46573\n",
      "Epoch: 2; Training Loss: 0.46469; Test Loss: 0.32820\n",
      "Epoch: 3; Training Loss: 0.36605; Test Loss: 0.24651\n",
      "Epoch: 4; Training Loss: 0.29888; Test Loss: 0.19547\n",
      "Epoch: 5; Training Loss: 0.25047; Test Loss: 0.16384\n",
      "Epoch: 6; Training Loss: 0.21463; Test Loss: 0.13962\n",
      "Epoch: 7; Training Loss: 0.18749; Test Loss: 0.12253\n",
      "Epoch: 8; Training Loss: 0.16588; Test Loss: 0.11021\n",
      "Epoch: 9; Training Loss: 0.14769; Test Loss: 0.10180\n",
      "Epoch: 10; Training Loss: 0.13269; Test Loss: 0.09241\n",
      "Epoch: 11; Training Loss: 0.12050; Test Loss: 0.08779\n",
      "Epoch: 12; Training Loss: 0.10941; Test Loss: 0.08407\n",
      "Epoch: 13; Training Loss: 0.10038; Test Loss: 0.07874\n",
      "Epoch: 14; Training Loss: 0.09316; Test Loss: 0.07423\n",
      "Epoch: 15; Training Loss: 0.08615; Test Loss: 0.07392\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=1.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.58196; Test Loss: 0.43102\n",
      "Epoch: 2; Training Loss: 0.44007; Test Loss: 0.30540\n",
      "Epoch: 3; Training Loss: 0.34795; Test Loss: 0.23289\n",
      "Epoch: 4; Training Loss: 0.28430; Test Loss: 0.18615\n",
      "Epoch: 5; Training Loss: 0.23795; Test Loss: 0.14997\n",
      "Epoch: 6; Training Loss: 0.20259; Test Loss: 0.12395\n",
      "Epoch: 7; Training Loss: 0.17441; Test Loss: 0.10291\n",
      "Epoch: 8; Training Loss: 0.15068; Test Loss: 0.08875\n",
      "Epoch: 9; Training Loss: 0.13147; Test Loss: 0.07602\n",
      "Epoch: 10; Training Loss: 0.11539; Test Loss: 0.06449\n",
      "Epoch: 11; Training Loss: 0.10193; Test Loss: 0.06127\n",
      "Epoch: 12; Training Loss: 0.09035; Test Loss: 0.05363\n",
      "Epoch: 13; Training Loss: 0.08008; Test Loss: 0.05103\n",
      "Epoch: 14; Training Loss: 0.07215; Test Loss: 0.04597\n",
      "Epoch: 15; Training Loss: 0.06541; Test Loss: 0.04193\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=1.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.59191; Test Loss: 0.44548\n",
      "Epoch: 2; Training Loss: 0.44755; Test Loss: 0.31622\n",
      "Epoch: 3; Training Loss: 0.35304; Test Loss: 0.24036\n",
      "Epoch: 4; Training Loss: 0.28822; Test Loss: 0.19058\n",
      "Epoch: 5; Training Loss: 0.24056; Test Loss: 0.15480\n",
      "Epoch: 6; Training Loss: 0.20475; Test Loss: 0.12765\n",
      "Epoch: 7; Training Loss: 0.17539; Test Loss: 0.10633\n",
      "Epoch: 8; Training Loss: 0.15174; Test Loss: 0.09050\n",
      "Epoch: 9; Training Loss: 0.13314; Test Loss: 0.07820\n",
      "Epoch: 10; Training Loss: 0.11586; Test Loss: 0.06840\n",
      "Epoch: 11; Training Loss: 0.10287; Test Loss: 0.06096\n",
      "Epoch: 12; Training Loss: 0.09036; Test Loss: 0.05526\n",
      "Epoch: 13; Training Loss: 0.08130; Test Loss: 0.05057\n",
      "Epoch: 14; Training Loss: 0.07215; Test Loss: 0.04706\n",
      "Epoch: 15; Training Loss: 0.06430; Test Loss: 0.04517\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=1.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.59749; Test Loss: 0.45222\n",
      "Epoch: 2; Training Loss: 0.45140; Test Loss: 0.31910\n",
      "Epoch: 3; Training Loss: 0.35529; Test Loss: 0.24250\n",
      "Epoch: 4; Training Loss: 0.28970; Test Loss: 0.19170\n",
      "Epoch: 5; Training Loss: 0.24187; Test Loss: 0.15463\n",
      "Epoch: 6; Training Loss: 0.20604; Test Loss: 0.12602\n",
      "Epoch: 7; Training Loss: 0.17653; Test Loss: 0.10415\n",
      "Epoch: 8; Training Loss: 0.15221; Test Loss: 0.08892\n",
      "Epoch: 9; Training Loss: 0.13354; Test Loss: 0.07707\n",
      "Epoch: 10; Training Loss: 0.11703; Test Loss: 0.06756\n",
      "Epoch: 11; Training Loss: 0.10319; Test Loss: 0.06024\n",
      "Epoch: 12; Training Loss: 0.09204; Test Loss: 0.05329\n",
      "Epoch: 13; Training Loss: 0.08133; Test Loss: 0.05106\n",
      "Epoch: 14; Training Loss: 0.07349; Test Loss: 0.04696\n",
      "Epoch: 15; Training Loss: 0.06566; Test Loss: 0.04385\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=1.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.59938; Test Loss: 0.45440\n",
      "Epoch: 2; Training Loss: 0.45307; Test Loss: 0.32081\n",
      "Epoch: 3; Training Loss: 0.35669; Test Loss: 0.24224\n",
      "Epoch: 4; Training Loss: 0.29037; Test Loss: 0.19110\n",
      "Epoch: 5; Training Loss: 0.24292; Test Loss: 0.15477\n",
      "Epoch: 6; Training Loss: 0.20604; Test Loss: 0.12759\n",
      "Epoch: 7; Training Loss: 0.17724; Test Loss: 0.10609\n",
      "Epoch: 8; Training Loss: 0.15316; Test Loss: 0.09027\n",
      "Epoch: 9; Training Loss: 0.13419; Test Loss: 0.07737\n",
      "Epoch: 10; Training Loss: 0.11690; Test Loss: 0.06855\n",
      "Epoch: 11; Training Loss: 0.10343; Test Loss: 0.05841\n",
      "Epoch: 12; Training Loss: 0.09149; Test Loss: 0.05465\n",
      "Epoch: 13; Training Loss: 0.08162; Test Loss: 0.04865\n",
      "Epoch: 14; Training Loss: 0.07276; Test Loss: 0.04450\n",
      "Epoch: 15; Training Loss: 0.06612; Test Loss: 0.04138\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 10; lambda_c: 0.00; neg_count: 1\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.58792; Test Loss: 0.44246\n",
      "Epoch: 2; Training Loss: 0.44583; Test Loss: 0.29600\n",
      "Epoch: 3; Training Loss: 0.33977; Test Loss: 0.20649\n",
      "Epoch: 4; Training Loss: 0.26357; Test Loss: 0.15805\n",
      "Epoch: 5; Training Loss: 0.20550; Test Loss: 0.12127\n",
      "Epoch: 6; Training Loss: 0.16813; Test Loss: 0.09818\n",
      "Epoch: 7; Training Loss: 0.13736; Test Loss: 0.08802\n",
      "Epoch: 8; Training Loss: 0.11723; Test Loss: 0.07349\n",
      "Epoch: 9; Training Loss: 0.09797; Test Loss: 0.07126\n",
      "Epoch: 10; Training Loss: 0.08476; Test Loss: 0.06288\n",
      "Epoch: 11; Training Loss: 0.07447; Test Loss: 0.06958\n",
      "Epoch: 12; Training Loss: 0.06933; Test Loss: 0.05387\n",
      "Epoch: 13; Training Loss: 0.05972; Test Loss: 0.04387\n",
      "Epoch: 14; Training Loss: 0.05333; Test Loss: 0.04974\n",
      "Epoch: 15; Training Loss: 0.05110; Test Loss: 0.04691\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.59221; Test Loss: 0.44987\n",
      "Epoch: 2; Training Loss: 0.44780; Test Loss: 0.30282\n",
      "Epoch: 3; Training Loss: 0.34445; Test Loss: 0.21131\n",
      "Epoch: 4; Training Loss: 0.26221; Test Loss: 0.15821\n",
      "Epoch: 5; Training Loss: 0.20914; Test Loss: 0.12000\n",
      "Epoch: 6; Training Loss: 0.16878; Test Loss: 0.10666\n",
      "Epoch: 7; Training Loss: 0.13821; Test Loss: 0.07213\n",
      "Epoch: 8; Training Loss: 0.11744; Test Loss: 0.06799\n",
      "Epoch: 9; Training Loss: 0.09774; Test Loss: 0.06944\n",
      "Epoch: 10; Training Loss: 0.08645; Test Loss: 0.05197\n",
      "Epoch: 11; Training Loss: 0.07681; Test Loss: 0.05103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12; Training Loss: 0.06760; Test Loss: 0.04730\n",
      "Epoch: 13; Training Loss: 0.06251; Test Loss: 0.04526\n",
      "Epoch: 14; Training Loss: 0.05585; Test Loss: 0.06167\n",
      "Epoch: 15; Training Loss: 0.05446; Test Loss: 0.04565\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.56922; Test Loss: 0.42395\n",
      "Epoch: 2; Training Loss: 0.42981; Test Loss: 0.28746\n",
      "Epoch: 3; Training Loss: 0.32805; Test Loss: 0.20496\n",
      "Epoch: 4; Training Loss: 0.25637; Test Loss: 0.15073\n",
      "Epoch: 5; Training Loss: 0.20094; Test Loss: 0.12610\n",
      "Epoch: 6; Training Loss: 0.16136; Test Loss: 0.09740\n",
      "Epoch: 7; Training Loss: 0.13713; Test Loss: 0.07486\n",
      "Epoch: 8; Training Loss: 0.11273; Test Loss: 0.07014\n",
      "Epoch: 9; Training Loss: 0.09618; Test Loss: 0.05721\n",
      "Epoch: 10; Training Loss: 0.08412; Test Loss: 0.05962\n",
      "Epoch: 11; Training Loss: 0.07331; Test Loss: 0.04698\n",
      "Epoch: 12; Training Loss: 0.06473; Test Loss: 0.04930\n",
      "Epoch: 13; Training Loss: 0.06009; Test Loss: 0.04408\n",
      "Epoch: 14; Training Loss: 0.05459; Test Loss: 0.05110\n",
      "Epoch: 15; Training Loss: 0.04911; Test Loss: 0.05272\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.58692; Test Loss: 0.43787\n",
      "Epoch: 2; Training Loss: 0.44416; Test Loss: 0.30111\n",
      "Epoch: 3; Training Loss: 0.34151; Test Loss: 0.20923\n",
      "Epoch: 4; Training Loss: 0.26224; Test Loss: 0.14978\n",
      "Epoch: 5; Training Loss: 0.20851; Test Loss: 0.11944\n",
      "Epoch: 6; Training Loss: 0.16659; Test Loss: 0.09150\n",
      "Epoch: 7; Training Loss: 0.13877; Test Loss: 0.08086\n",
      "Epoch: 8; Training Loss: 0.11846; Test Loss: 0.07673\n",
      "Epoch: 9; Training Loss: 0.09777; Test Loss: 0.04915\n",
      "Epoch: 10; Training Loss: 0.08564; Test Loss: 0.05611\n",
      "Epoch: 11; Training Loss: 0.07393; Test Loss: 0.04936\n",
      "Epoch: 12; Training Loss: 0.06762; Test Loss: 0.05017\n",
      "Epoch: 13; Training Loss: 0.06109; Test Loss: 0.04663\n",
      "Epoch: 14; Training Loss: 0.05542; Test Loss: 0.04361\n",
      "Epoch: 15; Training Loss: 0.05334; Test Loss: 0.04331\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.58767; Test Loss: 0.44090\n",
      "Epoch: 2; Training Loss: 0.44445; Test Loss: 0.29708\n",
      "Epoch: 3; Training Loss: 0.33816; Test Loss: 0.21123\n",
      "Epoch: 4; Training Loss: 0.26203; Test Loss: 0.15657\n",
      "Epoch: 5; Training Loss: 0.20191; Test Loss: 0.11810\n",
      "Epoch: 6; Training Loss: 0.16887; Test Loss: 0.09069\n",
      "Epoch: 7; Training Loss: 0.13841; Test Loss: 0.07343\n",
      "Epoch: 8; Training Loss: 0.11499; Test Loss: 0.06267\n",
      "Epoch: 9; Training Loss: 0.09871; Test Loss: 0.06169\n",
      "Epoch: 10; Training Loss: 0.08426; Test Loss: 0.05786\n",
      "Epoch: 11; Training Loss: 0.07935; Test Loss: 0.05816\n",
      "Epoch: 12; Training Loss: 0.06618; Test Loss: 0.03921\n",
      "Epoch: 13; Training Loss: 0.05904; Test Loss: 0.04802\n",
      "Epoch: 14; Training Loss: 0.05528; Test Loss: 0.04878\n",
      "Epoch: 15; Training Loss: 0.05021; Test Loss: 0.05213\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 10; lambda_c: 0.00; neg_count: 5\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.55285; Test Loss: 0.37357\n",
      "Epoch: 2; Training Loss: 0.38954; Test Loss: 0.24327\n",
      "Epoch: 3; Training Loss: 0.29633; Test Loss: 0.16915\n",
      "Epoch: 4; Training Loss: 0.23025; Test Loss: 0.11990\n",
      "Epoch: 5; Training Loss: 0.18144; Test Loss: 0.09367\n",
      "Epoch: 6; Training Loss: 0.14806; Test Loss: 0.07718\n",
      "Epoch: 7; Training Loss: 0.12295; Test Loss: 0.06194\n",
      "Epoch: 8; Training Loss: 0.10242; Test Loss: 0.05351\n",
      "Epoch: 9; Training Loss: 0.08791; Test Loss: 0.04674\n",
      "Epoch: 10; Training Loss: 0.07610; Test Loss: 0.04317\n",
      "Epoch: 11; Training Loss: 0.06544; Test Loss: 0.03959\n",
      "Epoch: 12; Training Loss: 0.05678; Test Loss: 0.03584\n",
      "Epoch: 13; Training Loss: 0.05143; Test Loss: 0.03863\n",
      "Epoch: 14; Training Loss: 0.04812; Test Loss: 0.03273\n",
      "Epoch: 15; Training Loss: 0.04459; Test Loss: 0.03518\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.54472; Test Loss: 0.36455\n",
      "Epoch: 2; Training Loss: 0.38488; Test Loss: 0.23798\n",
      "Epoch: 3; Training Loss: 0.28987; Test Loss: 0.16623\n",
      "Epoch: 4; Training Loss: 0.22687; Test Loss: 0.12095\n",
      "Epoch: 5; Training Loss: 0.18262; Test Loss: 0.09131\n",
      "Epoch: 6; Training Loss: 0.14630; Test Loss: 0.07200\n",
      "Epoch: 7; Training Loss: 0.12123; Test Loss: 0.06441\n",
      "Epoch: 8; Training Loss: 0.10278; Test Loss: 0.04946\n",
      "Epoch: 9; Training Loss: 0.08690; Test Loss: 0.04692\n",
      "Epoch: 10; Training Loss: 0.07550; Test Loss: 0.04053\n",
      "Epoch: 11; Training Loss: 0.06565; Test Loss: 0.03841\n",
      "Epoch: 12; Training Loss: 0.05945; Test Loss: 0.03360\n",
      "Epoch: 13; Training Loss: 0.05390; Test Loss: 0.03350\n",
      "Epoch: 14; Training Loss: 0.04794; Test Loss: 0.03150\n",
      "Epoch: 15; Training Loss: 0.04241; Test Loss: 0.03122\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.54258; Test Loss: 0.36282\n",
      "Epoch: 2; Training Loss: 0.38499; Test Loss: 0.23746\n",
      "Epoch: 3; Training Loss: 0.28932; Test Loss: 0.16705\n",
      "Epoch: 4; Training Loss: 0.22530; Test Loss: 0.12138\n",
      "Epoch: 5; Training Loss: 0.18003; Test Loss: 0.09183\n",
      "Epoch: 6; Training Loss: 0.14616; Test Loss: 0.07614\n",
      "Epoch: 7; Training Loss: 0.12094; Test Loss: 0.06273\n",
      "Epoch: 8; Training Loss: 0.10109; Test Loss: 0.05515\n",
      "Epoch: 9; Training Loss: 0.08856; Test Loss: 0.05183\n",
      "Epoch: 10; Training Loss: 0.07506; Test Loss: 0.04301\n",
      "Epoch: 11; Training Loss: 0.06608; Test Loss: 0.04133\n",
      "Epoch: 12; Training Loss: 0.05789; Test Loss: 0.03518\n",
      "Epoch: 13; Training Loss: 0.05061; Test Loss: 0.03549\n",
      "Epoch: 14; Training Loss: 0.04635; Test Loss: 0.03687\n",
      "Epoch: 15; Training Loss: 0.04259; Test Loss: 0.03379\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.54512; Test Loss: 0.36489\n",
      "Epoch: 2; Training Loss: 0.38492; Test Loss: 0.24021\n",
      "Epoch: 3; Training Loss: 0.29122; Test Loss: 0.16298\n",
      "Epoch: 4; Training Loss: 0.22691; Test Loss: 0.12032\n",
      "Epoch: 5; Training Loss: 0.18000; Test Loss: 0.09067\n",
      "Epoch: 6; Training Loss: 0.14848; Test Loss: 0.07156\n",
      "Epoch: 7; Training Loss: 0.12247; Test Loss: 0.05984\n",
      "Epoch: 8; Training Loss: 0.10235; Test Loss: 0.05286\n",
      "Epoch: 9; Training Loss: 0.08750; Test Loss: 0.04520\n",
      "Epoch: 10; Training Loss: 0.07493; Test Loss: 0.04281\n",
      "Epoch: 11; Training Loss: 0.06605; Test Loss: 0.03546\n",
      "Epoch: 12; Training Loss: 0.05785; Test Loss: 0.03654\n",
      "Epoch: 13; Training Loss: 0.05203; Test Loss: 0.03148\n",
      "Epoch: 14; Training Loss: 0.04625; Test Loss: 0.03031\n",
      "Epoch: 15; Training Loss: 0.04405; Test Loss: 0.02896\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.55256; Test Loss: 0.37546\n",
      "Epoch: 2; Training Loss: 0.39145; Test Loss: 0.24256\n",
      "Epoch: 3; Training Loss: 0.29504; Test Loss: 0.16925\n",
      "Epoch: 4; Training Loss: 0.23020; Test Loss: 0.12251\n",
      "Epoch: 5; Training Loss: 0.18200; Test Loss: 0.08942\n",
      "Epoch: 6; Training Loss: 0.14822; Test Loss: 0.07158\n",
      "Epoch: 7; Training Loss: 0.12167; Test Loss: 0.06218\n",
      "Epoch: 8; Training Loss: 0.10151; Test Loss: 0.05425\n",
      "Epoch: 9; Training Loss: 0.08643; Test Loss: 0.05030\n",
      "Epoch: 10; Training Loss: 0.07692; Test Loss: 0.04208\n",
      "Epoch: 11; Training Loss: 0.06547; Test Loss: 0.04008\n",
      "Epoch: 12; Training Loss: 0.05823; Test Loss: 0.03447\n",
      "Epoch: 13; Training Loss: 0.05196; Test Loss: 0.03364\n",
      "Epoch: 14; Training Loss: 0.04791; Test Loss: 0.03370\n",
      "Epoch: 15; Training Loss: 0.04211; Test Loss: 0.02878\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 10; lambda_c: 0.00; neg_count: 10\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.00; epochs=15; negative_count=10; synonym_count=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Training Loss: 0.51548; Test Loss: 0.31326\n",
      "Epoch: 2; Training Loss: 0.34247; Test Loss: 0.19616\n",
      "Epoch: 3; Training Loss: 0.25060; Test Loss: 0.13577\n",
      "Epoch: 4; Training Loss: 0.19348; Test Loss: 0.09965\n",
      "Epoch: 5; Training Loss: 0.15331; Test Loss: 0.07417\n",
      "Epoch: 6; Training Loss: 0.12495; Test Loss: 0.05912\n",
      "Epoch: 7; Training Loss: 0.10214; Test Loss: 0.05012\n",
      "Epoch: 8; Training Loss: 0.08674; Test Loss: 0.04112\n",
      "Epoch: 9; Training Loss: 0.07399; Test Loss: 0.03764\n",
      "Epoch: 10; Training Loss: 0.06369; Test Loss: 0.03363\n",
      "Epoch: 11; Training Loss: 0.05560; Test Loss: 0.03022\n",
      "Epoch: 12; Training Loss: 0.04925; Test Loss: 0.02740\n",
      "Epoch: 13; Training Loss: 0.04455; Test Loss: 0.02576\n",
      "Epoch: 14; Training Loss: 0.03923; Test Loss: 0.02494\n",
      "Epoch: 15; Training Loss: 0.03562; Test Loss: 0.02452\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.54133; Test Loss: 0.34000\n",
      "Epoch: 2; Training Loss: 0.35773; Test Loss: 0.20814\n",
      "Epoch: 3; Training Loss: 0.26044; Test Loss: 0.14263\n",
      "Epoch: 4; Training Loss: 0.19858; Test Loss: 0.10267\n",
      "Epoch: 5; Training Loss: 0.15745; Test Loss: 0.07680\n",
      "Epoch: 6; Training Loss: 0.12724; Test Loss: 0.06063\n",
      "Epoch: 7; Training Loss: 0.10560; Test Loss: 0.04949\n",
      "Epoch: 8; Training Loss: 0.08790; Test Loss: 0.04165\n",
      "Epoch: 9; Training Loss: 0.07373; Test Loss: 0.03738\n",
      "Epoch: 10; Training Loss: 0.06404; Test Loss: 0.03097\n",
      "Epoch: 11; Training Loss: 0.05682; Test Loss: 0.02918\n",
      "Epoch: 12; Training Loss: 0.05011; Test Loss: 0.02766\n",
      "Epoch: 13; Training Loss: 0.04451; Test Loss: 0.02665\n",
      "Epoch: 14; Training Loss: 0.03956; Test Loss: 0.02830\n",
      "Epoch: 15; Training Loss: 0.03561; Test Loss: 0.02558\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.52550; Test Loss: 0.32301\n",
      "Epoch: 2; Training Loss: 0.34743; Test Loss: 0.20184\n",
      "Epoch: 3; Training Loss: 0.25392; Test Loss: 0.14048\n",
      "Epoch: 4; Training Loss: 0.19621; Test Loss: 0.10083\n",
      "Epoch: 5; Training Loss: 0.15639; Test Loss: 0.07739\n",
      "Epoch: 6; Training Loss: 0.12488; Test Loss: 0.06055\n",
      "Epoch: 7; Training Loss: 0.10323; Test Loss: 0.05004\n",
      "Epoch: 8; Training Loss: 0.08744; Test Loss: 0.04350\n",
      "Epoch: 9; Training Loss: 0.07431; Test Loss: 0.04013\n",
      "Epoch: 10; Training Loss: 0.06389; Test Loss: 0.03432\n",
      "Epoch: 11; Training Loss: 0.05496; Test Loss: 0.03152\n",
      "Epoch: 12; Training Loss: 0.04887; Test Loss: 0.03091\n",
      "Epoch: 13; Training Loss: 0.04424; Test Loss: 0.02776\n",
      "Epoch: 14; Training Loss: 0.03917; Test Loss: 0.02734\n",
      "Epoch: 15; Training Loss: 0.03464; Test Loss: 0.02619\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.52862; Test Loss: 0.32748\n",
      "Epoch: 2; Training Loss: 0.34982; Test Loss: 0.20212\n",
      "Epoch: 3; Training Loss: 0.25485; Test Loss: 0.13932\n",
      "Epoch: 4; Training Loss: 0.19699; Test Loss: 0.09933\n",
      "Epoch: 5; Training Loss: 0.15562; Test Loss: 0.07578\n",
      "Epoch: 6; Training Loss: 0.12641; Test Loss: 0.05878\n",
      "Epoch: 7; Training Loss: 0.10363; Test Loss: 0.04806\n",
      "Epoch: 8; Training Loss: 0.08741; Test Loss: 0.04051\n",
      "Epoch: 9; Training Loss: 0.07422; Test Loss: 0.03667\n",
      "Epoch: 10; Training Loss: 0.06515; Test Loss: 0.03335\n",
      "Epoch: 11; Training Loss: 0.05620; Test Loss: 0.02993\n",
      "Epoch: 12; Training Loss: 0.04967; Test Loss: 0.02465\n",
      "Epoch: 13; Training Loss: 0.04431; Test Loss: 0.02273\n",
      "Epoch: 14; Training Loss: 0.04050; Test Loss: 0.02435\n",
      "Epoch: 15; Training Loss: 0.03622; Test Loss: 0.02204\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.52298; Test Loss: 0.32033\n",
      "Epoch: 2; Training Loss: 0.34563; Test Loss: 0.19990\n",
      "Epoch: 3; Training Loss: 0.25298; Test Loss: 0.13742\n",
      "Epoch: 4; Training Loss: 0.19507; Test Loss: 0.09855\n",
      "Epoch: 5; Training Loss: 0.15542; Test Loss: 0.07544\n",
      "Epoch: 6; Training Loss: 0.12573; Test Loss: 0.05804\n",
      "Epoch: 7; Training Loss: 0.10324; Test Loss: 0.04887\n",
      "Epoch: 8; Training Loss: 0.08741; Test Loss: 0.04177\n",
      "Epoch: 9; Training Loss: 0.07432; Test Loss: 0.03807\n",
      "Epoch: 10; Training Loss: 0.06366; Test Loss: 0.03385\n",
      "Epoch: 11; Training Loss: 0.05548; Test Loss: 0.02876\n",
      "Epoch: 12; Training Loss: 0.04925; Test Loss: 0.02787\n",
      "Epoch: 13; Training Loss: 0.04326; Test Loss: 0.02426\n",
      "Epoch: 14; Training Loss: 0.03954; Test Loss: 0.02465\n",
      "Epoch: 15; Training Loss: 0.03629; Test Loss: 0.02541\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 10; lambda_c: 0.10; neg_count: 1\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.10; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.58269; Test Loss: 0.43750\n",
      "Epoch: 2; Training Loss: 0.44141; Test Loss: 0.30134\n",
      "Epoch: 3; Training Loss: 0.33771; Test Loss: 0.20901\n",
      "Epoch: 4; Training Loss: 0.26218; Test Loss: 0.15637\n",
      "Epoch: 5; Training Loss: 0.20632; Test Loss: 0.12105\n",
      "Epoch: 6; Training Loss: 0.16640; Test Loss: 0.09796\n",
      "Epoch: 7; Training Loss: 0.13875; Test Loss: 0.08405\n",
      "Epoch: 8; Training Loss: 0.11507; Test Loss: 0.08193\n",
      "Epoch: 9; Training Loss: 0.09986; Test Loss: 0.06963\n",
      "Epoch: 10; Training Loss: 0.08666; Test Loss: 0.05531\n",
      "Epoch: 11; Training Loss: 0.07698; Test Loss: 0.05141\n",
      "Epoch: 12; Training Loss: 0.06539; Test Loss: 0.05237\n",
      "Epoch: 13; Training Loss: 0.06404; Test Loss: 0.04967\n",
      "Epoch: 14; Training Loss: 0.05521; Test Loss: 0.03811\n",
      "Epoch: 15; Training Loss: 0.04925; Test Loss: 0.05646\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.10; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.58990; Test Loss: 0.44141\n",
      "Epoch: 2; Training Loss: 0.44619; Test Loss: 0.30234\n",
      "Epoch: 3; Training Loss: 0.34196; Test Loss: 0.21483\n",
      "Epoch: 4; Training Loss: 0.26583; Test Loss: 0.15800\n",
      "Epoch: 5; Training Loss: 0.20324; Test Loss: 0.11904\n",
      "Epoch: 6; Training Loss: 0.16791; Test Loss: 0.09776\n",
      "Epoch: 7; Training Loss: 0.13799; Test Loss: 0.08520\n",
      "Epoch: 8; Training Loss: 0.11689; Test Loss: 0.06958\n",
      "Epoch: 9; Training Loss: 0.10032; Test Loss: 0.06820\n",
      "Epoch: 10; Training Loss: 0.08807; Test Loss: 0.06144\n",
      "Epoch: 11; Training Loss: 0.07829; Test Loss: 0.05055\n",
      "Epoch: 12; Training Loss: 0.06861; Test Loss: 0.05936\n",
      "Epoch: 13; Training Loss: 0.06279; Test Loss: 0.03959\n",
      "Epoch: 14; Training Loss: 0.05706; Test Loss: 0.03393\n",
      "Epoch: 15; Training Loss: 0.05268; Test Loss: 0.04376\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.10; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.58740; Test Loss: 0.44328\n",
      "Epoch: 2; Training Loss: 0.44495; Test Loss: 0.30578\n",
      "Epoch: 3; Training Loss: 0.33936; Test Loss: 0.21630\n",
      "Epoch: 4; Training Loss: 0.26444; Test Loss: 0.15761\n",
      "Epoch: 5; Training Loss: 0.20544; Test Loss: 0.11907\n",
      "Epoch: 6; Training Loss: 0.16523; Test Loss: 0.10077\n",
      "Epoch: 7; Training Loss: 0.13563; Test Loss: 0.07493\n",
      "Epoch: 8; Training Loss: 0.11338; Test Loss: 0.06696\n",
      "Epoch: 9; Training Loss: 0.09871; Test Loss: 0.06193\n",
      "Epoch: 10; Training Loss: 0.08478; Test Loss: 0.05965\n",
      "Epoch: 11; Training Loss: 0.07763; Test Loss: 0.05380\n",
      "Epoch: 12; Training Loss: 0.06428; Test Loss: 0.04235\n",
      "Epoch: 13; Training Loss: 0.06039; Test Loss: 0.04877\n",
      "Epoch: 14; Training Loss: 0.05364; Test Loss: 0.04569\n",
      "Epoch: 15; Training Loss: 0.05077; Test Loss: 0.06999\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.10; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.58418; Test Loss: 0.43491\n",
      "Epoch: 2; Training Loss: 0.44172; Test Loss: 0.29849\n",
      "Epoch: 3; Training Loss: 0.33713; Test Loss: 0.21457\n",
      "Epoch: 4; Training Loss: 0.26173; Test Loss: 0.16292\n",
      "Epoch: 5; Training Loss: 0.20488; Test Loss: 0.12145\n",
      "Epoch: 6; Training Loss: 0.16618; Test Loss: 0.10768\n",
      "Epoch: 7; Training Loss: 0.13939; Test Loss: 0.08211\n",
      "Epoch: 8; Training Loss: 0.11576; Test Loss: 0.06641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9; Training Loss: 0.09609; Test Loss: 0.06428\n",
      "Epoch: 10; Training Loss: 0.08500; Test Loss: 0.06067\n",
      "Epoch: 11; Training Loss: 0.07447; Test Loss: 0.06379\n",
      "Epoch: 12; Training Loss: 0.06602; Test Loss: 0.04749\n",
      "Epoch: 13; Training Loss: 0.06068; Test Loss: 0.04123\n",
      "Epoch: 14; Training Loss: 0.05752; Test Loss: 0.04034\n",
      "Epoch: 15; Training Loss: 0.05286; Test Loss: 0.04328\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.10; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.59786; Test Loss: 0.45578\n",
      "Epoch: 2; Training Loss: 0.45432; Test Loss: 0.31151\n",
      "Epoch: 3; Training Loss: 0.34736; Test Loss: 0.21686\n",
      "Epoch: 4; Training Loss: 0.26687; Test Loss: 0.15630\n",
      "Epoch: 5; Training Loss: 0.21123; Test Loss: 0.12346\n",
      "Epoch: 6; Training Loss: 0.17249; Test Loss: 0.09716\n",
      "Epoch: 7; Training Loss: 0.13730; Test Loss: 0.08666\n",
      "Epoch: 8; Training Loss: 0.11852; Test Loss: 0.07046\n",
      "Epoch: 9; Training Loss: 0.09854; Test Loss: 0.05773\n",
      "Epoch: 10; Training Loss: 0.08675; Test Loss: 0.05353\n",
      "Epoch: 11; Training Loss: 0.07595; Test Loss: 0.05073\n",
      "Epoch: 12; Training Loss: 0.06895; Test Loss: 0.05295\n",
      "Epoch: 13; Training Loss: 0.06520; Test Loss: 0.05768\n",
      "Epoch: 14; Training Loss: 0.05764; Test Loss: 0.03419\n",
      "Epoch: 15; Training Loss: 0.05232; Test Loss: 0.03731\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 10; lambda_c: 0.10; neg_count: 5\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.10; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.55056; Test Loss: 0.37034\n",
      "Epoch: 2; Training Loss: 0.38943; Test Loss: 0.24216\n",
      "Epoch: 3; Training Loss: 0.29399; Test Loss: 0.17025\n",
      "Epoch: 4; Training Loss: 0.22933; Test Loss: 0.12222\n",
      "Epoch: 5; Training Loss: 0.18366; Test Loss: 0.09191\n",
      "Epoch: 6; Training Loss: 0.14954; Test Loss: 0.07402\n",
      "Epoch: 7; Training Loss: 0.12181; Test Loss: 0.06392\n",
      "Epoch: 8; Training Loss: 0.10237; Test Loss: 0.05305\n",
      "Epoch: 9; Training Loss: 0.08758; Test Loss: 0.04848\n",
      "Epoch: 10; Training Loss: 0.07540; Test Loss: 0.04362\n",
      "Epoch: 11; Training Loss: 0.06634; Test Loss: 0.04537\n",
      "Epoch: 12; Training Loss: 0.05948; Test Loss: 0.04002\n",
      "Epoch: 13; Training Loss: 0.05292; Test Loss: 0.03533\n",
      "Epoch: 14; Training Loss: 0.04578; Test Loss: 0.03571\n",
      "Epoch: 15; Training Loss: 0.04308; Test Loss: 0.03074\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.10; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.56399; Test Loss: 0.38737\n",
      "Epoch: 2; Training Loss: 0.39976; Test Loss: 0.25217\n",
      "Epoch: 3; Training Loss: 0.30105; Test Loss: 0.17250\n",
      "Epoch: 4; Training Loss: 0.23348; Test Loss: 0.12482\n",
      "Epoch: 5; Training Loss: 0.18533; Test Loss: 0.09390\n",
      "Epoch: 6; Training Loss: 0.14945; Test Loss: 0.07373\n",
      "Epoch: 7; Training Loss: 0.12406; Test Loss: 0.06490\n",
      "Epoch: 8; Training Loss: 0.10432; Test Loss: 0.05232\n",
      "Epoch: 9; Training Loss: 0.08965; Test Loss: 0.04880\n",
      "Epoch: 10; Training Loss: 0.07632; Test Loss: 0.04049\n",
      "Epoch: 11; Training Loss: 0.06620; Test Loss: 0.03956\n",
      "Epoch: 12; Training Loss: 0.05894; Test Loss: 0.03539\n",
      "Epoch: 13; Training Loss: 0.05308; Test Loss: 0.03030\n",
      "Epoch: 14; Training Loss: 0.04824; Test Loss: 0.02947\n",
      "Epoch: 15; Training Loss: 0.04310; Test Loss: 0.03047\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.10; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.54512; Test Loss: 0.36692\n",
      "Epoch: 2; Training Loss: 0.38436; Test Loss: 0.23970\n",
      "Epoch: 3; Training Loss: 0.29160; Test Loss: 0.16777\n",
      "Epoch: 4; Training Loss: 0.22869; Test Loss: 0.12204\n",
      "Epoch: 5; Training Loss: 0.18048; Test Loss: 0.09294\n",
      "Epoch: 6; Training Loss: 0.14763; Test Loss: 0.07731\n",
      "Epoch: 7; Training Loss: 0.12305; Test Loss: 0.06298\n",
      "Epoch: 8; Training Loss: 0.10163; Test Loss: 0.05388\n",
      "Epoch: 9; Training Loss: 0.08546; Test Loss: 0.04774\n",
      "Epoch: 10; Training Loss: 0.07557; Test Loss: 0.04542\n",
      "Epoch: 11; Training Loss: 0.06646; Test Loss: 0.04048\n",
      "Epoch: 12; Training Loss: 0.05880; Test Loss: 0.03736\n",
      "Epoch: 13; Training Loss: 0.05152; Test Loss: 0.03558\n",
      "Epoch: 14; Training Loss: 0.04699; Test Loss: 0.03581\n",
      "Epoch: 15; Training Loss: 0.04230; Test Loss: 0.03439\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.10; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.54696; Test Loss: 0.36824\n",
      "Epoch: 2; Training Loss: 0.38866; Test Loss: 0.24183\n",
      "Epoch: 3; Training Loss: 0.29169; Test Loss: 0.16739\n",
      "Epoch: 4; Training Loss: 0.22707; Test Loss: 0.12381\n",
      "Epoch: 5; Training Loss: 0.18113; Test Loss: 0.09423\n",
      "Epoch: 6; Training Loss: 0.14837; Test Loss: 0.07872\n",
      "Epoch: 7; Training Loss: 0.12223; Test Loss: 0.06166\n",
      "Epoch: 8; Training Loss: 0.10254; Test Loss: 0.05165\n",
      "Epoch: 9; Training Loss: 0.08733; Test Loss: 0.04698\n",
      "Epoch: 10; Training Loss: 0.07565; Test Loss: 0.04251\n",
      "Epoch: 11; Training Loss: 0.06591; Test Loss: 0.03709\n",
      "Epoch: 12; Training Loss: 0.05803; Test Loss: 0.03409\n",
      "Epoch: 13; Training Loss: 0.05253; Test Loss: 0.03198\n",
      "Epoch: 14; Training Loss: 0.04887; Test Loss: 0.03142\n",
      "Epoch: 15; Training Loss: 0.04358; Test Loss: 0.03019\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.10; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.54150; Test Loss: 0.36219\n",
      "Epoch: 2; Training Loss: 0.38381; Test Loss: 0.23706\n",
      "Epoch: 3; Training Loss: 0.29034; Test Loss: 0.16516\n",
      "Epoch: 4; Training Loss: 0.22412; Test Loss: 0.11892\n",
      "Epoch: 5; Training Loss: 0.18143; Test Loss: 0.09162\n",
      "Epoch: 6; Training Loss: 0.14641; Test Loss: 0.07150\n",
      "Epoch: 7; Training Loss: 0.12047; Test Loss: 0.06217\n",
      "Epoch: 8; Training Loss: 0.10135; Test Loss: 0.05449\n",
      "Epoch: 9; Training Loss: 0.08742; Test Loss: 0.04676\n",
      "Epoch: 10; Training Loss: 0.07439; Test Loss: 0.04664\n",
      "Epoch: 11; Training Loss: 0.06598; Test Loss: 0.03803\n",
      "Epoch: 12; Training Loss: 0.05843; Test Loss: 0.03601\n",
      "Epoch: 13; Training Loss: 0.05204; Test Loss: 0.02791\n",
      "Epoch: 14; Training Loss: 0.04637; Test Loss: 0.03781\n",
      "Epoch: 15; Training Loss: 0.04306; Test Loss: 0.03370\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 10; lambda_c: 0.10; neg_count: 10\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.10; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.53137; Test Loss: 0.32881\n",
      "Epoch: 2; Training Loss: 0.35107; Test Loss: 0.20426\n",
      "Epoch: 3; Training Loss: 0.25652; Test Loss: 0.13966\n",
      "Epoch: 4; Training Loss: 0.19604; Test Loss: 0.10238\n",
      "Epoch: 5; Training Loss: 0.15528; Test Loss: 0.07601\n",
      "Epoch: 6; Training Loss: 0.12704; Test Loss: 0.06150\n",
      "Epoch: 7; Training Loss: 0.10499; Test Loss: 0.05128\n",
      "Epoch: 8; Training Loss: 0.08774; Test Loss: 0.04268\n",
      "Epoch: 9; Training Loss: 0.07345; Test Loss: 0.03856\n",
      "Epoch: 10; Training Loss: 0.06304; Test Loss: 0.03471\n",
      "Epoch: 11; Training Loss: 0.05555; Test Loss: 0.02955\n",
      "Epoch: 12; Training Loss: 0.04914; Test Loss: 0.02905\n",
      "Epoch: 13; Training Loss: 0.04375; Test Loss: 0.02736\n",
      "Epoch: 14; Training Loss: 0.03892; Test Loss: 0.02571\n",
      "Epoch: 15; Training Loss: 0.03584; Test Loss: 0.02670\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.10; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.52832; Test Loss: 0.32590\n",
      "Epoch: 2; Training Loss: 0.34931; Test Loss: 0.20242\n",
      "Epoch: 3; Training Loss: 0.25490; Test Loss: 0.13929\n",
      "Epoch: 4; Training Loss: 0.19617; Test Loss: 0.09980\n",
      "Epoch: 5; Training Loss: 0.15661; Test Loss: 0.07644\n",
      "Epoch: 6; Training Loss: 0.12581; Test Loss: 0.06062\n",
      "Epoch: 7; Training Loss: 0.10362; Test Loss: 0.04775\n",
      "Epoch: 8; Training Loss: 0.08729; Test Loss: 0.04013\n",
      "Epoch: 9; Training Loss: 0.07473; Test Loss: 0.03545\n",
      "Epoch: 10; Training Loss: 0.06388; Test Loss: 0.03385\n",
      "Epoch: 11; Training Loss: 0.05616; Test Loss: 0.02854\n",
      "Epoch: 12; Training Loss: 0.04947; Test Loss: 0.02534\n",
      "Epoch: 13; Training Loss: 0.04381; Test Loss: 0.02713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14; Training Loss: 0.04042; Test Loss: 0.02514\n",
      "Epoch: 15; Training Loss: 0.03542; Test Loss: 0.02419\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.10; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.52713; Test Loss: 0.32469\n",
      "Epoch: 2; Training Loss: 0.34779; Test Loss: 0.20409\n",
      "Epoch: 3; Training Loss: 0.25463; Test Loss: 0.14039\n",
      "Epoch: 4; Training Loss: 0.19628; Test Loss: 0.10225\n",
      "Epoch: 5; Training Loss: 0.15593; Test Loss: 0.07656\n",
      "Epoch: 6; Training Loss: 0.12616; Test Loss: 0.05999\n",
      "Epoch: 7; Training Loss: 0.10310; Test Loss: 0.05165\n",
      "Epoch: 8; Training Loss: 0.08798; Test Loss: 0.04429\n",
      "Epoch: 9; Training Loss: 0.07367; Test Loss: 0.03667\n",
      "Epoch: 10; Training Loss: 0.06411; Test Loss: 0.03485\n",
      "Epoch: 11; Training Loss: 0.05531; Test Loss: 0.03001\n",
      "Epoch: 12; Training Loss: 0.04960; Test Loss: 0.02851\n",
      "Epoch: 13; Training Loss: 0.04380; Test Loss: 0.02702\n",
      "Epoch: 14; Training Loss: 0.03986; Test Loss: 0.02497\n",
      "Epoch: 15; Training Loss: 0.03515; Test Loss: 0.02487\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.10; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.53300; Test Loss: 0.33180\n",
      "Epoch: 2; Training Loss: 0.35248; Test Loss: 0.20551\n",
      "Epoch: 3; Training Loss: 0.25707; Test Loss: 0.13974\n",
      "Epoch: 4; Training Loss: 0.19801; Test Loss: 0.10012\n",
      "Epoch: 5; Training Loss: 0.15703; Test Loss: 0.07473\n",
      "Epoch: 6; Training Loss: 0.12577; Test Loss: 0.05930\n",
      "Epoch: 7; Training Loss: 0.10449; Test Loss: 0.04964\n",
      "Epoch: 8; Training Loss: 0.08774; Test Loss: 0.04198\n",
      "Epoch: 9; Training Loss: 0.07504; Test Loss: 0.03621\n",
      "Epoch: 10; Training Loss: 0.06401; Test Loss: 0.03249\n",
      "Epoch: 11; Training Loss: 0.05636; Test Loss: 0.02810\n",
      "Epoch: 12; Training Loss: 0.04924; Test Loss: 0.02791\n",
      "Epoch: 13; Training Loss: 0.04360; Test Loss: 0.02599\n",
      "Epoch: 14; Training Loss: 0.04048; Test Loss: 0.02644\n",
      "Epoch: 15; Training Loss: 0.03673; Test Loss: 0.02233\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=0.10; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.53593; Test Loss: 0.33399\n",
      "Epoch: 2; Training Loss: 0.35335; Test Loss: 0.20598\n",
      "Epoch: 3; Training Loss: 0.25812; Test Loss: 0.14056\n",
      "Epoch: 4; Training Loss: 0.19810; Test Loss: 0.10082\n",
      "Epoch: 5; Training Loss: 0.15713; Test Loss: 0.07666\n",
      "Epoch: 6; Training Loss: 0.12707; Test Loss: 0.06000\n",
      "Epoch: 7; Training Loss: 0.10495; Test Loss: 0.04928\n",
      "Epoch: 8; Training Loss: 0.08858; Test Loss: 0.04161\n",
      "Epoch: 9; Training Loss: 0.07446; Test Loss: 0.03532\n",
      "Epoch: 10; Training Loss: 0.06516; Test Loss: 0.03114\n",
      "Epoch: 11; Training Loss: 0.05689; Test Loss: 0.03095\n",
      "Epoch: 12; Training Loss: 0.04951; Test Loss: 0.03011\n",
      "Epoch: 13; Training Loss: 0.04385; Test Loss: 0.02658\n",
      "Epoch: 14; Training Loss: 0.04046; Test Loss: 0.02509\n",
      "Epoch: 15; Training Loss: 0.03563; Test Loss: 0.02543\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 10; lambda_c: 1.00; neg_count: 1\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=1.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.59783; Test Loss: 0.45461\n",
      "Epoch: 2; Training Loss: 0.45265; Test Loss: 0.30852\n",
      "Epoch: 3; Training Loss: 0.34269; Test Loss: 0.21277\n",
      "Epoch: 4; Training Loss: 0.26702; Test Loss: 0.16339\n",
      "Epoch: 5; Training Loss: 0.21064; Test Loss: 0.12019\n",
      "Epoch: 6; Training Loss: 0.17190; Test Loss: 0.09388\n",
      "Epoch: 7; Training Loss: 0.13921; Test Loss: 0.08620\n",
      "Epoch: 8; Training Loss: 0.11473; Test Loss: 0.07247\n",
      "Epoch: 9; Training Loss: 0.10005; Test Loss: 0.07555\n",
      "Epoch: 10; Training Loss: 0.08500; Test Loss: 0.06172\n",
      "Epoch: 11; Training Loss: 0.07882; Test Loss: 0.06132\n",
      "Epoch: 12; Training Loss: 0.07005; Test Loss: 0.04849\n",
      "Epoch: 13; Training Loss: 0.05858; Test Loss: 0.05113\n",
      "Epoch: 14; Training Loss: 0.05684; Test Loss: 0.03540\n",
      "Epoch: 15; Training Loss: 0.05404; Test Loss: 0.04001\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=1.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.59718; Test Loss: 0.45188\n",
      "Epoch: 2; Training Loss: 0.45109; Test Loss: 0.30843\n",
      "Epoch: 3; Training Loss: 0.34441; Test Loss: 0.22109\n",
      "Epoch: 4; Training Loss: 0.26589; Test Loss: 0.16247\n",
      "Epoch: 5; Training Loss: 0.21240; Test Loss: 0.11775\n",
      "Epoch: 6; Training Loss: 0.16967; Test Loss: 0.10413\n",
      "Epoch: 7; Training Loss: 0.13772; Test Loss: 0.08431\n",
      "Epoch: 8; Training Loss: 0.11838; Test Loss: 0.06239\n",
      "Epoch: 9; Training Loss: 0.10102; Test Loss: 0.05742\n",
      "Epoch: 10; Training Loss: 0.08560; Test Loss: 0.05798\n",
      "Epoch: 11; Training Loss: 0.07529; Test Loss: 0.04585\n",
      "Epoch: 12; Training Loss: 0.06727; Test Loss: 0.05193\n",
      "Epoch: 13; Training Loss: 0.06191; Test Loss: 0.04580\n",
      "Epoch: 14; Training Loss: 0.05768; Test Loss: 0.04202\n",
      "Epoch: 15; Training Loss: 0.05170; Test Loss: 0.03523\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=1.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.59310; Test Loss: 0.45114\n",
      "Epoch: 2; Training Loss: 0.44885; Test Loss: 0.31079\n",
      "Epoch: 3; Training Loss: 0.34448; Test Loss: 0.21980\n",
      "Epoch: 4; Training Loss: 0.26922; Test Loss: 0.17001\n",
      "Epoch: 5; Training Loss: 0.21133; Test Loss: 0.12106\n",
      "Epoch: 6; Training Loss: 0.17061; Test Loss: 0.11491\n",
      "Epoch: 7; Training Loss: 0.14077; Test Loss: 0.09190\n",
      "Epoch: 8; Training Loss: 0.11683; Test Loss: 0.07496\n",
      "Epoch: 9; Training Loss: 0.09896; Test Loss: 0.06483\n",
      "Epoch: 10; Training Loss: 0.08896; Test Loss: 0.06496\n",
      "Epoch: 11; Training Loss: 0.07792; Test Loss: 0.06171\n",
      "Epoch: 12; Training Loss: 0.07068; Test Loss: 0.07209\n",
      "Epoch: 13; Training Loss: 0.06438; Test Loss: 0.05210\n",
      "Epoch: 14; Training Loss: 0.05974; Test Loss: 0.04343\n",
      "Epoch: 15; Training Loss: 0.05280; Test Loss: 0.04648\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=1.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.58805; Test Loss: 0.44146\n",
      "Epoch: 2; Training Loss: 0.44668; Test Loss: 0.30242\n",
      "Epoch: 3; Training Loss: 0.33930; Test Loss: 0.21116\n",
      "Epoch: 4; Training Loss: 0.26330; Test Loss: 0.15786\n",
      "Epoch: 5; Training Loss: 0.21104; Test Loss: 0.12172\n",
      "Epoch: 6; Training Loss: 0.16839; Test Loss: 0.10064\n",
      "Epoch: 7; Training Loss: 0.13801; Test Loss: 0.07656\n",
      "Epoch: 8; Training Loss: 0.11846; Test Loss: 0.06719\n",
      "Epoch: 9; Training Loss: 0.10069; Test Loss: 0.06696\n",
      "Epoch: 10; Training Loss: 0.08789; Test Loss: 0.05984\n",
      "Epoch: 11; Training Loss: 0.07221; Test Loss: 0.05487\n",
      "Epoch: 12; Training Loss: 0.06800; Test Loss: 0.04364\n",
      "Epoch: 13; Training Loss: 0.05898; Test Loss: 0.04313\n",
      "Epoch: 14; Training Loss: 0.05743; Test Loss: 0.04051\n",
      "Epoch: 15; Training Loss: 0.05363; Test Loss: 0.04494\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=1.00; epochs=15; negative_count=1; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.59430; Test Loss: 0.45235\n",
      "Epoch: 2; Training Loss: 0.45464; Test Loss: 0.31093\n",
      "Epoch: 3; Training Loss: 0.35093; Test Loss: 0.22019\n",
      "Epoch: 4; Training Loss: 0.27651; Test Loss: 0.16116\n",
      "Epoch: 5; Training Loss: 0.22036; Test Loss: 0.13482\n",
      "Epoch: 6; Training Loss: 0.17895; Test Loss: 0.10901\n",
      "Epoch: 7; Training Loss: 0.14825; Test Loss: 0.08857\n",
      "Epoch: 8; Training Loss: 0.12608; Test Loss: 0.08039\n",
      "Epoch: 9; Training Loss: 0.10849; Test Loss: 0.06932\n",
      "Epoch: 10; Training Loss: 0.09727; Test Loss: 0.06826\n",
      "Epoch: 11; Training Loss: 0.08114; Test Loss: 0.05881\n",
      "Epoch: 12; Training Loss: 0.07457; Test Loss: 0.04609\n",
      "Epoch: 13; Training Loss: 0.06704; Test Loss: 0.05211\n",
      "Epoch: 14; Training Loss: 0.06101; Test Loss: 0.04460\n",
      "Epoch: 15; Training Loss: 0.05437; Test Loss: 0.04335\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 10; lambda_c: 1.00; neg_count: 5\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=1.00; epochs=15; negative_count=5; synonym_count=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Training Loss: 0.55865; Test Loss: 0.37528\n",
      "Epoch: 2; Training Loss: 0.39298; Test Loss: 0.24730\n",
      "Epoch: 3; Training Loss: 0.29764; Test Loss: 0.16995\n",
      "Epoch: 4; Training Loss: 0.23041; Test Loss: 0.12348\n",
      "Epoch: 5; Training Loss: 0.18360; Test Loss: 0.09519\n",
      "Epoch: 6; Training Loss: 0.14744; Test Loss: 0.07687\n",
      "Epoch: 7; Training Loss: 0.12334; Test Loss: 0.06241\n",
      "Epoch: 8; Training Loss: 0.10183; Test Loss: 0.05831\n",
      "Epoch: 9; Training Loss: 0.08681; Test Loss: 0.04421\n",
      "Epoch: 10; Training Loss: 0.07671; Test Loss: 0.04172\n",
      "Epoch: 11; Training Loss: 0.06682; Test Loss: 0.04093\n",
      "Epoch: 12; Training Loss: 0.05856; Test Loss: 0.03636\n",
      "Epoch: 13; Training Loss: 0.05290; Test Loss: 0.03387\n",
      "Epoch: 14; Training Loss: 0.04700; Test Loss: 0.03565\n",
      "Epoch: 15; Training Loss: 0.04250; Test Loss: 0.03107\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=1.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.57169; Test Loss: 0.39038\n",
      "Epoch: 2; Training Loss: 0.40516; Test Loss: 0.25399\n",
      "Epoch: 3; Training Loss: 0.30609; Test Loss: 0.18007\n",
      "Epoch: 4; Training Loss: 0.24078; Test Loss: 0.13497\n",
      "Epoch: 5; Training Loss: 0.19490; Test Loss: 0.10562\n",
      "Epoch: 6; Training Loss: 0.15879; Test Loss: 0.08362\n",
      "Epoch: 7; Training Loss: 0.13432; Test Loss: 0.07331\n",
      "Epoch: 8; Training Loss: 0.11150; Test Loss: 0.06765\n",
      "Epoch: 9; Training Loss: 0.09931; Test Loss: 0.05718\n",
      "Epoch: 10; Training Loss: 0.08484; Test Loss: 0.05233\n",
      "Epoch: 11; Training Loss: 0.07484; Test Loss: 0.04825\n",
      "Epoch: 12; Training Loss: 0.06764; Test Loss: 0.04470\n",
      "Epoch: 13; Training Loss: 0.05897; Test Loss: 0.04216\n",
      "Epoch: 14; Training Loss: 0.05548; Test Loss: 0.04206\n",
      "Epoch: 15; Training Loss: 0.05020; Test Loss: 0.03975\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=1.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.55738; Test Loss: 0.37607\n",
      "Epoch: 2; Training Loss: 0.39329; Test Loss: 0.24797\n",
      "Epoch: 3; Training Loss: 0.29740; Test Loss: 0.17043\n",
      "Epoch: 4; Training Loss: 0.23026; Test Loss: 0.12326\n",
      "Epoch: 5; Training Loss: 0.18511; Test Loss: 0.09566\n",
      "Epoch: 6; Training Loss: 0.14977; Test Loss: 0.07860\n",
      "Epoch: 7; Training Loss: 0.12372; Test Loss: 0.06350\n",
      "Epoch: 8; Training Loss: 0.10286; Test Loss: 0.05421\n",
      "Epoch: 9; Training Loss: 0.08869; Test Loss: 0.05177\n",
      "Epoch: 10; Training Loss: 0.07591; Test Loss: 0.04413\n",
      "Epoch: 11; Training Loss: 0.06633; Test Loss: 0.04076\n",
      "Epoch: 12; Training Loss: 0.05819; Test Loss: 0.04073\n",
      "Epoch: 13; Training Loss: 0.05182; Test Loss: 0.03514\n",
      "Epoch: 14; Training Loss: 0.04800; Test Loss: 0.03594\n",
      "Epoch: 15; Training Loss: 0.04284; Test Loss: 0.03220\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=1.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.56089; Test Loss: 0.38196\n",
      "Epoch: 2; Training Loss: 0.39571; Test Loss: 0.24631\n",
      "Epoch: 3; Training Loss: 0.30073; Test Loss: 0.17110\n",
      "Epoch: 4; Training Loss: 0.23145; Test Loss: 0.12136\n",
      "Epoch: 5; Training Loss: 0.18597; Test Loss: 0.09394\n",
      "Epoch: 6; Training Loss: 0.14815; Test Loss: 0.07515\n",
      "Epoch: 7; Training Loss: 0.12447; Test Loss: 0.06139\n",
      "Epoch: 8; Training Loss: 0.10190; Test Loss: 0.05331\n",
      "Epoch: 9; Training Loss: 0.08847; Test Loss: 0.04906\n",
      "Epoch: 10; Training Loss: 0.07549; Test Loss: 0.04359\n",
      "Epoch: 11; Training Loss: 0.06749; Test Loss: 0.04056\n",
      "Epoch: 12; Training Loss: 0.05840; Test Loss: 0.03713\n",
      "Epoch: 13; Training Loss: 0.05411; Test Loss: 0.03285\n",
      "Epoch: 14; Training Loss: 0.04658; Test Loss: 0.03344\n",
      "Epoch: 15; Training Loss: 0.04382; Test Loss: 0.03086\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=1.00; epochs=15; negative_count=5; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.54977; Test Loss: 0.36955\n",
      "Epoch: 2; Training Loss: 0.39068; Test Loss: 0.24106\n",
      "Epoch: 3; Training Loss: 0.29396; Test Loss: 0.16531\n",
      "Epoch: 4; Training Loss: 0.23055; Test Loss: 0.12133\n",
      "Epoch: 5; Training Loss: 0.18370; Test Loss: 0.09504\n",
      "Epoch: 6; Training Loss: 0.14905; Test Loss: 0.07526\n",
      "Epoch: 7; Training Loss: 0.12400; Test Loss: 0.06384\n",
      "Epoch: 8; Training Loss: 0.10514; Test Loss: 0.05486\n",
      "Epoch: 9; Training Loss: 0.08942; Test Loss: 0.04870\n",
      "Epoch: 10; Training Loss: 0.07776; Test Loss: 0.04282\n",
      "Epoch: 11; Training Loss: 0.06694; Test Loss: 0.03781\n",
      "Epoch: 12; Training Loss: 0.05964; Test Loss: 0.03745\n",
      "Epoch: 13; Training Loss: 0.05311; Test Loss: 0.03585\n",
      "Epoch: 14; Training Loss: 0.04874; Test Loss: 0.03462\n",
      "Epoch: 15; Training Loss: 0.04496; Test Loss: 0.03062\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 10; lambda_c: 1.00; neg_count: 10\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=1.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.53956; Test Loss: 0.33157\n",
      "Epoch: 2; Training Loss: 0.35452; Test Loss: 0.20639\n",
      "Epoch: 3; Training Loss: 0.25762; Test Loss: 0.14097\n",
      "Epoch: 4; Training Loss: 0.19815; Test Loss: 0.10164\n",
      "Epoch: 5; Training Loss: 0.15751; Test Loss: 0.07730\n",
      "Epoch: 6; Training Loss: 0.12785; Test Loss: 0.06176\n",
      "Epoch: 7; Training Loss: 0.10485; Test Loss: 0.04880\n",
      "Epoch: 8; Training Loss: 0.08860; Test Loss: 0.04208\n",
      "Epoch: 9; Training Loss: 0.07528; Test Loss: 0.03783\n",
      "Epoch: 10; Training Loss: 0.06432; Test Loss: 0.03610\n",
      "Epoch: 11; Training Loss: 0.05742; Test Loss: 0.03213\n",
      "Epoch: 12; Training Loss: 0.04945; Test Loss: 0.02876\n",
      "Epoch: 13; Training Loss: 0.04330; Test Loss: 0.02967\n",
      "Epoch: 14; Training Loss: 0.04005; Test Loss: 0.02616\n",
      "Epoch: 15; Training Loss: 0.03576; Test Loss: 0.02597\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=1.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.54564; Test Loss: 0.33913\n",
      "Epoch: 2; Training Loss: 0.35950; Test Loss: 0.20893\n",
      "Epoch: 3; Training Loss: 0.26119; Test Loss: 0.14277\n",
      "Epoch: 4; Training Loss: 0.20046; Test Loss: 0.10277\n",
      "Epoch: 5; Training Loss: 0.15894; Test Loss: 0.07729\n",
      "Epoch: 6; Training Loss: 0.12837; Test Loss: 0.06228\n",
      "Epoch: 7; Training Loss: 0.10565; Test Loss: 0.04944\n",
      "Epoch: 8; Training Loss: 0.08869; Test Loss: 0.04279\n",
      "Epoch: 9; Training Loss: 0.07545; Test Loss: 0.03681\n",
      "Epoch: 10; Training Loss: 0.06571; Test Loss: 0.03425\n",
      "Epoch: 11; Training Loss: 0.05719; Test Loss: 0.02891\n",
      "Epoch: 12; Training Loss: 0.04985; Test Loss: 0.02849\n",
      "Epoch: 13; Training Loss: 0.04361; Test Loss: 0.02614\n",
      "Epoch: 14; Training Loss: 0.03992; Test Loss: 0.02272\n",
      "Epoch: 15; Training Loss: 0.03616; Test Loss: 0.02341\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=1.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.53873; Test Loss: 0.33294\n",
      "Epoch: 2; Training Loss: 0.35478; Test Loss: 0.20557\n",
      "Epoch: 3; Training Loss: 0.25717; Test Loss: 0.14133\n",
      "Epoch: 4; Training Loss: 0.19809; Test Loss: 0.10284\n",
      "Epoch: 5; Training Loss: 0.15851; Test Loss: 0.07678\n",
      "Epoch: 6; Training Loss: 0.12699; Test Loss: 0.06092\n",
      "Epoch: 7; Training Loss: 0.10551; Test Loss: 0.04932\n",
      "Epoch: 8; Training Loss: 0.08770; Test Loss: 0.04393\n",
      "Epoch: 9; Training Loss: 0.07556; Test Loss: 0.03611\n",
      "Epoch: 10; Training Loss: 0.06426; Test Loss: 0.03571\n",
      "Epoch: 11; Training Loss: 0.05629; Test Loss: 0.03158\n",
      "Epoch: 12; Training Loss: 0.04960; Test Loss: 0.02841\n",
      "Epoch: 13; Training Loss: 0.04468; Test Loss: 0.02858\n",
      "Epoch: 14; Training Loss: 0.03911; Test Loss: 0.02665\n",
      "Epoch: 15; Training Loss: 0.03591; Test Loss: 0.02494\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=1.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.53753; Test Loss: 0.33449\n",
      "Epoch: 2; Training Loss: 0.35604; Test Loss: 0.20609\n",
      "Epoch: 3; Training Loss: 0.26043; Test Loss: 0.14246\n",
      "Epoch: 4; Training Loss: 0.20082; Test Loss: 0.10205\n",
      "Epoch: 5; Training Loss: 0.15882; Test Loss: 0.07715\n",
      "Epoch: 6; Training Loss: 0.12927; Test Loss: 0.06129\n",
      "Epoch: 7; Training Loss: 0.10646; Test Loss: 0.04942\n",
      "Epoch: 8; Training Loss: 0.08996; Test Loss: 0.04253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9; Training Loss: 0.07595; Test Loss: 0.03696\n",
      "Epoch: 10; Training Loss: 0.06578; Test Loss: 0.03128\n",
      "Epoch: 11; Training Loss: 0.05754; Test Loss: 0.02885\n",
      "Epoch: 12; Training Loss: 0.05058; Test Loss: 0.02721\n",
      "Epoch: 13; Training Loss: 0.04521; Test Loss: 0.02642\n",
      "Epoch: 14; Training Loss: 0.04058; Test Loss: 0.02805\n",
      "Epoch: 15; Training Loss: 0.03717; Test Loss: 0.02415\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=1.00; epochs=15; negative_count=10; synonym_count=5\n",
      "Epoch: 1; Training Loss: 0.53529; Test Loss: 0.32948\n",
      "Epoch: 2; Training Loss: 0.35234; Test Loss: 0.20430\n",
      "Epoch: 3; Training Loss: 0.25684; Test Loss: 0.13923\n",
      "Epoch: 4; Training Loss: 0.19953; Test Loss: 0.10088\n",
      "Epoch: 5; Training Loss: 0.15696; Test Loss: 0.07619\n",
      "Epoch: 6; Training Loss: 0.12807; Test Loss: 0.05902\n",
      "Epoch: 7; Training Loss: 0.10592; Test Loss: 0.04859\n",
      "Epoch: 8; Training Loss: 0.08782; Test Loss: 0.04068\n",
      "Epoch: 9; Training Loss: 0.07562; Test Loss: 0.03558\n",
      "Epoch: 10; Training Loss: 0.06492; Test Loss: 0.03250\n",
      "Epoch: 11; Training Loss: 0.05585; Test Loss: 0.03060\n",
      "Epoch: 12; Training Loss: 0.04994; Test Loss: 0.03045\n",
      "Epoch: 13; Training Loss: 0.04374; Test Loss: 0.02602\n",
      "Epoch: 14; Training Loss: 0.03989; Test Loss: 0.02569\n",
      "Epoch: 15; Training Loss: 0.03624; Test Loss: 0.02298\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# initialise final_scores dictionary\n",
    "final_scores = {k:defaultdict(list) for k in param_list}\n",
    "\n",
    "for _param in param_list:\n",
    "    print (\"Running test with following parameters: phi_k: %d; lambda_c: %0.2f; neg_count: %d\" \\\n",
    "           % (_param[0], _param[1], _param[2]))\n",
    "\n",
    "    args['phi_k'] = _param[0]\n",
    "    args['lambda_c'] = _param[1]\n",
    "    args['negative_sample_n'] = _param[2]    \n",
    "    \n",
    "    # iterate over every split to get score distribution\n",
    "    for idx, td in enumerate(train_data_split):              \n",
    "        hyp_model.reset_model(args=args)\n",
    "        \n",
    "        scores = train_and_evaluate_1_fold(hyp_model, td, test_data_split[idx])\n",
    "        for s, v  in scores.items():\n",
    "            final_scores[_param][s].append(v)\n",
    "    print (\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 0, 1): defaultdict(list,\n",
       "             {'MRR': [0.53408, 0.52189, 0.52402, 0.53716, 0.52682],\n",
       "              'MAP': [0.35661, 0.35583, 0.36184, 0.35876, 0.36184],\n",
       "              'P@1': [0.51386, 0.49353, 0.50227, 0.51522, 0.50334],\n",
       "              'P@5': [0.34542, 0.34828, 0.34841, 0.34856, 0.35148],\n",
       "              'P@10': [0.32982, 0.32958, 0.33627, 0.32854, 0.33504]}),\n",
       " (1, 0, 5): defaultdict(list,\n",
       "             {'MRR': [0.53396, 0.52407, 0.52383, 0.53747, 0.53255],\n",
       "              'MAP': [0.36177, 0.35579, 0.36238, 0.36598, 0.36429],\n",
       "              'P@1': [0.50959, 0.48922, 0.50227, 0.51288, 0.5078],\n",
       "              'P@5': [0.35299, 0.34853, 0.35197, 0.35461, 0.35612],\n",
       "              'P@10': [0.33688, 0.32936, 0.33606, 0.33696, 0.3368]}),\n",
       " (1, 0, 10): defaultdict(list,\n",
       "             {'MRR': [0.53397, 0.52863, 0.52576, 0.53774, 0.53313],\n",
       "              'MAP': [0.36001, 0.35801, 0.35885, 0.36343, 0.368],\n",
       "              'P@1': [0.51173, 0.50431, 0.50227, 0.51991, 0.51002],\n",
       "              'P@5': [0.35036, 0.34767, 0.34572, 0.35203, 0.35943],\n",
       "              'P@10': [0.33443, 0.3311, 0.33244, 0.33311, 0.33983]}),\n",
       " (1, 0.1, 1): defaultdict(list,\n",
       "             {'MRR': [0.51613, 0.51396, 0.52102, 0.5434, 0.53541],\n",
       "              'MAP': [0.34899, 0.34578, 0.36029, 0.35864, 0.36538],\n",
       "              'P@1': [0.4968, 0.49138, 0.49545, 0.52927, 0.51002],\n",
       "              'P@5': [0.3376, 0.33581, 0.3483, 0.345, 0.3569],\n",
       "              'P@10': [0.324, 0.31831, 0.33474, 0.32723, 0.33795]}),\n",
       " (1, 0.1, 5): defaultdict(list,\n",
       "             {'MRR': [0.5172, 0.52478, 0.53049, 0.55121, 0.53721],\n",
       "              'MAP': [0.34634, 0.35249, 0.35932, 0.35885, 0.36266],\n",
       "              'P@1': [0.49467, 0.50431, 0.50909, 0.54098, 0.51448],\n",
       "              'P@5': [0.33351, 0.34314, 0.34602, 0.34442, 0.35264],\n",
       "              'P@10': [0.32059, 0.32468, 0.33085, 0.32505, 0.33486]}),\n",
       " (1, 0.1, 10): defaultdict(list,\n",
       "             {'MRR': [0.52186, 0.52155, 0.53076, 0.54817, 0.53187],\n",
       "              'MAP': [0.3442, 0.34591, 0.35627, 0.36013, 0.35626],\n",
       "              'P@1': [0.50533, 0.50216, 0.51136, 0.5363, 0.51002],\n",
       "              'P@5': [0.33092, 0.3324, 0.34261, 0.34461, 0.34618],\n",
       "              'P@10': [0.31813, 0.31754, 0.32813, 0.32703, 0.32747]}),\n",
       " (1, 1, 1): defaultdict(list,\n",
       "             {'MRR': [0.49446, 0.52037, 0.52481, 0.54055, 0.52624],\n",
       "              'MAP': [0.31951, 0.34362, 0.34152, 0.35334, 0.3403],\n",
       "              'P@1': [0.47548, 0.49569, 0.50682, 0.52459, 0.51002],\n",
       "              'P@5': [0.30512, 0.33193, 0.32913, 0.33774, 0.32465],\n",
       "              'P@10': [0.29326, 0.31495, 0.31219, 0.31972, 0.31128]}),\n",
       " (1, 1, 5): defaultdict(list,\n",
       "             {'MRR': [0.49476, 0.52414, 0.52773, 0.53142, 0.53267],\n",
       "              'MAP': [0.32113, 0.34502, 0.34394, 0.35265, 0.34777],\n",
       "              'P@1': [0.47761, 0.50216, 0.50909, 0.51288, 0.51225],\n",
       "              'P@5': [0.30764, 0.33254, 0.33148, 0.34094, 0.33512],\n",
       "              'P@10': [0.29464, 0.31643, 0.31402, 0.32159, 0.31934]}),\n",
       " (1, 1, 10): defaultdict(list,\n",
       "             {'MRR': [0.49405, 0.52173, 0.51915, 0.53806, 0.52895],\n",
       "              'MAP': [0.32056, 0.34117, 0.33989, 0.35082, 0.33868],\n",
       "              'P@1': [0.47974, 0.50216, 0.5, 0.52225, 0.5167],\n",
       "              'P@5': [0.30721, 0.32816, 0.32962, 0.33731, 0.32394],\n",
       "              'P@10': [0.29379, 0.31167, 0.31138, 0.31886, 0.30848]}),\n",
       " (5, 0, 1): defaultdict(list,\n",
       "             {'MRR': [0.51994, 0.51721, 0.50746, 0.51862, 0.52635],\n",
       "              'MAP': [0.37038, 0.36648, 0.3659, 0.36863, 0.37013],\n",
       "              'P@1': [0.48827, 0.4806, 0.475, 0.48009, 0.49443],\n",
       "              'P@5': [0.36457, 0.3597, 0.36205, 0.36405, 0.3637],\n",
       "              'P@10': [0.34923, 0.34488, 0.34473, 0.34503, 0.34656]}),\n",
       " (5, 0, 5): defaultdict(list,\n",
       "             {'MRR': [0.52839, 0.51958, 0.51814, 0.53138, 0.52877],\n",
       "              'MAP': [0.37493, 0.37053, 0.37223, 0.3828, 0.37864],\n",
       "              'P@1': [0.4968, 0.47845, 0.49545, 0.49883, 0.50111],\n",
       "              'P@5': [0.36983, 0.36325, 0.3678, 0.37779, 0.3755],\n",
       "              'P@10': [0.35287, 0.34922, 0.3492, 0.35808, 0.35517]}),\n",
       " (5, 0, 10): defaultdict(list,\n",
       "             {'MRR': [0.53358, 0.53195, 0.52261, 0.54167, 0.53411],\n",
       "              'MAP': [0.37447, 0.38243, 0.38188, 0.38499, 0.38048],\n",
       "              'P@1': [0.5032, 0.49353, 0.49545, 0.5082, 0.5078],\n",
       "              'P@5': [0.36859, 0.37446, 0.37693, 0.38162, 0.37669],\n",
       "              'P@10': [0.35122, 0.36065, 0.36101, 0.35885, 0.35718]}),\n",
       " (5, 0.1, 1): defaultdict(list,\n",
       "             {'MRR': [0.51944, 0.51114, 0.50824, 0.52455, 0.51955],\n",
       "              'MAP': [0.36341, 0.36407, 0.36383, 0.37206, 0.36762],\n",
       "              'P@1': [0.49041, 0.47845, 0.48409, 0.4918, 0.4922],\n",
       "              'P@5': [0.35608, 0.35855, 0.3583, 0.3676, 0.3605],\n",
       "              'P@10': [0.34163, 0.34135, 0.34251, 0.34849, 0.34424]}),\n",
       " (5, 0.1, 5): defaultdict(list,\n",
       "             {'MRR': [0.5156, 0.5268, 0.5142, 0.53884, 0.53953],\n",
       "              'MAP': [0.36414, 0.37389, 0.37523, 0.38835, 0.38232],\n",
       "              'P@1': [0.48401, 0.48922, 0.48864, 0.50351, 0.51225],\n",
       "              'P@5': [0.35792, 0.36606, 0.3697, 0.38548, 0.37962],\n",
       "              'P@10': [0.34377, 0.35144, 0.35351, 0.36309, 0.35766]}),\n",
       " (5, 0.1, 10): defaultdict(list,\n",
       "             {'MRR': [0.52861, 0.52935, 0.52886, 0.54028, 0.549],\n",
       "              'MAP': [0.3768, 0.38228, 0.38624, 0.38603, 0.38656],\n",
       "              'P@1': [0.49467, 0.49138, 0.50455, 0.51054, 0.52116],\n",
       "              'P@5': [0.3731, 0.37719, 0.3814, 0.38259, 0.38189],\n",
       "              'P@10': [0.35564, 0.36064, 0.36296, 0.35979, 0.36192]}),\n",
       " (5, 1, 1): defaultdict(list,\n",
       "             {'MRR': [0.51274, 0.52011, 0.51686, 0.52233, 0.52849],\n",
       "              'MAP': [0.35987, 0.3675, 0.37597, 0.36909, 0.3674],\n",
       "              'P@1': [0.47974, 0.48922, 0.49091, 0.48946, 0.50334],\n",
       "              'P@5': [0.3532, 0.36081, 0.37053, 0.3619, 0.36024],\n",
       "              'P@10': [0.33878, 0.3434, 0.35449, 0.3443, 0.34349]}),\n",
       " (5, 1, 5): defaultdict(list,\n",
       "             {'MRR': [0.52443, 0.52486, 0.52231, 0.54785, 0.5422],\n",
       "              'MAP': [0.37414, 0.38133, 0.38281, 0.38729, 0.39105],\n",
       "              'P@1': [0.48401, 0.4806, 0.49773, 0.52459, 0.52116],\n",
       "              'P@5': [0.36802, 0.37446, 0.37795, 0.37947, 0.38682],\n",
       "              'P@10': [0.35401, 0.36052, 0.36113, 0.36123, 0.36732]}),\n",
       " (5, 1, 10): defaultdict(list,\n",
       "             {'MRR': [0.51247, 0.52822, 0.52273, 0.53997, 0.53868],\n",
       "              'MAP': [0.3551, 0.38124, 0.38401, 0.3872, 0.38291],\n",
       "              'P@1': [0.4968, 0.49138, 0.49318, 0.50585, 0.51448],\n",
       "              'P@5': [0.34769, 0.37687, 0.37898, 0.38372, 0.38066],\n",
       "              'P@10': [0.33178, 0.35888, 0.36242, 0.36264, 0.359]}),\n",
       " (10, 0, 1): defaultdict(list,\n",
       "             {'MRR': [0.51439, 0.51078, 0.50451, 0.52871, 0.52194],\n",
       "              'MAP': [0.37026, 0.36853, 0.3716, 0.38354, 0.37629],\n",
       "              'P@1': [0.47974, 0.47629, 0.46818, 0.49415, 0.48552],\n",
       "              'P@5': [0.36414, 0.36588, 0.37148, 0.37908, 0.37446],\n",
       "              'P@10': [0.34916, 0.34868, 0.3522, 0.36085, 0.35468]}),\n",
       " (10, 0, 5): defaultdict(list,\n",
       "             {'MRR': [0.51944, 0.53265, 0.5168, 0.5201, 0.54098],\n",
       "              'MAP': [0.37679, 0.38871, 0.38239, 0.38141, 0.39792],\n",
       "              'P@1': [0.49041, 0.49784, 0.48864, 0.48478, 0.51002],\n",
       "              'P@5': [0.3699, 0.38606, 0.37848, 0.37818, 0.39766],\n",
       "              'P@10': [0.35819, 0.36808, 0.36228, 0.3611, 0.37629]}),\n",
       " (10, 0, 10): defaultdict(list,\n",
       "             {'MRR': [0.51944, 0.52137, 0.53409, 0.53162, 0.53545],\n",
       "              'MAP': [0.3719, 0.38822, 0.40248, 0.3944, 0.39974],\n",
       "              'P@1': [0.49041, 0.4806, 0.51136, 0.49883, 0.49889],\n",
       "              'P@5': [0.36603, 0.38739, 0.40167, 0.39137, 0.40223],\n",
       "              'P@10': [0.35096, 0.36858, 0.38203, 0.37247, 0.37953]}),\n",
       " (10, 0.1, 1): defaultdict(list,\n",
       "             {'MRR': [0.51141, 0.49784, 0.51061, 0.5109, 0.52413],\n",
       "              'MAP': [0.36603, 0.35479, 0.37074, 0.38151, 0.38558],\n",
       "              'P@1': [0.47548, 0.45043, 0.48182, 0.46838, 0.4922],\n",
       "              'P@5': [0.36109, 0.35169, 0.36947, 0.37877, 0.38422],\n",
       "              'P@10': [0.3475, 0.33482, 0.35027, 0.36304, 0.36593]}),\n",
       " (10, 0.1, 5): defaultdict(list,\n",
       "             {'MRR': [0.51328, 0.51634, 0.52102, 0.53025, 0.51288],\n",
       "              'MAP': [0.37248, 0.37597, 0.3883, 0.387, 0.37826],\n",
       "              'P@1': [0.48614, 0.48276, 0.49545, 0.49883, 0.46771],\n",
       "              'P@5': [0.36805, 0.37234, 0.38519, 0.38064, 0.37884],\n",
       "              'P@10': [0.35386, 0.35532, 0.36918, 0.36451, 0.35979]}),\n",
       " (10, 0.1, 10): defaultdict(list,\n",
       "             {'MRR': [0.53223, 0.5282, 0.50767, 0.5276, 0.54009],\n",
       "              'MAP': [0.38103, 0.38235, 0.38641, 0.39264, 0.40105],\n",
       "              'P@1': [0.50533, 0.49138, 0.475, 0.49415, 0.49889],\n",
       "              'P@5': [0.37544, 0.37841, 0.38633, 0.39282, 0.40275],\n",
       "              'P@10': [0.3606, 0.36116, 0.36839, 0.37047, 0.38115]}),\n",
       " (10, 1, 1): defaultdict(list,\n",
       "             {'MRR': [0.50627, 0.50593, 0.52148, 0.5137, 0.52747],\n",
       "              'MAP': [0.36088, 0.36084, 0.37995, 0.37176, 0.37871],\n",
       "              'P@1': [0.47122, 0.46983, 0.49773, 0.47541, 0.50334],\n",
       "              'P@5': [0.35597, 0.35618, 0.37723, 0.37053, 0.37546],\n",
       "              'P@10': [0.34184, 0.34148, 0.35903, 0.35127, 0.35788]}),\n",
       " (10, 1, 5): defaultdict(list,\n",
       "             {'MRR': [0.51357, 0.52211, 0.52907, 0.50717, 0.53589],\n",
       "              'MAP': [0.37772, 0.38804, 0.39485, 0.38346, 0.39565],\n",
       "              'P@1': [0.47761, 0.48922, 0.50682, 0.47073, 0.50111],\n",
       "              'P@5': [0.37392, 0.38491, 0.39174, 0.38119, 0.39454],\n",
       "              'P@10': [0.35913, 0.36707, 0.37394, 0.36389, 0.37597]}),\n",
       " (10, 1, 10): defaultdict(list,\n",
       "             {'MRR': [0.52257, 0.51703, 0.52716, 0.53716, 0.52532],\n",
       "              'MAP': [0.37769, 0.38764, 0.407, 0.39881, 0.38663],\n",
       "              'P@1': [0.49041, 0.47414, 0.49318, 0.51522, 0.48998],\n",
       "              'P@5': [0.37445, 0.38793, 0.40689, 0.3945, 0.38797],\n",
       "              'P@10': [0.35722, 0.36991, 0.38927, 0.37631, 0.36689]})}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'MRR': 0.48189,\n",
    " 'MAP': 0.28752,\n",
    " 'P@1': 0.46398,\n",
    " 'P@5': 0.27521,\n",
    " 'P@10': 0.25954}\n",
    " \n",
    "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=1.00; epochs=15; negative_count=10; synonym_count=5\n",
    " {'MRR': 0.45751,\n",
    " 'MAP': 0.30459,\n",
    " 'P@1': 0.39407,\n",
    " 'P@5': 0.30215,\n",
    " 'P@10': 0.28868}\n",
    "\n",
    "{'MRR': 0.26624,\n",
    " 'MAP': 0.17138,\n",
    " 'P@1': 0.19492,\n",
    " 'P@5': 0.1721,\n",
    " 'P@10': 0.16572}\n",
    " \n",
    "{'MRR': 0.43478,\n",
    " 'MAP': 0.29167,\n",
    " 'P@1': 0.38136,\n",
    " 'P@5': 0.28898,\n",
    " 'P@10': 0.27513}\n",
    " \n",
    "{'MRR': 0.52221,\n",
    " 'MAP': 0.3812,\n",
    " 'P@1': 0.49467,\n",
    " 'P@5': 0.37676,\n",
    " 'P@10': 0.36168}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(crim_data)\n",
    "reload(multiprojection_model)\n",
    "reload(crim_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get queries from tuples\n",
    "#hyp_model.evaluator.predict_word('mare')\n",
    "predictions = hyp_model.evaluator.predict(test_data_split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tuples = data.token_to_words(test_data_split[0])\n",
    "scorer = semeval_eval.HypernymEvaluation(test_tuples)\n",
    "\n",
    "# get scores\n",
    "score_names, all_scores = scorer.get_evaluation_scores(predictions)\n",
    "\n",
    "scores = {s:0.0 for s in score_names }\n",
    "\n",
    "for k in range(len(score_names)):    \n",
    "    scores[score_names[k]] = float('%.5f' % (sum([score_list[k] for score_list in all_scores]) / len(all_scores)))    \n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "# create score dictionary\n",
    "_clusters = [1, 5, 10]\n",
    "_lambda_c = [0, 0.1, 1]\n",
    "_neg_count = [1, 5, 10]\n",
    "\n",
    "parameters = [_clusters, _lambda_c, _neg_count]\n",
    "\n",
    "param_list = list(product(*parameters))\n",
    "final_scores = {k:defaultdict(list) for k in param_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

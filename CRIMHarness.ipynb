{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "import codecs\n",
    "import argparse\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# very useful feature used to reload python modules\n",
    "from importlib import reload\n",
    "\n",
    "# import module that loads data, tokenises the tuples, initialises the embeddings matrix\n",
    "import crim_data\n",
    "\n",
    "import multiprojection_model\n",
    "import yamane_model\n",
    "# contains code to evaluate according to semeval2018 metrics\n",
    "import semeval_eval\n",
    "import crim_evaluator\n",
    "import yamane_evaluator\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise embeddings and normalise to unit-norm\n",
    "#model = KeyedVectors.load_word2vec_format('embeddings/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model = KeyedVectors.load_word2vec_format('embeddings/glove.42B.300d.txt', binary=False)\n",
    "#model = KeyedVectors.load_word2vec_format('embeddings/wiki-news-300d-1M.vec', binary=False)\n",
    "\n",
    "#model.save_word2vec_format('embeddings/GoogleNews-vectors-negative300.txt', binary=False)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Ustalov et al. prepare module\n",
    "import prepare\n",
    "\n",
    "sub_train = prepare.read_subsumptions('subsumptions-train.txt')\n",
    "sub_test = prepare.read_subsumptions('subsumptions-test.txt')\n",
    "sub_validation = prepare.read_subsumptions('subsumptions-validation.txt')\n",
    "\n",
    "\n",
    "# remove vocab term having no vector in embeddings\n",
    "def get_terms_having_vectors(w2v, dataset):\n",
    "    return [(q,h) for q, h in dataset if q in w2v and h in w2v]\n",
    "\n",
    "sub_train = get_terms_having_vectors(model, sub_train)\n",
    "sub_test = get_terms_having_vectors(model, sub_test)\n",
    "sub_validation = get_terms_having_vectors(model, sub_validation)\n",
    "\n",
    "\n",
    "# create hypernym dictionary\n",
    "hyper_dict = defaultdict(list)\n",
    "for x, y in sub_train + sub_test + sub_validation:        \n",
    "    hyper_dict[x].append(y)\n",
    "    \n",
    "hyper_dict.default_factory = None\n",
    "\n",
    "# to ensure that synonyms are not also hypernyms\n",
    "synonyms = prepare.read_synonyms('synonyms.txt', hyper_dict)  \n",
    "synonyms = prepare.get_synonymys_having_vectors(synonyms, model)\n",
    "synonyms.default_factory = None\n",
    "\n",
    "\n",
    "print (\"Total number of tuples in entire set: %d\" % (len([x for (x,y) in sub_train + sub_test + sub_validation])))\n",
    "print (\"Unique hyponyms in set: %d\" % (len(set([x for (x,y) in sub_train + sub_test + sub_validation]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'w2v':model,\n",
    "        'train':sub_train, 'test':sub_test, 'validation':sub_validation, 'synonyms':synonyms, \n",
    "        'limited_vocab_n': 250000\n",
    "       }\n",
    "data = crim_data.CrimData(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert full dataset to array\n",
    "all_data_tokens = np.asarray(data.all_data_token)\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=42)\n",
    "kf.get_n_splits(all_data_tokens[:,0])\n",
    "\n",
    "# split data into 5 different train-test folds\n",
    "train_data_split = []\n",
    "test_data_split = []\n",
    "for k in kf.split(all_data_tokens[:,0]):    \n",
    "    k_train_split = all_data_tokens[k[0]]\n",
    "    k_test_split = all_data_tokens[k[1]]\n",
    "    \n",
    "    train_data_split.append(k_train_split)\n",
    "    test_data_split.append(k_test_split)\n",
    "\n",
    "# output training-test split sizes    \n",
    "for tr, te in zip(train_data_split, test_data_split):\n",
    "    print (\"Training tuples: %d; test tuples: %d\" % (len(tr), len(te)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_1_fold(hyp_model, train_split, test_split):    \n",
    "    \n",
    "    # fit model\n",
    "    # the test split is only used to measure the test loss\n",
    "    hyp_model.fit(train_split, test_split)    \n",
    "    # this step should not be required since the model is dynamically linked to the evaluator\n",
    "    hyp_model.evaluator.set_model(hyp_model.model)\n",
    "    # generates predictions according to trained model\n",
    "    predictions = hyp_model.evaluator.predict(test_split)\n",
    "    # this converts the tokens back to words for evaluation\n",
    "    test_tuples = data.token_to_words(test_split)\n",
    "    # here we have a scorer that will mark our effort according to this particular test split\n",
    "    scorer = semeval_eval.HypernymEvaluation(test_tuples)\n",
    "    # get scores\n",
    "    score_names, all_scores = scorer.get_evaluation_scores(predictions)\n",
    "    # initialise scores (MRR, MAP, ...)\n",
    "    scores = {s:0.0 for s in score_names }\n",
    "    for k in range(len(score_names)):    \n",
    "        scores[score_names[k]] = float('%.5f' % (sum([score_list[k] for score_list in all_scores]) / len(all_scores)))    \n",
    "\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI-PROJECTION Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_layer = multiprojection_model.get_embeddings_model(data.embeddings_matrix, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# standard model parameters - we won't be changing these\n",
    "args['data']              = data\n",
    "args['embeddings_layer']  = embeddings_layer\n",
    "args['epochs']            = 10\n",
    "args['batch_size']        = 32\n",
    "args['synonym_sample_n']  = 5\n",
    "args['phi_k']             = 5\n",
    "args['lambda_c']          = 0.\n",
    "args['negative_sample_n'] = 10\n",
    "args['save_path']         = 'glove_multiproj.npz'\n",
    "args['patience']          = 2\n",
    "args['eval_after_epoch']  = True\n",
    "\n",
    "# generate parameter combinations\n",
    "_clusters = [10, 5, 1]\n",
    "_lambda_c = [0, 0.1, 1]\n",
    "_neg_count = [10, 5, 1]\n",
    "\n",
    "parameters = [_clusters, _lambda_c, _neg_count]\n",
    "\n",
    "param_list = list(product(*parameters))\n",
    "\n",
    "# initialise hypernymy discovery model which we will reuse by resetting the model with new args\n",
    "hyp_model = multiprojection_model.MultiProjModel(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise final_scores dictionary\n",
    "final_scores = {k:defaultdict(list) for k in param_list}\n",
    "\n",
    "for idx2, _param in enumerate(param_list):\n",
    "    print (\"Running test with following parameters: phi_k: %d; lambda_c: %0.2f; neg_count: %d\" \\\n",
    "           % (_param[0], _param[1], _param[2]))\n",
    "\n",
    "    args['phi_k'] = _param[0]\n",
    "    args['lambda_c'] = _param[1]\n",
    "    args['negative_sample_n'] = _param[2]    \n",
    "    \n",
    "    # iterate over every split to get score distribution\n",
    "    for idx, td in enumerate(train_data_split):              \n",
    "        hyp_model.reset_model(args=args)\n",
    "        \n",
    "        scores = train_and_evaluate_1_fold(hyp_model, td, test_data_split[idx])\n",
    "        for s, v  in scores.items():\n",
    "            final_scores[_param][s].append(v)\n",
    "    print (\"\")\n",
    "    print (\"Finished %d from %d experiments\" % (idx2+1, len(param_list)))\n",
    "    print (\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in final_scores.items():    \n",
    "    cl_size = k[0]\n",
    "    lam = k[1]\n",
    "    neg = k[2]\n",
    "    for k2, v2 in v.items():        \n",
    "        print (\"%d,%0.1f,%d,%s,%0.5f,%0.5f,%0.5f,%0.5f,%0.5f\" % (cl_size, lam, neg, k2, v2[0], v2[1], v2[2], v2[3], v2[4]) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_model.epochs=5\n",
    "hyp_model.fit(train_data_split[0], test_data_split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_model.model.get_weights()[2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YAMANE Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yam_train_and_evaluate_1_fold(yam, train_split, test_split):    \n",
    "    \n",
    "    # fit model\n",
    "    # the test split is only used to measure the test loss\n",
    "    yam.fit(train_split, test_split)    \n",
    "    # this step should not be required since the model is dynamically linked to the evaluator\n",
    "    yam.evaluator.set_ensemble(yam)\n",
    "    # generates predictions according to trained model\n",
    "    predictions = yam.evaluator.predict(test_split)\n",
    "    # this converts the tokens back to words for evaluation\n",
    "    test_tuples = data.token_to_words(test_split)\n",
    "    # here we have a scorer that will mark our effort according to this particular test split\n",
    "    scorer = semeval_eval.HypernymEvaluation(test_tuples)\n",
    "    # get scores\n",
    "    score_names, all_scores = scorer.get_evaluation_scores(predictions)\n",
    "    # initialise scores (MRR, MAP, ...)\n",
    "    scores = {s:0.0 for s in score_names }\n",
    "    for k in range(len(score_names)):    \n",
    "        scores[score_names[k]] = float('%.5f' % (sum([score_list[k] for score_list in all_scores]) / len(all_scores)))    \n",
    "\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings_layer2 = yamane_model.get_embeddings_model(data.embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# initialise Yamane Model\n",
    "args={'data':data, 'embeddings_layer': embeddings_layer2, 'lr':0.001,'lambda_c':0.16, \n",
    "      'negative_sample_n':5, 'epochs':10, 'save_path':'glove_yamane_016.npz', 'patience':2}\n",
    "\n",
    "# generate parameter combinations\n",
    "_lambda_c = [0.16]\n",
    "_neg_count = [5]\n",
    "\n",
    "parameters = [_lambda_c, _neg_count]\n",
    "param_list = list(product(*parameters))\n",
    "\n",
    "yummy = yamane_model.YamaneEnsemble(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialise final_scores dictionary\n",
    "final_scores = {k:defaultdict(list) for k in param_list}\n",
    "\n",
    "for idx2, _param in enumerate(param_list):\n",
    "    print (\"Running test with following parameters: lambda_c: %0.2f; neg_count: %d\" \\\n",
    "           % (_param[0], _param[1]))\n",
    "    \n",
    "    args['lambda_c'] = _param[0]\n",
    "    args['negative_sample_n'] = _param[1]    \n",
    "    \n",
    "    # iterate over every split to get score distribution\n",
    "    for idx, td in enumerate(train_data_split):              \n",
    "        yummy.reset_ensemble(args=args)\n",
    "        \n",
    "        scores = yam_train_and_evaluate_1_fold(yummy, td, test_data_split[idx])\n",
    "        for s, v  in scores.items():\n",
    "            final_scores[_param][s].append(v)\n",
    "    print (\"\")\n",
    "    print (\"Finished %d from %d experiments\" % (idx2+1, len(param_list)))\n",
    "    print (\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yummy.sample_clusters\n",
    "Counter(yummy.sample_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_scores\n",
    "# nicer output of final scores\n",
    "for k, v in final_scores.items():    \n",
    "    lam = k[0]\n",
    "    neg = k[1]\n",
    "    for k2, v2 in v.items():        \n",
    "        print (\"%0.2f,%d,%s,%0.5f,%0.5f,%0.5f,%0.5f,%0.5f\" % (lam, neg, k2, v2[0], v2[1], v2[2], v2[3], v2[4]) )\n",
    "\n",
    "# get mean scores        \n",
    "for v in final_scores.values():\n",
    "    for k, v2 in v.items():\n",
    "        print (\"%s: %0.5f\" % (k, np.mean(v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek into the one of the clusters\n",
    "for c in range(len(Counter(yummy.sample_clusters))):\n",
    "    print (\"Cluster %d\" % (c))\n",
    "    for idx, i in enumerate(np.where(yummy.sample_clusters == c)[0]):\n",
    "        if idx < 30:\n",
    "            print (data.tokenizer.sequences_to_texts([train_data_split[4][i]]))\n",
    "        else:\n",
    "            break\n",
    "    print (\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hypernym representation by total in cluster\n",
    "def get_hypernym_rep_in_cluster(fold, clusters, c, top_running_perc):\n",
    "    hyper_freq = list(map(lambda w:data.tokenizer.index_word[w], fold[:,1][np.where(clusters == c)]))\n",
    "    # group hypernyms and count instances\n",
    "    hyper_freq = Counter(hyper_freq)\n",
    "    total_pairs = sum(hyper_freq.values())    \n",
    "    total_uniq_hypers = len(hyper_freq.keys())\n",
    "    \n",
    "    running_total = 0.\n",
    "    result = []\n",
    "    for count, word in sorted(((value, key) for (key,value) in hyper_freq.items()), reverse = True):                \n",
    "        perc_total = round(count / (1. * total_pairs), 5)\n",
    "        running_total += perc_total        \n",
    "        result.append((word, count, total_uniq_hypers, perc_total))        \n",
    "        if running_total > top_running_perc:\n",
    "            break\n",
    "    return result\n",
    "\n",
    "for c in range(len(Counter(yummy.sample_clusters))):\n",
    "    print (\"Cluster %d\" % (c))\n",
    "    for word, pairs, tot, perc_pairs in get_hypernym_rep_in_cluster(train_data_split[4], yummy.sample_clusters, c, 0.4):\n",
    "        print (\"%s,%d,%d,%0.5f\" % (word, pairs, tot, perc_pairs))\n",
    "    print (\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(crim_data)\n",
    "reload(multiprojection_model)\n",
    "reload(crim_evaluator)\n",
    "reload(yamane_model)\n",
    "reload(yamane_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get queries from tuples\n",
    "#hyp_model.evaluator.predict_word('mare')\n",
    "predictions = hyp_model.evaluator.predict(test_data_split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tuples = data.token_to_words(test_data_split[0])\n",
    "scorer = semeval_eval.HypernymEvaluation(test_tuples)\n",
    "\n",
    "# get scores\n",
    "score_names, all_scores = scorer.get_evaluation_scores(predictions)\n",
    "\n",
    "scores = {s:0.0 for s in score_names }\n",
    "\n",
    "for k in range(len(score_names)):    \n",
    "    scores[score_names[k]] = float('%.5f' % (sum([score_list[k] for score_list in all_scores]) / len(all_scores)))    \n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "# create score dictionary\n",
    "_clusters = [1, 5, 10]\n",
    "_lambda_c = [0, 0.1, 1]\n",
    "_neg_count = [1, 5, 10]\n",
    "\n",
    "parameters = [_clusters, _lambda_c, _neg_count]\n",
    "\n",
    "param_list = list(product(*parameters))\n",
    "final_scores = {k:defaultdict(list) for k in param_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = yummy.evaluator.predict(test_data_split[4])\n",
    "test_tuples = data.token_to_words(test_data_split[4])\n",
    "scorer = semeval_eval.HypernymEvaluation(test_tuples)\n",
    "# get scores\n",
    "score_names, all_scores = scorer.get_evaluation_scores(predictions)\n",
    "# initialise scores (MRR, MAP, ...)\n",
    "scores = {s:0.0 for s in score_names }\n",
    "for k in range(len(score_names)):    \n",
    "    scores[score_names[k]] = float('%.5f' % (sum([score_list[k] for score_list in all_scores]) / len(all_scores)))    \n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "import codecs\n",
    "import argparse\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# very useful feature used to reload python modules\n",
    "from importlib import reload\n",
    "\n",
    "# import module that loads data, tokenises the tuples, initialises the embeddings matrix\n",
    "import crim_data\n",
    "\n",
    "import multiprojection_model\n",
    "# contains code to evaluate according to semeval2018 metrics\n",
    "import semeval_eval\n",
    "import crim_evaluator\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise embeddings and normalise to unit-norm\n",
    "#model = KeyedVectors.load_word2vec_format('embeddings/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "#model = KeyedVectors.load_word2vec_format('embeddings/glove.42B.300d.txt', binary=False)\n",
    "model = KeyedVectors.load_word2vec_format('embeddings/wiki-news-300d-1M.vec', binary=False)\n",
    "\n",
    "#model.save_word2vec_format('embeddings/GoogleNews-vectors-negative300.txt', binary=False)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Ustalov et al. prepare module\n",
    "import prepare\n",
    "\n",
    "sub_train = prepare.read_subsumptions('subsumptions-train.txt')\n",
    "sub_test = prepare.read_subsumptions('subsumptions-test.txt')\n",
    "sub_validation = prepare.read_subsumptions('subsumptions-validation.txt')\n",
    "synonyms = prepare.read_synonyms('synonyms.txt')  \n",
    "\n",
    "# remove vocab term having no vector in embeddings\n",
    "def get_terms_having_vectors(w2v, dataset):\n",
    "    return [(q,h) for q, h in dataset if q in w2v and h in w2v]\n",
    "\n",
    "sub_train = get_terms_having_vectors(model, sub_train)\n",
    "sub_test = get_terms_having_vectors(model, sub_test)\n",
    "sub_validation = get_terms_having_vectors(model, sub_validation)\n",
    "synonyms = prepare.get_synonymys_having_vectors(synonyms, model)\n",
    "synonyms.default_factory = None\n",
    "\n",
    "# create hypernym dictionary\n",
    "hyper_dict = defaultdict(list)\n",
    "for x, y in sub_train + sub_test + sub_validation:        \n",
    "    hyper_dict[x].append(y)\n",
    "    \n",
    "hyper_dict.default_factory = None\n",
    "\n",
    "print (\"Total number of tuples in entire set: %d\" % (len([x for (x,y) in sub_train + sub_test + sub_validation])))\n",
    "print (\"Unique hyponyms in set: %d\" % (len(set([x for (x,y) in sub_train + sub_test + sub_validation]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'w2v':model,\n",
    "        'train':sub_train, 'test':sub_test, 'validation':sub_validation, 'synonyms':synonyms, \n",
    "        'limited_vocab_n': 250000\n",
    "       }\n",
    "data = crim_data.CrimData(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert full dataset to array\n",
    "all_data_tokens = np.asarray(data.all_data_token)\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(all_data_tokens[:,0])\n",
    "\n",
    "# split data into 5 different train-test folds\n",
    "train_data_split = []\n",
    "test_data_split = []\n",
    "for k in kf.split(all_data_tokens[:,0]):    \n",
    "    k_train_split = all_data_tokens[k[0]]\n",
    "    k_test_split = all_data_tokens[k[1]]\n",
    "    \n",
    "    train_data_split.append(k_train_split)\n",
    "    test_data_split.append(k_test_split)\n",
    "\n",
    "# output training-test split sizes    \n",
    "for tr, te in zip(train_data_split, test_data_split):\n",
    "    print (\"Training tuples: %d; test tuples: %d\" % (len(tr), len(te)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_1_fold(hyp_model, train_split, test_split):    \n",
    "    \n",
    "    # fit model\n",
    "    # the test split is only used to measure the test loss\n",
    "    hyp_model.fit(train_split, test_split)    \n",
    "    # this step should not be required since the model is dynamically linked to the evaluator\n",
    "    hyp_model.evaluator.set_model(hyp_model.model)\n",
    "    # generates predictions according to trained model\n",
    "    predictions = hyp_model.evaluator.predict(test_split)\n",
    "    # this converts the tokens back to words for evaluation\n",
    "    test_tuples = data.token_to_words(test_split)\n",
    "    # here we have a scorer that will mark our effort according to this particular test split\n",
    "    scorer = semeval_eval.HypernymEvaluation(test_tuples)\n",
    "    # get scores\n",
    "    score_names, all_scores = scorer.get_evaluation_scores(predictions)\n",
    "    # initialise scores (MRR, MAP, ...)\n",
    "    scores = {s:0.0 for s in score_names }\n",
    "    for k in range(len(score_names)):    \n",
    "        scores[score_names[k]] = float('%.5f' % (sum([score_list[k] for score_list in all_scores]) / len(all_scores)))    \n",
    "\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# standard model parameters - we won't be changing these\n",
    "args['data'] = data\n",
    "args['epochs'] = 15\n",
    "args['batch_size']= 32\n",
    "args['synonym_sample_n']= 5\n",
    "args['phi_k'] = 1\n",
    "args['lambda_c'] = 0.\n",
    "args['negative_sample_n'] = 10\n",
    "\n",
    "# generate parameter combinations\n",
    "_clusters = [1, 5, 10]\n",
    "_lambda_c = [0, 0.1, 1]\n",
    "_neg_count = [1, 5, 10]\n",
    "\n",
    "parameters = [_clusters, _lambda_c, _neg_count]\n",
    "\n",
    "param_list = list(product(*parameters))\n",
    "\n",
    "# initialise hypernymy discovery model which we will reuse by resetting the model with new args\n",
    "hyp_model = multiprojection_model.MultiProjModel(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise final_scores dictionary\n",
    "final_scores = {k:defaultdict(list) for k in param_list}\n",
    "\n",
    "for _param in param_list:\n",
    "    print (\"Running test with following parameters: phi_k: %d; lambda_c: %0.2f; neg_count: %d\" \\\n",
    "           % (_param[0], _param[1], _param[2]))\n",
    "\n",
    "    args['phi_k'] = _param[0]\n",
    "    args['lambda_c'] = _param[1]\n",
    "    args['negative_sample_n'] = _param[2]    \n",
    "    \n",
    "    # iterate over every split to get score distribution\n",
    "    for idx, td in enumerate(train_data_split):              \n",
    "        hyp_model.reset_model(args=args)\n",
    "        \n",
    "        scores = train_and_evaluate_1_fold(hyp_model, td, test_data_split[idx])\n",
    "        for s, v  in scores.items():\n",
    "            final_scores[_param][s].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'MRR': 0.48189,\n",
    " 'MAP': 0.28752,\n",
    " 'P@1': 0.46398,\n",
    " 'P@5': 0.27521,\n",
    " 'P@10': 0.25954}\n",
    " \n",
    "Fitting model with following parameters: batch_size=32; phi_k=10; lambda_c=1.00; epochs=15; negative_count=10; synonym_count=5\n",
    " {'MRR': 0.45751,\n",
    " 'MAP': 0.30459,\n",
    " 'P@1': 0.39407,\n",
    " 'P@5': 0.30215,\n",
    " 'P@10': 0.28868}\n",
    "\n",
    "{'MRR': 0.26624,\n",
    " 'MAP': 0.17138,\n",
    " 'P@1': 0.19492,\n",
    " 'P@5': 0.1721,\n",
    " 'P@10': 0.16572}\n",
    " \n",
    "{'MRR': 0.43478,\n",
    " 'MAP': 0.29167,\n",
    " 'P@1': 0.38136,\n",
    " 'P@5': 0.28898,\n",
    " 'P@10': 0.27513}\n",
    " \n",
    "{'MRR': 0.52221,\n",
    " 'MAP': 0.3812,\n",
    " 'P@1': 0.49467,\n",
    " 'P@5': 0.37676,\n",
    " 'P@10': 0.36168}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(crim_data)\n",
    "reload(multiprojection_model)\n",
    "reload(crim_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get queries from tuples\n",
    "#hyp_model.evaluator.predict_word('mare')\n",
    "predictions = hyp_model.evaluator.predict(test_data_split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tuples = data.token_to_words(test_data_split[0])\n",
    "scorer = semeval_eval.HypernymEvaluation(test_tuples)\n",
    "\n",
    "# get scores\n",
    "score_names, all_scores = scorer.get_evaluation_scores(predictions)\n",
    "\n",
    "scores = {s:0.0 for s in score_names }\n",
    "\n",
    "for k in range(len(score_names)):    \n",
    "    scores[score_names[k]] = float('%.5f' % (sum([score_list[k] for score_list in all_scores]) / len(all_scores)))    \n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "# create score dictionary\n",
    "_clusters = [1, 5, 10]\n",
    "_lambda_c = [0, 0.1, 1]\n",
    "_neg_count = [1, 5, 10]\n",
    "\n",
    "parameters = [_clusters, _lambda_c, _neg_count]\n",
    "\n",
    "param_list = list(product(*parameters))\n",
    "final_scores = {k:defaultdict(list) for k in param_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

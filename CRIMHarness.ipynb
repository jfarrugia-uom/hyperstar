{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "import codecs\n",
    "import argparse\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# very useful feature used to reload python modules\n",
    "from importlib import reload\n",
    "\n",
    "# import module that loads data, tokenises the tuples, initialises the embeddings matrix\n",
    "import crim_data\n",
    "\n",
    "import multiprojection_model\n",
    "import yamane_model\n",
    "# contains code to evaluate according to semeval2018 metrics\n",
    "import semeval_eval\n",
    "import crim_evaluator\n",
    "import yamane_evaluator\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise embeddings and normalise to unit-norm\n",
    "#model = KeyedVectors.load_word2vec_format('embeddings/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model = KeyedVectors.load_word2vec_format('embeddings/glove.42B.300d.txt', binary=False)\n",
    "#model = KeyedVectors.load_word2vec_format('embeddings/wiki-news-300d-1M.vec', binary=False)\n",
    "\n",
    "#model.save_word2vec_format('embeddings/GoogleNews-vectors-negative300.txt', binary=False)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tuples in entire set: 7391\n",
      "Unique hyponyms in set: 2254\n"
     ]
    }
   ],
   "source": [
    "# import Ustalov et al. prepare module\n",
    "import prepare\n",
    "\n",
    "sub_train = prepare.read_subsumptions('subsumptions-train.txt')\n",
    "sub_test = prepare.read_subsumptions('subsumptions-test.txt')\n",
    "sub_validation = prepare.read_subsumptions('subsumptions-validation.txt')\n",
    "\n",
    "\n",
    "# remove vocab term having no vector in embeddings\n",
    "def get_terms_having_vectors(w2v, dataset):\n",
    "    return [(q,h) for q, h in dataset if q in w2v and h in w2v]\n",
    "\n",
    "sub_train = get_terms_having_vectors(model, sub_train)\n",
    "sub_test = get_terms_having_vectors(model, sub_test)\n",
    "sub_validation = get_terms_having_vectors(model, sub_validation)\n",
    "\n",
    "\n",
    "# create hypernym dictionary\n",
    "hyper_dict = defaultdict(list)\n",
    "for x, y in sub_train + sub_test + sub_validation:        \n",
    "    hyper_dict[x].append(y)\n",
    "    \n",
    "hyper_dict.default_factory = None\n",
    "\n",
    "# to ensure that synonyms are not also hypernyms\n",
    "synonyms = prepare.read_synonyms('synonyms.txt', hyper_dict)  \n",
    "synonyms = prepare.get_synonymys_having_vectors(synonyms, model)\n",
    "synonyms.default_factory = None\n",
    "\n",
    "\n",
    "print (\"Total number of tuples in entire set: %d\" % (len([x for (x,y) in sub_train + sub_test + sub_validation])))\n",
    "print (\"Unique hyponyms in set: %d\" % (len(set([x for (x,y) in sub_train + sub_test + sub_validation]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating limited vocabulary of 250000\n",
      "Dataset vocabulary size is 3204\n",
      "Truncated vocab length is 250000\n",
      "Vocab size is 250000 words\n",
      "Initialising negative sampler\n",
      "Tokenising all dataset tuples\n",
      "Creating embeddings matrix\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "args = {'w2v':model,\n",
    "        'train':sub_train, 'test':sub_test, 'validation':sub_validation, 'synonyms':synonyms, \n",
    "        'limited_vocab_n': 250000\n",
    "       }\n",
    "data = crim_data.CrimData(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tuples: 5912; test tuples: 1479\n",
      "Training tuples: 5913; test tuples: 1478\n",
      "Training tuples: 5913; test tuples: 1478\n",
      "Training tuples: 5913; test tuples: 1478\n",
      "Training tuples: 5913; test tuples: 1478\n"
     ]
    }
   ],
   "source": [
    "# convert full dataset to array\n",
    "all_data_tokens = np.asarray(data.all_data_token)\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=42)\n",
    "kf.get_n_splits(all_data_tokens[:,0])\n",
    "\n",
    "# split data into 5 different train-test folds\n",
    "train_data_split = []\n",
    "test_data_split = []\n",
    "for k in kf.split(all_data_tokens[:,0]):    \n",
    "    k_train_split = all_data_tokens[k[0]]\n",
    "    k_test_split = all_data_tokens[k[1]]\n",
    "    \n",
    "    train_data_split.append(k_train_split)\n",
    "    test_data_split.append(k_test_split)\n",
    "\n",
    "# output training-test split sizes    \n",
    "for tr, te in zip(train_data_split, test_data_split):\n",
    "    print (\"Training tuples: %d; test tuples: %d\" % (len(tr), len(te)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_1_fold(hyp_model, train_split, test_split):    \n",
    "    \n",
    "    # fit model\n",
    "    # the test split is only used to measure the test loss\n",
    "    hyp_model.fit(train_split, test_split)    \n",
    "    # this step should not be required since the model is dynamically linked to the evaluator\n",
    "    hyp_model.evaluator.set_model(hyp_model.model)\n",
    "    # generates predictions according to trained model\n",
    "    predictions = hyp_model.evaluator.predict(test_split)\n",
    "    # this converts the tokens back to words for evaluation\n",
    "    test_tuples = data.token_to_words(test_split)\n",
    "    # here we have a scorer that will mark our effort according to this particular test split\n",
    "    scorer = semeval_eval.HypernymEvaluation(test_tuples)\n",
    "    # get scores\n",
    "    score_names, all_scores = scorer.get_evaluation_scores(predictions)\n",
    "    # initialise scores (MRR, MAP, ...)\n",
    "    scores = {s:0.0 for s in score_names }\n",
    "    for k in range(len(score_names)):    \n",
    "        scores[score_names[k]] = float('%.5f' % (sum([score_list[k] for score_list in all_scores]) / len(all_scores)))    \n",
    "\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI-PROJECTION Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "embeddings_layer = multiprojection_model.get_embeddings_model(data.embeddings_matrix, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# standard model parameters - we won't be changing these\n",
    "args['data']              = data\n",
    "args['embeddings_layer']  = embeddings_layer\n",
    "args['epochs']            = 10\n",
    "args['batch_size']        = 32\n",
    "args['synonym_sample_n']  = 5\n",
    "args['phi_k']             = 5\n",
    "args['lambda_c']          = 0.\n",
    "args['negative_sample_n'] = 10\n",
    "args['save_path']         = 'glove_multiproj.npz'\n",
    "args['patience']          = 2\n",
    "args['eval_after_epoch']  = True\n",
    "\n",
    "# generate parameter combinations\n",
    "_clusters = [10, 5, 1]\n",
    "_lambda_c = [0, 0.1, 1]\n",
    "_neg_count = [10, 5, 1]\n",
    "\n",
    "parameters = [_clusters, _lambda_c, _neg_count]\n",
    "\n",
    "param_list = list(product(*parameters))\n",
    "\n",
    "# initialise hypernymy discovery model which we will reuse by resetting the model with new args\n",
    "hyp_model = multiprojection_model.MultiProjModel(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise final_scores dictionary\n",
    "final_scores = {k:defaultdict(list) for k in param_list}\n",
    "\n",
    "for idx2, _param in enumerate(param_list):\n",
    "    print (\"Running test with following parameters: phi_k: %d; lambda_c: %0.2f; neg_count: %d\" \\\n",
    "           % (_param[0], _param[1], _param[2]))\n",
    "\n",
    "    args['phi_k'] = _param[0]\n",
    "    args['lambda_c'] = _param[1]\n",
    "    args['negative_sample_n'] = _param[2]    \n",
    "    \n",
    "    # iterate over every split to get score distribution\n",
    "    for idx, td in enumerate(train_data_split):              \n",
    "        hyp_model.reset_model(args=args)\n",
    "        \n",
    "        scores = train_and_evaluate_1_fold(hyp_model, td, test_data_split[idx])\n",
    "        for s, v  in scores.items():\n",
    "            final_scores[_param][s].append(v)\n",
    "    print (\"\")\n",
    "    print (\"Finished %d from %d experiments\" % (idx2+1, len(param_list)))\n",
    "    print (\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in final_scores.items():    \n",
    "    cl_size = k[0]\n",
    "    lam = k[1]\n",
    "    neg = k[2]\n",
    "    for k2, v2 in v.items():        \n",
    "        print (\"%d,%0.1f,%d,%s,%0.5f,%0.5f,%0.5f,%0.5f,%0.5f\" % (cl_size, lam, neg, k2, v2[0], v2[1], v2[2], v2[3], v2[4]) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with following parameters: batch_size=32; phi_k=5; lambda_c=0.00; epochs=5; negative_count=10; synonym_count=5\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Epoch: 1; Training Loss: 0.05127; Test Loss: 0.04611; Test MAP: 0.28225; Test MRR: 0.45454\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Epoch: 2; Training Loss: 0.04772; Test Loss: 0.04071; Test MAP: 0.30149; Test MRR: 0.47153\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Epoch: 3; Training Loss: 0.04372; Test Loss: 0.04034; Test MAP: 0.30333; Test MRR: 0.46793\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Epoch: 4; Training Loss: 0.04055; Test Loss: 0.04140; Test MAP: 0.30636; Test MRR: 0.46344\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Epoch: 5; Training Loss: 0.03679; Test Loss: 0.03576; Test MAP: 0.30877; Test MRR: 0.46174\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "hyp_model.epochs=5\n",
    "hyp_model.fit(train_data_split[0], test_data_split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.27615955, -0.01881126,  0.06860597, ..., -0.0149341 ,\n",
       "         -0.02715511,  0.01282344],\n",
       "        [ 0.01523856, -0.19490959, -0.04700712, ..., -0.07261097,\n",
       "         -0.02743305,  0.01195487],\n",
       "        [ 0.07227079, -0.0014679 , -0.22675948, ..., -0.04206238,\n",
       "         -0.07107186,  0.01601199],\n",
       "        ...,\n",
       "        [ 0.05580143,  0.01436739, -0.05042887, ..., -0.27090022,\n",
       "          0.03965047,  0.11350769],\n",
       "        [-0.05462441,  0.01770023, -0.05261305, ..., -0.03071618,\n",
       "         -0.32597223, -0.09411716],\n",
       "        [ 0.08957611, -0.04007797, -0.03419819, ...,  0.04283368,\n",
       "         -0.0474022 , -0.27025035]], dtype=float32),\n",
       " array([[-0.28287882, -0.05325396,  0.01187337, ..., -0.00332887,\n",
       "         -0.00272602,  0.00399361],\n",
       "        [ 0.01889306, -0.20423381, -0.05310702, ..., -0.05409317,\n",
       "          0.00654182,  0.02243645],\n",
       "        [ 0.08035896,  0.00041493, -0.23361142, ..., -0.05962481,\n",
       "         -0.08410495,  0.03525608],\n",
       "        ...,\n",
       "        [ 0.06335561,  0.02922312, -0.06981944, ..., -0.28999534,\n",
       "          0.08579689,  0.08864019],\n",
       "        [-0.07464323,  0.020739  , -0.0623582 , ..., -0.08522341,\n",
       "         -0.2667365 , -0.09686365],\n",
       "        [ 0.12703831, -0.02168736, -0.03716576, ...,  0.07671072,\n",
       "         -0.03714127, -0.25877964]], dtype=float32),\n",
       " array([[-0.28494635, -0.00214148,  0.06068549, ..., -0.01177674,\n",
       "         -0.03110349,  0.02185374],\n",
       "        [ 0.06438556, -0.20306146, -0.04102373, ..., -0.06576002,\n",
       "         -0.01859888,  0.01130562],\n",
       "        [ 0.07181667, -0.03470663, -0.19840722, ..., -0.03833047,\n",
       "         -0.10721449,  0.04292726],\n",
       "        ...,\n",
       "        [ 0.06233471,  0.03748212, -0.03771146, ..., -0.2969812 ,\n",
       "          0.08473434,  0.11958043],\n",
       "        [-0.0454617 ,  0.04658247, -0.04844323, ..., -0.05594809,\n",
       "         -0.2783444 , -0.09940948],\n",
       "        [ 0.05937927, -0.0300975 , -0.01095144, ...,  0.04701936,\n",
       "         -0.06212748, -0.23089962]], dtype=float32),\n",
       " array([[ 0.23570748,  0.01144713, -0.0106785 , ..., -0.0231462 ,\n",
       "          0.01997985,  0.00613095],\n",
       "        [ 0.00808879,  0.18875983,  0.04958187, ...,  0.07035969,\n",
       "          0.03318576, -0.08515768],\n",
       "        [-0.07458916,  0.02634828,  0.25342983, ...,  0.02301934,\n",
       "          0.09788463, -0.01559617],\n",
       "        ...,\n",
       "        [-0.02731982, -0.0370667 ,  0.05303926, ...,  0.28979117,\n",
       "         -0.05339854, -0.1316741 ],\n",
       "        [ 0.08464712, -0.03617041,  0.06037314, ...,  0.01265998,\n",
       "          0.26551947,  0.10121416],\n",
       "        [-0.09629262,  0.03151209,  0.05064084, ..., -0.05414432,\n",
       "          0.02203098,  0.22853279]], dtype=float32),\n",
       " array([[-2.4746756e-01, -6.9213691e-03,  3.6422197e-02, ...,\n",
       "         -4.8762299e-03, -7.1392496e-05,  2.1759983e-02],\n",
       "        [ 1.1070005e-02, -1.7631239e-01, -3.3897568e-02, ...,\n",
       "         -9.1151871e-02, -2.9015582e-02,  1.0520754e-02],\n",
       "        [ 8.2095169e-02,  6.5504182e-03, -2.4373038e-01, ...,\n",
       "         -2.2073040e-02, -4.1511089e-02,  5.5250698e-03],\n",
       "        ...,\n",
       "        [ 7.1010210e-02,  4.8721144e-03, -5.6670003e-02, ...,\n",
       "         -2.5883064e-01,  5.6565437e-02,  1.1971135e-01],\n",
       "        [-1.0291463e-01,  5.6507796e-02, -4.5619331e-02, ...,\n",
       "         -6.1856028e-02, -2.8569636e-01, -3.9083842e-02],\n",
       "        [ 9.2306457e-02, -1.4352874e-02, -3.1899157e-04, ...,\n",
       "          6.4273700e-02, -5.1719725e-02, -2.2005741e-01]], dtype=float32),\n",
       " array([[-3.5188842],\n",
       "        [-3.5770633],\n",
       "        [-3.5406144],\n",
       "        [ 3.562597 ],\n",
       "        [-3.5745158]], dtype=float32),\n",
       " array([-3.599369], dtype=float32)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp_model.model.get_weights()[2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YAMANE Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yam_train_and_evaluate_1_fold(yam, train_split, test_split):    \n",
    "    \n",
    "    # fit model\n",
    "    # the test split is only used to measure the test loss\n",
    "    yam.fit(train_split, test_split)    \n",
    "    # this step should not be required since the model is dynamically linked to the evaluator\n",
    "    yam.evaluator.set_ensemble(yam)\n",
    "    # generates predictions according to trained model\n",
    "    predictions = yam.evaluator.predict(test_split)\n",
    "    # this converts the tokens back to words for evaluation\n",
    "    test_tuples = data.token_to_words(test_split)\n",
    "    # here we have a scorer that will mark our effort according to this particular test split\n",
    "    scorer = semeval_eval.HypernymEvaluation(test_tuples)\n",
    "    # get scores\n",
    "    score_names, all_scores = scorer.get_evaluation_scores(predictions)\n",
    "    # initialise scores (MRR, MAP, ...)\n",
    "    scores = {s:0.0 for s in score_names }\n",
    "    for k in range(len(score_names)):    \n",
    "        scores[score_names[k]] = float('%.5f' % (sum([score_list[k] for score_list in all_scores]) / len(all_scores)))    \n",
    "\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings_layer2 = yamane_model.get_embeddings_model(data.embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# initialise Yamane Model\n",
    "args={'data':data, 'embeddings_layer': embeddings_layer2, 'lr':0.001,'lambda_c':0.16, \n",
    "      'negative_sample_n':5, 'epochs':10, 'save_path':'glove_yamane_016.npz', 'patience':2}\n",
    "\n",
    "# generate parameter combinations\n",
    "_lambda_c = [0.16]\n",
    "_neg_count = [5]\n",
    "\n",
    "parameters = [_lambda_c, _neg_count]\n",
    "param_list = list(product(*parameters))\n",
    "\n",
    "yummy = yamane_model.YamaneEnsemble(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialise final_scores dictionary\n",
    "final_scores = {k:defaultdict(list) for k in param_list}\n",
    "\n",
    "for idx2, _param in enumerate(param_list):\n",
    "    print (\"Running test with following parameters: lambda_c: %0.2f; neg_count: %d\" \\\n",
    "           % (_param[0], _param[1]))\n",
    "    \n",
    "    args['lambda_c'] = _param[0]\n",
    "    args['negative_sample_n'] = _param[1]    \n",
    "    \n",
    "    # iterate over every split to get score distribution\n",
    "    for idx, td in enumerate(train_data_split):              \n",
    "        yummy.reset_ensemble(args=args)\n",
    "        \n",
    "        scores = yam_train_and_evaluate_1_fold(yummy, td, test_data_split[idx])\n",
    "        for s, v  in scores.items():\n",
    "            final_scores[_param][s].append(v)\n",
    "    print (\"\")\n",
    "    print (\"Finished %d from %d experiments\" % (idx2+1, len(param_list)))\n",
    "    print (\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yummy.sample_clusters\n",
    "Counter(yummy.sample_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_scores\n",
    "# nicer output of final scores\n",
    "for k, v in final_scores.items():    \n",
    "    lam = k[0]\n",
    "    neg = k[1]\n",
    "    for k2, v2 in v.items():        \n",
    "        print (\"%0.2f,%d,%s,%0.5f,%0.5f,%0.5f,%0.5f,%0.5f\" % (lam, neg, k2, v2[0], v2[1], v2[2], v2[3], v2[4]) )\n",
    "\n",
    "# get mean scores        \n",
    "for v in final_scores.values():\n",
    "    for k, v2 in v.items():\n",
    "        print (\"%s: %0.5f\" % (k, np.mean(v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek into the one of the clusters\n",
    "for c in range(len(Counter(yummy.sample_clusters))):\n",
    "    print (\"Cluster %d\" % (c))\n",
    "    for idx, i in enumerate(np.where(yummy.sample_clusters == c)[0]):\n",
    "        if idx < 30:\n",
    "            print (data.tokenizer.sequences_to_texts([train_data_split[4][i]]))\n",
    "        else:\n",
    "            break\n",
    "    print (\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hypernym representation by total in cluster\n",
    "def get_hypernym_rep_in_cluster(fold, clusters, c, top_running_perc):\n",
    "    hyper_freq = list(map(lambda w:data.tokenizer.index_word[w], fold[:,1][np.where(clusters == c)]))\n",
    "    # group hypernyms and count instances\n",
    "    hyper_freq = Counter(hyper_freq)\n",
    "    total_pairs = sum(hyper_freq.values())    \n",
    "    total_uniq_hypers = len(hyper_freq.keys())\n",
    "    \n",
    "    running_total = 0.\n",
    "    result = []\n",
    "    for count, word in sorted(((value, key) for (key,value) in hyper_freq.items()), reverse = True):                \n",
    "        perc_total = round(count / (1. * total_pairs), 5)\n",
    "        running_total += perc_total        \n",
    "        result.append((word, count, total_uniq_hypers, perc_total))        \n",
    "        if running_total > top_running_perc:\n",
    "            break\n",
    "    return result\n",
    "\n",
    "for c in range(len(Counter(yummy.sample_clusters))):\n",
    "    print (\"Cluster %d\" % (c))\n",
    "    for word, pairs, tot, perc_pairs in get_hypernym_rep_in_cluster(train_data_split[4], yummy.sample_clusters, c, 0.4):\n",
    "        print (\"%s,%d,%d,%0.5f\" % (word, pairs, tot, perc_pairs))\n",
    "    print (\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'yamane_evaluator' from '/home/jovyan/work/hyperstar/yamane_evaluator.py'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(crim_data)\n",
    "reload(multiprojection_model)\n",
    "reload(crim_evaluator)\n",
    "reload(yamane_model)\n",
    "reload(yamane_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n"
     ]
    }
   ],
   "source": [
    "# get queries from tuples\n",
    "#hyp_model.evaluator.predict_word('mare')\n",
    "predictions = hyp_model.evaluator.predict(test_data_split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MRR': 0.46174,\n",
       " 'MAP': 0.30877,\n",
       " 'P@1': 0.41915,\n",
       " 'P@5': 0.30348,\n",
       " 'P@10': 0.28967}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tuples = data.token_to_words(test_data_split[0])\n",
    "scorer = semeval_eval.HypernymEvaluation(test_tuples)\n",
    "\n",
    "# get scores\n",
    "score_names, all_scores = scorer.get_evaluation_scores(predictions)\n",
    "\n",
    "scores = {s:0.0 for s in score_names }\n",
    "\n",
    "for k in range(len(score_names)):    \n",
    "    scores[score_names[k]] = float('%.5f' % (sum([score_list[k] for score_list in all_scores]) / len(all_scores)))    \n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "# create score dictionary\n",
    "_clusters = [1, 5, 10]\n",
    "_lambda_c = [0, 0.1, 1]\n",
    "_neg_count = [1, 5, 10]\n",
    "\n",
    "parameters = [_clusters, _lambda_c, _neg_count]\n",
    "\n",
    "param_list = list(product(*parameters))\n",
    "final_scores = {k:defaultdict(list) for k in param_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = yummy.evaluator.predict(test_data_split[4])\n",
    "test_tuples = data.token_to_words(test_data_split[4])\n",
    "scorer = semeval_eval.HypernymEvaluation(test_tuples)\n",
    "# get scores\n",
    "score_names, all_scores = scorer.get_evaluation_scores(predictions)\n",
    "# initialise scores (MRR, MAP, ...)\n",
    "scores = {s:0.0 for s in score_names }\n",
    "for k in range(len(score_names)):    \n",
    "    scores[score_names[k]] = float('%.5f' % (sum([score_list[k] for score_list in all_scores]) / len(all_scores)))    \n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

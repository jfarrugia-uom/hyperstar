{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_sim.nn_vec import nn_vec\n",
    "import argparse\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "#from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from projlearn import MODELS\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build arguments dictionary\n",
    "args = {}\n",
    "#args['w2v'] = 'GoogleNews-vectors-negative300.bin'\n",
    "args['w2v'] = \"wiki-news-300d-1M-subword.vec\"\n",
    "args['test'] = 'test.npz'\n",
    "#args['test'] = 'validation.npz'\n",
    "args['subsumptions'] = 'subsumptions-test.txt'\n",
    "#args['subsumptions'] = 'subsumptions-validation.txt'\n",
    "args['non_optimized'] = False\n",
    "args['threads'] = cpu_count()\n",
    "#args['path'] = ['./en_300-k25-l0.0', './en_300-k25-l1.0']\n",
    "args['path'] = ['./ft-300-k25-l0.1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w2v': 'wiki-news-300d-1M-subword.vec',\n",
       " 'test': 'test.npz',\n",
       " 'subsumptions': 'subsumptions-test.txt',\n",
       " 'non_optimized': False,\n",
       " 'threads': 4,\n",
       " 'path': ['./ft-300-k25-l0.1']}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double checks arguments\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec pre-trained embeddings\n",
    "w2v = Word2Vec.load_word2vec_format(args['w2v'], binary=True, unicode_errors='ignore')\n",
    "w2v.init_sims(replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format(args['w2v'] + \".bin\", binary=True)\n",
    "w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1535 1535\n"
     ]
    }
   ],
   "source": [
    "with np.load(args['test']) as npz:\n",
    "    X_index_test  = npz['X_index']\n",
    "    Y_all_test    = npz['Y_all']\n",
    "    Z_all_test    = npz['Z_all']\n",
    "\n",
    "X_all_test  = Z_all_test[X_index_test[:, 0],   :]\n",
    "\n",
    "subsumptions_test = []\n",
    "\n",
    "with open(args['subsumptions']) as f:\n",
    "    reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "    for row in reader:\n",
    "        subsumptions_test.append((row[0], row[1]))\n",
    "\n",
    "# remove out-of-vocab entries\n",
    "def confirmVocab(wordList):\n",
    "    return [*filter(lambda x: x[0] in w2v.vocab and x[1] in w2v.vocab, wordList)]\n",
    "\n",
    "\n",
    "subsumptions_test = confirmVocab(subsumptions_test)\n",
    "\n",
    "print (len(subsumptions_test), X_all_test.shape[0])\n",
    "assert len(subsumptions_test) == X_all_test.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(clusters, Y_hat_clusters):\n",
    "    cluster_indices = {cluster: 0 for cluster in Y_hat_clusters}\n",
    "\n",
    "    Y_all_hat = []\n",
    "\n",
    "    for cluster in clusters:\n",
    "        Y_hat = Y_hat_clusters[cluster][cluster_indices[cluster]]\n",
    "        cluster_indices[cluster] += 1\n",
    "\n",
    "        Y_all_hat.append(Y_hat)\n",
    "\n",
    "    assert sum(cluster_indices.values()) == len(clusters)\n",
    "\n",
    "    return np.array(Y_all_hat)\n",
    "\n",
    "def compute_ats(measures):\n",
    "    return [sum(measures[j].values()) / len(subsumptions_test) for j in range(len(measures))]\n",
    "\n",
    "def compute_auc(ats):\n",
    "    return sum([ats[j] + ats[j + 1] for j in range(0, len(ats) - 1)]) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing \"./ft-300-k25-l0.1\" on \"test.npz\" and \"subsumptions-test.txt\".\n",
      "The number of clusters is 25.\n",
      "nn_vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:33: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n",
      "100 examples out of 1535 done for \"./ft-300-k25-l0.1/<class 'projlearn.regularized_synonym_phi.RegularizedSynonymPhi'>\": A@1=0.025407, A@2=0.028013, A@3=0.029316, A@4=0.029316, A@5=0.029316, A@6=0.029316, A@7=0.029316, A@8=0.029967, A@9=0.029967, A@10=0.029967. AUC=0.262215.\n",
      "200 examples out of 1535 done for \"./ft-300-k25-l0.1/<class 'projlearn.regularized_synonym_phi.RegularizedSynonymPhi'>\": A@1=0.055375, A@2=0.059283, A@3=0.061238, A@4=0.063192, A@5=0.063844, A@6=0.064495, A@7=0.064495, A@8=0.065798, A@9=0.065798, A@10=0.066450. AUC=0.569055.\n",
      "300 examples out of 1535 done for \"./ft-300-k25-l0.1/<class 'projlearn.regularized_synonym_phi.RegularizedSynonymPhi'>\": A@1=0.089902, A@2=0.097068, A@3=0.099023, A@4=0.102280, A@5=0.104235, A@6=0.104886, A@7=0.104886, A@8=0.106840, A@9=0.106840, A@10=0.107492. AUC=0.924756.\n",
      "400 examples out of 1535 done for \"./ft-300-k25-l0.1/<class 'projlearn.regularized_synonym_phi.RegularizedSynonymPhi'>\": A@1=0.124430, A@2=0.133550, A@3=0.136156, A@4=0.139414, A@5=0.141368, A@6=0.142671, A@7=0.143322, A@8=0.145277, A@9=0.146580, A@10=0.147231. AUC=1.264169.\n",
      "500 examples out of 1535 done for \"./ft-300-k25-l0.1/<class 'projlearn.regularized_synonym_phi.RegularizedSynonymPhi'>\": A@1=0.162215, A@2=0.172638, A@3=0.176547, A@4=0.180456, A@5=0.183062, A@6=0.184365, A@7=0.185668, A@8=0.188274, A@9=0.190228, A@10=0.190879. AUC=1.637785.\n",
      "600 examples out of 1535 done for \"./ft-300-k25-l0.1/<class 'projlearn.regularized_synonym_phi.RegularizedSynonymPhi'>\": A@1=0.190879, A@2=0.204560, A@3=0.210423, A@4=0.214984, A@5=0.218241, A@6=0.219544, A@7=0.220847, A@8=0.223453, A@9=0.225407, A@10=0.226059. AUC=1.945928.\n",
      "700 examples out of 1535 done for \"./ft-300-k25-l0.1/<class 'projlearn.regularized_synonym_phi.RegularizedSynonymPhi'>\": A@1=0.226059, A@2=0.242997, A@3=0.251466, A@4=0.256026, A@5=0.259283, A@6=0.260586, A@7=0.262541, A@8=0.265798, A@9=0.267752, A@10=0.268404. AUC=2.313681.\n",
      "800 examples out of 1535 done for \"./ft-300-k25-l0.1/<class 'projlearn.regularized_synonym_phi.RegularizedSynonymPhi'>\": A@1=0.263844, A@2=0.282085, A@3=0.291205, A@4=0.296417, A@5=0.299674, A@6=0.301629, A@7=0.303583, A@8=0.306840, A@9=0.308795, A@10=0.309446. AUC=2.676873.\n",
      "900 examples out of 1535 done for \"./ft-300-k25-l0.1/<class 'projlearn.regularized_synonym_phi.RegularizedSynonymPhi'>\": A@1=0.297068, A@2=0.319870, A@3=0.331596, A@4=0.337459, A@5=0.341368, A@6=0.343322, A@7=0.345277, A@8=0.348534, A@9=0.350489, A@10=0.351140. AUC=3.042020.\n",
      "1000 examples out of 1535 done for \"./ft-300-k25-l0.1/<class 'projlearn.regularized_synonym_phi.RegularizedSynonymPhi'>\": A@1=0.328990, A@2=0.353746, A@3=0.366775, A@4=0.373941, A@5=0.377850, A@6=0.379805, A@7=0.382410, A@8=0.385668, A@9=0.387622, A@10=0.388274. AUC=3.366450.\n",
      "1100 examples out of 1535 done for \"./ft-300-k25-l0.1/<class 'projlearn.regularized_synonym_phi.RegularizedSynonymPhi'>\": A@1=0.363518, A@2=0.390228, A@3=0.403257, A@4=0.410423, A@5=0.414332, A@6=0.417590, A@7=0.420195, A@8=0.423453, A@9=0.425407, A@10=0.426059. AUC=3.699674.\n",
      "1200 examples out of 1535 done for \"./ft-300-k25-l0.1/<class 'projlearn.regularized_synonym_phi.RegularizedSynonymPhi'>\": A@1=0.399349, A@2=0.428013, A@3=0.442997, A@4=0.450163, A@5=0.454723, A@6=0.457980, A@7=0.460586, A@8=0.463844, A@9=0.465798, A@10=0.466450. AUC=4.057003.\n",
      "1300 examples out of 1535 done for \"./ft-300-k25-l0.1/<class 'projlearn.regularized_synonym_phi.RegularizedSynonymPhi'>\": A@1=0.425407, A@2=0.457980, A@3=0.472964, A@4=0.480130, A@5=0.484691, A@6=0.488599, A@7=0.491205, A@8=0.494463, A@9=0.496417, A@10=0.497068. AUC=4.327687.\n",
      "1400 examples out of 1535 done for \"./ft-300-k25-l0.1/<class 'projlearn.regularized_synonym_phi.RegularizedSynonymPhi'>\": A@1=0.463844, A@2=0.497720, A@3=0.512704, A@4=0.519870, A@5=0.524430, A@6=0.528339, A@7=0.530945, A@8=0.534202, A@9=0.536156, A@10=0.536808. AUC=4.684691.\n",
      "1500 examples out of 1535 done for \"./ft-300-k25-l0.1/<class 'projlearn.regularized_synonym_phi.RegularizedSynonymPhi'>\": A@1=0.501629, A@2=0.537459, A@3=0.553746, A@4=0.560912, A@5=0.566775, A@6=0.570684, A@7=0.573941, A@8=0.577850, A@9=0.579805, A@10=0.580456. AUC=5.062215.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn_vec results covert...\n",
      "done\n",
      "For \"./ft-300-k25-l0.1/<class 'projlearn.regularized_synonym_phi.RegularizedSynonymPhi'>\": overall A@1=0.5101, A@2=0.5459, A@3=0.5622, A@4=0.5694, A@5=0.5752, A@6=0.5798, A@7=0.5831, A@8=0.5870, A@9=0.5889, A@10=0.5896. AUC=5.141368.\n"
     ]
    }
   ],
   "source": [
    "for path in args['path']:\n",
    "    print('Doing \"%s\" on \"%s\" and \"%s\".' % (path, args['test'], args['subsumptions']), flush=True)\n",
    "\n",
    "    kmeans = pickle.load(open(os.path.join(path, 'kmeans.pickle'), 'rb'))\n",
    "    print('The number of clusters is %d.' % (kmeans.n_clusters), flush=True)\n",
    "\n",
    "    clusters_test  = kmeans.predict(Y_all_test - X_all_test)\n",
    "    \n",
    "    #model_types = ['baseline', 'regularized_hyponym', 'regularized_synonym']\n",
    "    model_types = ['regularized_synonym_phi']\n",
    "    for m in model_types:        \n",
    "        model = MODELS[m]\n",
    "        try:\n",
    "            with np.load(os.path.join(path, '%s.test.npz') % m) as npz:\n",
    "                Y_hat_clusters = {int(cluster): npz[cluster] for cluster in npz.files}\n",
    "        except FileNotFoundError:\n",
    "            Y_hat_clusters = {}\n",
    "\n",
    "        if kmeans.n_clusters != len(Y_hat_clusters):\n",
    "            print('Missing the output for the model \"%s\"!' % model, file=sys.stderr, flush=True)\n",
    "            continue\n",
    "\n",
    "        Y_all_hat = extract(clusters_test, Y_hat_clusters)\n",
    "\n",
    "        assert len(subsumptions_test) == Y_all_hat.shape[0]\n",
    "\n",
    "        measures = [{} for _ in range(10)]\n",
    "\n",
    "        if not args['non_optimized']:\n",
    "            # normalize Y_all_hat to make dot product equeal to cosine and monotonically decreasing function of euclidean distance\n",
    "            Y_all_hat_norm = Y_all_hat / np.linalg.norm(Y_all_hat,axis=1)[:,np.newaxis]\n",
    "            print('nn_vec...')\n",
    "            similar_indices = nn_vec(Y_all_hat_norm, w2v.syn0norm, topn=10, sort=True, return_sims=False, nthreads=args['threads'], verbose=False)\n",
    "            print('nn_vec results covert...')\n",
    "            similar_words = [[w2v.index2word[ind] for ind in row] for row in similar_indices]\n",
    "            print('done')\n",
    "\n",
    "        for i, (hyponym, hypernym) in enumerate(subsumptions_test):\n",
    "            if args['non_optimized']:\n",
    "                Y_hat  = Y_all_hat[i].reshape(X_all_test.shape[1],)\n",
    "                actual = [w for w, _ in w2v.most_similar(positive=[Y_hat], topn=10)]\n",
    "            else:\n",
    "                actual = similar_words[i]\n",
    "\n",
    "            for j in range(0, len(measures)):\n",
    "                measures[j][(hyponym, hypernym)] = 1. if hypernym in actual[:j + 1] else 0.\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                ats = compute_ats(measures)\n",
    "                auc = compute_auc(ats)\n",
    "                ats_string = ', '.join(['A@%d=%.6f' % (j + 1, ats[j]) for j in range(len(ats))])\n",
    "                print('%d examples out of %d done for \"%s/%s\": %s. AUC=%.6f.' % (\n",
    "                    i + 1,\n",
    "                    len(subsumptions_test),\n",
    "                    path,\n",
    "                    model,\n",
    "                    ats_string,\n",
    "                    auc),\n",
    "                file=sys.stderr, flush=True)\n",
    "\n",
    "        ats = compute_ats(measures)\n",
    "        auc = compute_auc(ats)\n",
    "        ats_string = ', '.join(['A@%d=%.4f' % (j + 1, ats[j]) for j in range(len(ats))])\n",
    "        print('For \"%s/%s\": overall %s. AUC=%.6f.' % (\n",
    "            path,\n",
    "            model,\n",
    "            ats_string,\n",
    "            auc),\n",
    "        flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

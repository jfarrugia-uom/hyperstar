{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "import io\n",
    "import os\n",
    "import codecs\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "# very useful feature used to reload python modules\n",
    "from importlib import reload\n",
    "\n",
    "# import module that loads data, tokenises the tuples, initialises the embeddings matrix\n",
    "import crim_data\n",
    "import semeval_data\n",
    "\n",
    "import multiprojection_model\n",
    "import multiprojection_dual\n",
    "# contains code to evaluate according to semeval2018 metrics\n",
    "import semeval_eval\n",
    "import crim_evaluator\n",
    "import crim_dual_evaluator\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemEval 2018, Task 9 Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise embeddings and normalise to unit-norm\n",
    "#model = KeyedVectors.load_word2vec_format('embeddings/w2v_umbc_8w_10n_300d.txt', binary=False)\n",
    "#model = KeyedVectors.load_word2vec_format('embeddings/glove_umbc_15w_300d.txt', binary=False)\n",
    "model = KeyedVectors.load_word2vec_format('embeddings/fast_umbc_5ng_8w_300d.vec', binary=False)\n",
    "\n",
    "#model.save_word2vec_format('embeddings/GoogleNews-vectors-negative300.txt', binary=False)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore words which don't feature in embeddings model\n",
    "def read_subsumptions(filenames, w2v, word_type='Both'):\n",
    "    hypo, hyper = filenames\n",
    "        \n",
    "    subsumptions = []\n",
    "    is_concept = []\n",
    "    \n",
    "    with open(hypo, mode='r') as f_hypo, open(hyper, mode='r') as f_hyper:         \n",
    "        for x, y in zip(f_hypo, f_hyper):\n",
    "            query, category = x.strip().split(\"\\t\")                        \n",
    "            query = query.replace(\" \", \"_\").lower()                        \n",
    "            y = y.strip()            \n",
    "            # check that we have embeddings for query word            \n",
    "            if (query in w2v and (word_type==category or word_type=='Both')):\n",
    "                for h in y.split(\"\\t\"):\n",
    "                    h = h.replace(\" \", \"_\").lower()\n",
    "                    if h in w2v:\n",
    "                        subsumptions.append((query, h))\n",
    "                        is_concept.append(0 if category=='Entity' else 1 )\n",
    "        \n",
    "        return is_concept, subsumptions\n",
    "                    \n",
    "# ignore vocab entries not having correspononding embeddings                                                \n",
    "def read_vocab(filename, w2v):\n",
    "        \n",
    "    vocab = []    \n",
    "    # load data itemsf\n",
    "    with open(filename, mode='r') as f:        \n",
    "        for word in f:\n",
    "            word = word.strip().replace(\" \",\"_\").lower()            \n",
    "            if word in w2v:\n",
    "                vocab.append(word)\n",
    "                              \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept\n",
      "Tuples in validation set: 112; tuples in test set: 4935; tuples in training: 7182\n",
      "Unique hyponyms in validation set: 30; hyponyms in test set: 1057; hyponyms in training: 978\n",
      "------------------------------\n",
      "Entity\n",
      "Tuples in validation set: 88; tuples in test set: 2112; tuples in training: 4595\n",
      "Unique hyponyms in validation set: 20; hyponyms in test set: 443; hyponyms in training: 521\n",
      "------------------------------\n",
      "Both\n",
      "Tuples in validation set: 200; tuples in test set: 7047; tuples in training: 11777\n",
      "Unique hyponyms in validation set: 50; hyponyms in test set: 1499; hyponyms in training: 1499\n",
      "------------------------------\n",
      "Vocab size: 218106\n"
     ]
    }
   ],
   "source": [
    "# import SemEval data\n",
    "data_file_names = list(map(lambda x: '1A.english.%s.data.txt'%(x), ['trial', 'test', 'training']))\n",
    "gold_file_names = list(map(lambda x: '1A.english.%s.gold.txt'%(x), ['trial', 'test', 'training']))\n",
    "vocab_file_name = '1A.english.vocabulary.txt'\n",
    "\n",
    "file_names = list(zip(data_file_names, gold_file_names))\n",
    "# 0 = validation; 1 = test; 2 = training\n",
    "# create a dictionary for every dataset which maintains concepts, entities, and both\n",
    "validation, test, training = {}, {}, {}\n",
    "word_types = ['Concept', 'Entity','Both']\n",
    "for w in word_types:\n",
    "    validation[w] = read_subsumptions(file_names[0], model, w)[1] \n",
    "    test[w] = read_subsumptions(file_names[1], model, w)[1]\n",
    "    if w == 'Both':\n",
    "        is_concept, training[w] = read_subsumptions(file_names[2], model, w)\n",
    "    else:\n",
    "        training[w] = read_subsumptions(file_names[2], model, w)[1]\n",
    "\n",
    "vocabulary = read_vocab(vocab_file_name, model)\n",
    "    \n",
    "# create hypernym dictionary\n",
    "hyper_dict = defaultdict(list)\n",
    "for x, y in validation['Both'] + test['Both'] + training['Both']:\n",
    "    hyper_dict[x].append(y)\n",
    "    \n",
    "hyper_dict.default_factory = None\n",
    "\n",
    "# print some quick stats\n",
    "for w in word_types:\n",
    "    print (w)\n",
    "    print (\"Tuples in validation set: %d; tuples in test set: %d; tuples in training: %d\" \n",
    "          % (len(validation[w]), len(test[w]), len(training[w])))\n",
    "    \n",
    "    print (\"Unique hyponyms in validation set: %d; hyponyms in test set: %d; hyponyms in training: %d\"\n",
    "          % (len(set([x for (x,y) in validation[w]])), len(set([x for (x,y) in test[w]])), len(set([x for (x,y) in training[w]]))))\n",
    "    print (\"-\"*30)\n",
    "    \n",
    "print (\"Vocab size: %d\" % len(vocabulary))\n",
    "\n",
    "# 647 terms were missing from the model vocab due to them appearing only once in the corpus.\n",
    "# According to the technical paper, only words appearing at least 5 times were considered for vocab \n",
    "# from within the general-purpose corpus\n",
    "\n",
    "# Also the word épée, is found without accents in the model.  Don't know why. But we'll ignore this single\n",
    "# word since it should not make a difference in the scheme of things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising SemevalData...\n",
      "Creating tokenizer\n",
      "Dataset vocabulary size is 219034\n",
      "Vocab size is 219034 words\n",
      "Initialising negative sampler\n",
      "Tokenising all dataset tuples\n",
      "Creating embeddings matrix\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "args = {'w2v':model,\n",
    "        'train':training, 'test':test, 'validation':validation, 'vocabulary':vocabulary, \n",
    "        'is_concept':is_concept        \n",
    "       }\n",
    "\n",
    "data = semeval_data.SemevalData(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(hyp_model, train_split, valid_split, test_split):    \n",
    "    \n",
    "    # fit model\n",
    "    # the test split is only used to measure the test loss\n",
    "    hyp_model.fit(train_split, valid_split)    \n",
    "    # this step should not be required since the model is dynamically linked to the evaluator\n",
    "    hyp_model.evaluator.set_model(hyp_model.model)\n",
    "    \n",
    "    scores_all = []\n",
    "    # evaluate trained model on word in either category separately and together\n",
    "    for w in ['Concept', 'Entity', 'Both']:\n",
    "        print (\"Evaluating model on %s\" % (w) )\n",
    "        # generates predictions according to trained model\n",
    "        predictions = hyp_model.evaluator.predict(test_split[w])\n",
    "        # this converts the tokens back to words for evaluation\n",
    "        test_tuples = data.token_to_words(test_split[w])\n",
    "        # here we have a scorer that will mark our effort according to this particular test split\n",
    "        scorer = semeval_eval.HypernymEvaluation(test_tuples)\n",
    "        # get scores\n",
    "        score_names, all_scores = scorer.get_evaluation_scores(predictions)\n",
    "        # initialise scores (MRR, MAP, ...)\n",
    "        scores = {s:0.0 for s in score_names }\n",
    "        for k in range(len(score_names)):    \n",
    "            scores[score_names[k]] = float('%.5f' % (sum([score_list[k] for score_list in all_scores]) / len(all_scores)))    \n",
    "        \n",
    "        scores_all.append(scores)\n",
    "    return scores_all\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI-PROJECTION Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_layer = multiprojection_model.get_embeddings_model(data.embeddings_matrix, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard model parameters - we won't be changing these\n",
    "args['data']              = data\n",
    "args['embeddings_layer']  = embeddings_layer\n",
    "args['epochs']            = 15\n",
    "args['batch_size']        = 32\n",
    "args['synonym_sample_n']  = 1\n",
    "args['phi_k']             = 1\n",
    "args['lambda_c']          = 0.\n",
    "args['negative_sample_n'] = 10\n",
    "args['save_path']         = 'ft_semeval.npz'\n",
    "args['patience']          = 3\n",
    "args['eval_after_epoch']  = True\n",
    "args['lr']                = 0.001\n",
    "args['beta1']             = 0.9\n",
    "args['beta2']             = 0.9\n",
    "args['clip_value']        = 1.\n",
    "\n",
    "# generate parameter combinations\n",
    "_clusters = [10, 5, 1]\n",
    "#_clusters = [10]\n",
    "#_lambda_c = [0, 0.1, 1]\n",
    "_lambda_c = [0., 0.1, 1.]\n",
    "#_neg_count = [10, 5, 1]\n",
    "_neg_count = [10]\n",
    "\n",
    "parameters = [_clusters, _lambda_c, _neg_count]\n",
    "\n",
    "param_list = list(product(*parameters))\n",
    "\n",
    "# initialise hypernymy discovery model which we will reuse by resetting the model with new args\n",
    "hyp_model = multiprojection_model.MultiProjModel(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with following parameters: phi_k: 10; lambda_c: 0.00; neg_count: 10\n",
      "Training model on Both\n",
      "Fitting model with following parameters:\n",
      " batch_size=32;\n",
      " phi_k=10;\n",
      " lambda_c=0.00;\n",
      " epochs=15;\n",
      " negative_count=10;\n",
      " synonym_count=1\n",
      "Optimizer parameters:\n",
      " lr=0.00100;\n",
      " beta1=0.900;\n",
      " beta2=0.900;\n",
      " clip=1.00\n",
      "--------------------\n",
      "Epoch: 1; Training Loss: 0.44248; Test Loss: 0.27799; Test MAP: 0.00080; Test MRR: 0.00200\n",
      "Epoch: 2; Training Loss: 0.25851; Test Loss: 0.19276; Test MAP: 0.07218; Test MRR: 0.13889\n",
      "Epoch: 3; Training Loss: 0.19556; Test Loss: 0.14808; Test MAP: 0.10019; Test MRR: 0.19450\n",
      "Epoch: 4; Training Loss: 0.15900; Test Loss: 0.13329; Test MAP: 0.11655; Test MRR: 0.20286\n",
      "Epoch: 5; Training Loss: 0.13677; Test Loss: 0.11714; Test MAP: 0.12212; Test MRR: 0.21852\n",
      "Epoch: 6; Training Loss: 0.12074; Test Loss: 0.12016; Test MAP: 0.09525; Test MRR: 0.18300\n",
      "Epoch: 7; Training Loss: 0.11036; Test Loss: 0.11579; Test MAP: 0.12051; Test MRR: 0.21667\n",
      "Epoch: 8; Training Loss: 0.10157; Test Loss: 0.11310; Test MAP: 0.13518; Test MRR: 0.23567\n",
      "Epoch: 9; Training Loss: 0.09707; Test Loss: 0.11033; Test MAP: 0.14450; Test MRR: 0.26286\n",
      "Epoch: 10; Training Loss: 0.09132; Test Loss: 0.11576; Test MAP: 0.12602; Test MRR: 0.23067\n",
      "Epoch: 11; Training Loss: 0.08826; Test Loss: 0.12231; Test MAP: 0.13153; Test MRR: 0.23583\n",
      "Epoch: 12; Training Loss: 0.08654; Test Loss: 0.12655; Test MAP: 0.14990; Test MRR: 0.25952\n",
      "Epoch: 13; Training Loss: 0.08463; Test Loss: 0.11332; Test MAP: 0.15216; Test MRR: 0.26098\n",
      "Epoch: 14; Training Loss: 0.08231; Test Loss: 0.11390; Test MAP: 0.15390; Test MRR: 0.26400\n",
      "Epoch: 15; Training Loss: 0.08197; Test Loss: 0.12228; Test MAP: 0.17525; Test MRR: 0.30367\n",
      "Done!\n",
      "Evaluating model on Concept\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Evaluating model on Entity\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Evaluating model on Both\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Done 1100\n",
      "Done 1200\n",
      "Done 1300\n",
      "Done 1400\n",
      "\n",
      "Finished 1 from 9 experiments\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 10; lambda_c: 0.10; neg_count: 10\n",
      "Training model on Both\n",
      "Fitting model with following parameters:\n",
      " batch_size=32;\n",
      " phi_k=10;\n",
      " lambda_c=0.10;\n",
      " epochs=15;\n",
      " negative_count=10;\n",
      " synonym_count=1\n",
      "Optimizer parameters:\n",
      " lr=0.00100;\n",
      " beta1=0.900;\n",
      " beta2=0.900;\n",
      " clip=1.00\n",
      "--------------------\n",
      "Epoch: 1; Training Loss: 0.43273; Test Loss: 0.27346; Test MAP: 0.00179; Test MRR: 0.00400\n",
      "Epoch: 2; Training Loss: 0.25613; Test Loss: 0.19557; Test MAP: 0.06801; Test MRR: 0.14133\n",
      "Epoch: 3; Training Loss: 0.19398; Test Loss: 0.14961; Test MAP: 0.08328; Test MRR: 0.15650\n",
      "Epoch: 4; Training Loss: 0.15914; Test Loss: 0.12867; Test MAP: 0.11189; Test MRR: 0.22186\n",
      "Epoch: 5; Training Loss: 0.13640; Test Loss: 0.11499; Test MAP: 0.12226; Test MRR: 0.23186\n",
      "Epoch: 6; Training Loss: 0.12044; Test Loss: 0.11350; Test MAP: 0.12441; Test MRR: 0.23119\n",
      "Epoch: 7; Training Loss: 0.11034; Test Loss: 0.10832; Test MAP: 0.10808; Test MRR: 0.19686\n",
      "Epoch: 8; Training Loss: 0.10156; Test Loss: 0.11590; Test MAP: 0.13285; Test MRR: 0.23933\n",
      "Epoch: 9; Training Loss: 0.09615; Test Loss: 0.10901; Test MAP: 0.12072; Test MRR: 0.21738\n",
      "Epoch: 10; Training Loss: 0.09286; Test Loss: 0.11062; Test MAP: 0.11130; Test MRR: 0.22717\n",
      "Epoch: 11; Training Loss: 0.08925; Test Loss: 0.12332; Test MAP: 0.11866; Test MRR: 0.21583\n",
      "Early Stop invoked at epoch 11\n",
      "Done!\n",
      "Evaluating model on Concept\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Evaluating model on Entity\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Evaluating model on Both\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Done 1100\n",
      "Done 1200\n",
      "Done 1300\n",
      "Done 1400\n",
      "\n",
      "Finished 2 from 9 experiments\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 10; lambda_c: 1.00; neg_count: 10\n",
      "Training model on Both\n",
      "Fitting model with following parameters:\n",
      " batch_size=32;\n",
      " phi_k=10;\n",
      " lambda_c=1.00;\n",
      " epochs=15;\n",
      " negative_count=10;\n",
      " synonym_count=1\n",
      "Optimizer parameters:\n",
      " lr=0.00100;\n",
      " beta1=0.900;\n",
      " beta2=0.900;\n",
      " clip=1.00\n",
      "--------------------\n",
      "Epoch: 1; Training Loss: 0.45255; Test Loss: 0.27989; Test MAP: 0.00153; Test MRR: 0.00333\n",
      "Epoch: 2; Training Loss: 0.26126; Test Loss: 0.19599; Test MAP: 0.07393; Test MRR: 0.14619\n",
      "Epoch: 3; Training Loss: 0.19739; Test Loss: 0.15011; Test MAP: 0.09371; Test MRR: 0.17900\n",
      "Epoch: 4; Training Loss: 0.16054; Test Loss: 0.13411; Test MAP: 0.10413; Test MRR: 0.18333\n",
      "Epoch: 5; Training Loss: 0.13759; Test Loss: 0.12064; Test MAP: 0.12484; Test MRR: 0.23400\n",
      "Epoch: 6; Training Loss: 0.12178; Test Loss: 0.11029; Test MAP: 0.12711; Test MRR: 0.20750\n",
      "Epoch: 7; Training Loss: 0.10975; Test Loss: 0.11940; Test MAP: 0.15593; Test MRR: 0.24300\n",
      "Epoch: 8; Training Loss: 0.10233; Test Loss: 0.11016; Test MAP: 0.12953; Test MRR: 0.23483\n",
      "Epoch: 9; Training Loss: 0.09736; Test Loss: 0.10128; Test MAP: 0.13590; Test MRR: 0.23452\n",
      "Epoch: 10; Training Loss: 0.09219; Test Loss: 0.11931; Test MAP: 0.12290; Test MRR: 0.21833\n",
      "Early Stop invoked at epoch 10\n",
      "Done!\n",
      "Evaluating model on Concept\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Evaluating model on Entity\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Evaluating model on Both\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Done 1100\n",
      "Done 1200\n",
      "Done 1300\n",
      "Done 1400\n",
      "\n",
      "Finished 3 from 9 experiments\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 5; lambda_c: 0.00; neg_count: 10\n",
      "Training model on Both\n",
      "Fitting model with following parameters:\n",
      " batch_size=32;\n",
      " phi_k=5;\n",
      " lambda_c=0.00;\n",
      " epochs=15;\n",
      " negative_count=10;\n",
      " synonym_count=1\n",
      "Optimizer parameters:\n",
      " lr=0.00100;\n",
      " beta1=0.900;\n",
      " beta2=0.900;\n",
      " clip=1.00\n",
      "--------------------\n",
      "Epoch: 1; Training Loss: 0.51323; Test Loss: 0.35128; Test MAP: 0.00000; Test MRR: 0.00000\n",
      "Epoch: 2; Training Loss: 0.33076; Test Loss: 0.26023; Test MAP: 0.01018; Test MRR: 0.01317\n",
      "Epoch: 3; Training Loss: 0.25719; Test Loss: 0.21007; Test MAP: 0.06136; Test MRR: 0.12367\n",
      "Epoch: 4; Training Loss: 0.21804; Test Loss: 0.17458; Test MAP: 0.08557; Test MRR: 0.16467\n",
      "Epoch: 5; Training Loss: 0.19085; Test Loss: 0.15074; Test MAP: 0.09816; Test MRR: 0.20433\n",
      "Epoch: 6; Training Loss: 0.17051; Test Loss: 0.14487; Test MAP: 0.10530; Test MRR: 0.18700\n",
      "Epoch: 7; Training Loss: 0.15416; Test Loss: 0.13084; Test MAP: 0.10427; Test MRR: 0.19286\n",
      "Epoch: 8; Training Loss: 0.14109; Test Loss: 0.12044; Test MAP: 0.14208; Test MRR: 0.27515\n",
      "Epoch: 9; Training Loss: 0.13163; Test Loss: 0.11910; Test MAP: 0.11062; Test MRR: 0.21000\n",
      "Epoch: 10; Training Loss: 0.12278; Test Loss: 0.12183; Test MAP: 0.12171; Test MRR: 0.22983\n",
      "Epoch: 11; Training Loss: 0.11567; Test Loss: 0.11443; Test MAP: 0.14748; Test MRR: 0.26686\n",
      "Epoch: 12; Training Loss: 0.10881; Test Loss: 0.11909; Test MAP: 0.11512; Test MRR: 0.23067\n",
      "Epoch: 13; Training Loss: 0.10553; Test Loss: 0.10985; Test MAP: 0.14179; Test MRR: 0.24833\n",
      "Epoch: 14; Training Loss: 0.10141; Test Loss: 0.12074; Test MAP: 0.13173; Test MRR: 0.26222\n",
      "Early Stop invoked at epoch 14\n",
      "Done!\n",
      "Evaluating model on Concept\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Evaluating model on Entity\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Evaluating model on Both\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Done 1100\n",
      "Done 1200\n",
      "Done 1300\n",
      "Done 1400\n",
      "\n",
      "Finished 4 from 9 experiments\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 5; lambda_c: 0.10; neg_count: 10\n",
      "Training model on Both\n",
      "Fitting model with following parameters:\n",
      " batch_size=32;\n",
      " phi_k=5;\n",
      " lambda_c=0.10;\n",
      " epochs=15;\n",
      " negative_count=10;\n",
      " synonym_count=1\n",
      "Optimizer parameters:\n",
      " lr=0.00100;\n",
      " beta1=0.900;\n",
      " beta2=0.900;\n",
      " clip=1.00\n",
      "--------------------\n",
      "Epoch: 1; Training Loss: 0.51458; Test Loss: 0.35348; Test MAP: 0.00000; Test MRR: 0.00000\n",
      "Epoch: 2; Training Loss: 0.33136; Test Loss: 0.26110; Test MAP: 0.00902; Test MRR: 0.01167\n",
      "Epoch: 3; Training Loss: 0.25777; Test Loss: 0.20855; Test MAP: 0.06833; Test MRR: 0.13254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4; Training Loss: 0.21803; Test Loss: 0.17618; Test MAP: 0.07017; Test MRR: 0.14867\n",
      "Epoch: 5; Training Loss: 0.19044; Test Loss: 0.15555; Test MAP: 0.06695; Test MRR: 0.12241\n",
      "Epoch: 6; Training Loss: 0.17005; Test Loss: 0.13644; Test MAP: 0.12920; Test MRR: 0.22810\n",
      "Epoch: 7; Training Loss: 0.15478; Test Loss: 0.12589; Test MAP: 0.12358; Test MRR: 0.23733\n",
      "Epoch: 8; Training Loss: 0.14159; Test Loss: 0.12316; Test MAP: 0.10282; Test MRR: 0.19333\n",
      "Epoch: 9; Training Loss: 0.13181; Test Loss: 0.12714; Test MAP: 0.12300; Test MRR: 0.21852\n",
      "Early Stop invoked at epoch 9\n",
      "Done!\n",
      "Evaluating model on Concept\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Evaluating model on Entity\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Evaluating model on Both\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Done 1100\n",
      "Done 1200\n",
      "Done 1300\n",
      "Done 1400\n",
      "\n",
      "Finished 5 from 9 experiments\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 5; lambda_c: 1.00; neg_count: 10\n",
      "Training model on Both\n",
      "Fitting model with following parameters:\n",
      " batch_size=32;\n",
      " phi_k=5;\n",
      " lambda_c=1.00;\n",
      " epochs=15;\n",
      " negative_count=10;\n",
      " synonym_count=1\n",
      "Optimizer parameters:\n",
      " lr=0.00100;\n",
      " beta1=0.900;\n",
      " beta2=0.900;\n",
      " clip=1.00\n",
      "--------------------\n",
      "Epoch: 1; Training Loss: 0.51177; Test Loss: 0.35146; Test MAP: 0.00000; Test MRR: 0.00000\n",
      "Epoch: 2; Training Loss: 0.33077; Test Loss: 0.25839; Test MAP: 0.01868; Test MRR: 0.02567\n",
      "Epoch: 3; Training Loss: 0.25728; Test Loss: 0.20862; Test MAP: 0.08069; Test MRR: 0.17250\n",
      "Epoch: 4; Training Loss: 0.21897; Test Loss: 0.17846; Test MAP: 0.12042; Test MRR: 0.22150\n",
      "Epoch: 5; Training Loss: 0.19189; Test Loss: 0.15327; Test MAP: 0.10034; Test MRR: 0.20100\n",
      "Epoch: 6; Training Loss: 0.17286; Test Loss: 0.14004; Test MAP: 0.09688; Test MRR: 0.18900\n",
      "Epoch: 7; Training Loss: 0.15759; Test Loss: 0.12989; Test MAP: 0.11180; Test MRR: 0.22567\n",
      "Early Stop invoked at epoch 7\n",
      "Done!\n",
      "Evaluating model on Concept\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Evaluating model on Entity\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Evaluating model on Both\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Done 1100\n",
      "Done 1200\n",
      "Done 1300\n",
      "Done 1400\n",
      "\n",
      "Finished 6 from 9 experiments\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 1; lambda_c: 0.00; neg_count: 10\n",
      "Training model on Both\n",
      "Fitting model with following parameters:\n",
      " batch_size=32;\n",
      " phi_k=1;\n",
      " lambda_c=0.00;\n",
      " epochs=15;\n",
      " negative_count=10;\n",
      " synonym_count=1\n",
      "Optimizer parameters:\n",
      " lr=0.00100;\n",
      " beta1=0.900;\n",
      " beta2=0.900;\n",
      " clip=1.00\n",
      "--------------------\n",
      "Epoch: 1; Training Loss: 0.59597; Test Loss: 0.49953; Test MAP: 0.00000; Test MRR: 0.00000\n",
      "Epoch: 2; Training Loss: 0.45540; Test Loss: 0.38324; Test MAP: 0.00000; Test MRR: 0.00000\n",
      "Epoch: 3; Training Loss: 0.36532; Test Loss: 0.31837; Test MAP: 0.00111; Test MRR: 0.00250\n",
      "Epoch: 4; Training Loss: 0.31339; Test Loss: 0.28407; Test MAP: 0.03591; Test MRR: 0.07686\n",
      "Epoch: 5; Training Loss: 0.28535; Test Loss: 0.26165; Test MAP: 0.06378; Test MRR: 0.11833\n",
      "Epoch: 6; Training Loss: 0.26969; Test Loss: 0.24772; Test MAP: 0.07340; Test MRR: 0.15967\n",
      "Epoch: 7; Training Loss: 0.26098; Test Loss: 0.23904; Test MAP: 0.08754; Test MRR: 0.16650\n",
      "Epoch: 8; Training Loss: 0.25485; Test Loss: 0.23034; Test MAP: 0.08638; Test MRR: 0.17233\n",
      "Epoch: 9; Training Loss: 0.24864; Test Loss: 0.22367; Test MAP: 0.09548; Test MRR: 0.18539\n",
      "Epoch: 10; Training Loss: 0.24252; Test Loss: 0.21577; Test MAP: 0.08963; Test MRR: 0.17150\n",
      "Epoch: 11; Training Loss: 0.23728; Test Loss: 0.20812; Test MAP: 0.09151; Test MRR: 0.20300\n",
      "Epoch: 12; Training Loss: 0.23160; Test Loss: 0.20285; Test MAP: 0.09649; Test MRR: 0.19752\n",
      "Epoch: 13; Training Loss: 0.22627; Test Loss: 0.19711; Test MAP: 0.08732; Test MRR: 0.18000\n",
      "Epoch: 14; Training Loss: 0.22116; Test Loss: 0.19197; Test MAP: 0.08831; Test MRR: 0.16341\n",
      "Epoch: 15; Training Loss: 0.21592; Test Loss: 0.18768; Test MAP: 0.09642; Test MRR: 0.18650\n",
      "Early Stop invoked at epoch 15\n",
      "Done!\n",
      "Evaluating model on Concept\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Evaluating model on Entity\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Evaluating model on Both\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Done 1100\n",
      "Done 1200\n",
      "Done 1300\n",
      "Done 1400\n",
      "\n",
      "Finished 7 from 9 experiments\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 1; lambda_c: 0.10; neg_count: 10\n",
      "Training model on Both\n",
      "Fitting model with following parameters:\n",
      " batch_size=32;\n",
      " phi_k=1;\n",
      " lambda_c=0.10;\n",
      " epochs=15;\n",
      " negative_count=10;\n",
      " synonym_count=1\n",
      "Optimizer parameters:\n",
      " lr=0.00100;\n",
      " beta1=0.900;\n",
      " beta2=0.900;\n",
      " clip=1.00\n",
      "--------------------\n",
      "Epoch: 1; Training Loss: 0.60515; Test Loss: 0.51165; Test MAP: 0.00000; Test MRR: 0.00000\n",
      "Epoch: 2; Training Loss: 0.46301; Test Loss: 0.39246; Test MAP: 0.00000; Test MRR: 0.00000\n",
      "Epoch: 3; Training Loss: 0.37036; Test Loss: 0.32160; Test MAP: 0.01140; Test MRR: 0.02700\n",
      "Epoch: 4; Training Loss: 0.31592; Test Loss: 0.28570; Test MAP: 0.03565; Test MRR: 0.07000\n",
      "Epoch: 5; Training Loss: 0.28727; Test Loss: 0.26498; Test MAP: 0.07274; Test MRR: 0.14333\n",
      "Epoch: 6; Training Loss: 0.27293; Test Loss: 0.25496; Test MAP: 0.08697; Test MRR: 0.16250\n",
      "Epoch: 7; Training Loss: 0.26541; Test Loss: 0.24659; Test MAP: 0.07571; Test MRR: 0.16556\n",
      "Epoch: 8; Training Loss: 0.25922; Test Loss: 0.24060; Test MAP: 0.08699; Test MRR: 0.19886\n",
      "Epoch: 9; Training Loss: 0.25328; Test Loss: 0.23384; Test MAP: 0.07803; Test MRR: 0.18400\n",
      "Epoch: 10; Training Loss: 0.24743; Test Loss: 0.22573; Test MAP: 0.07622; Test MRR: 0.16917\n",
      "Epoch: 11; Training Loss: 0.24187; Test Loss: 0.22003; Test MAP: 0.07827; Test MRR: 0.17300\n",
      "Early Stop invoked at epoch 11\n",
      "Done!\n",
      "Evaluating model on Concept\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Evaluating model on Entity\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Evaluating model on Both\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Done 1100\n",
      "Done 1200\n",
      "Done 1300\n",
      "Done 1400\n",
      "\n",
      "Finished 8 from 9 experiments\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 1; lambda_c: 1.00; neg_count: 10\n",
      "Training model on Both\n",
      "Fitting model with following parameters:\n",
      " batch_size=32;\n",
      " phi_k=1;\n",
      " lambda_c=1.00;\n",
      " epochs=15;\n",
      " negative_count=10;\n",
      " synonym_count=1\n",
      "Optimizer parameters:\n",
      " lr=0.00100;\n",
      " beta1=0.900;\n",
      " beta2=0.900;\n",
      " clip=1.00\n",
      "--------------------\n",
      "Epoch: 1; Training Loss: 0.60806; Test Loss: 0.51547; Test MAP: 0.01206; Test MRR: 0.02833\n",
      "Epoch: 2; Training Loss: 0.46531; Test Loss: 0.39635; Test MAP: 0.01660; Test MRR: 0.03583\n",
      "Epoch: 3; Training Loss: 0.37297; Test Loss: 0.32505; Test MAP: 0.03446; Test MRR: 0.06119\n",
      "Epoch: 4; Training Loss: 0.31821; Test Loss: 0.28883; Test MAP: 0.04828; Test MRR: 0.10007\n",
      "Epoch: 5; Training Loss: 0.28957; Test Loss: 0.26998; Test MAP: 0.07050; Test MRR: 0.14483\n",
      "Epoch: 6; Training Loss: 0.27666; Test Loss: 0.26176; Test MAP: 0.07892; Test MRR: 0.16286\n",
      "Epoch: 7; Training Loss: 0.26978; Test Loss: 0.25444; Test MAP: 0.08168; Test MRR: 0.17400\n",
      "Epoch: 8; Training Loss: 0.26407; Test Loss: 0.24844; Test MAP: 0.07105; Test MRR: 0.17200\n",
      "Epoch: 9; Training Loss: 0.25835; Test Loss: 0.24344; Test MAP: 0.08019; Test MRR: 0.18000\n",
      "Epoch: 10; Training Loss: 0.25257; Test Loss: 0.23618; Test MAP: 0.07032; Test MRR: 0.15000\n",
      "Early Stop invoked at epoch 10\n",
      "Done!\n",
      "Evaluating model on Concept\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Evaluating model on Entity\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Evaluating model on Both\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Done 1100\n",
      "Done 1200\n",
      "Done 1300\n",
      "Done 1400\n",
      "\n",
      "Finished 9 from 9 experiments\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# initialise final_scores dictionary\n",
    "final_scores = {k:defaultdict(list) for k in param_list}\n",
    "\n",
    "for idx2, _param in enumerate(param_list):\n",
    "    print (\"Running test with following parameters: phi_k: %d; lambda_c: %0.2f; neg_count: %d\" \\\n",
    "           % (_param[0], _param[1], _param[2]))\n",
    "\n",
    "    args['phi_k'] = _param[0]\n",
    "    args['lambda_c'] = _param[1]\n",
    "    args['negative_sample_n'] = _param[2]    \n",
    "\n",
    "    for w in ['Both']:\n",
    "        # train model on three sets of data\n",
    "        print (\"Training model on %s\" % (w))\n",
    "        args['save_path'] = 'ft_semeval_%s.npz' % (w)\n",
    "        hyp_model.reset_model(args=args)\n",
    "\n",
    "        all_scores = train_and_evaluate(hyp_model, \n",
    "                                        data.train_data_token[w], \n",
    "                                        data.valid_data_token[w],\n",
    "                                        data.test_data_token)\n",
    "        # run predictions on test\n",
    "\n",
    "        for scores in all_scores:\n",
    "            for s, v  in scores.items():\n",
    "                final_scores[_param][s].append(v)\n",
    "\n",
    "    print (\"\")\n",
    "    print (\"Finished %d from %d experiments\" % (idx2+1, len(param_list)))\n",
    "    print (\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(10, 0.0, 10): defaultdict(list,\n",
       "             {'MRR': [0.19424, 0.41247, 0.2587],\n",
       "              'MAP': [0.09542, 0.20048, 0.12654],\n",
       "              'P@1': [0.14286, 0.33409, 0.19947],\n",
       "              'P@5': [0.09308, 0.19195, 0.12236],\n",
       "              'P@10': [0.08757, 0.18038, 0.11504]}),\n",
       " (10, 0.1, 10): defaultdict(list,\n",
       "             {'MRR': [0.20307, 0.38316, 0.25643],\n",
       "              'MAP': [0.0978, 0.19038, 0.12523],\n",
       "              'P@1': [0.15421, 0.30023, 0.19746],\n",
       "              'P@5': [0.09399, 0.18213, 0.1201],\n",
       "              'P@10': [0.08853, 0.17363, 0.11374]}),\n",
       " (10, 1.0, 10): defaultdict(list,\n",
       "             {'MRR': [0.20497, 0.385, 0.25831],\n",
       "              'MAP': [0.1004, 0.18851, 0.12648],\n",
       "              'P@1': [0.15043, 0.30474, 0.19613],\n",
       "              'P@5': [0.09724, 0.18134, 0.12216],\n",
       "              'P@10': [0.09173, 0.17209, 0.11552]}),\n",
       " (5, 0.0, 10): defaultdict(list,\n",
       "             {'MRR': [0.19229, 0.37878, 0.24758],\n",
       "              'MAP': [0.09421, 0.19232, 0.12328],\n",
       "              'P@1': [0.14191, 0.30248, 0.18946],\n",
       "              'P@5': [0.08988, 0.18363, 0.11765],\n",
       "              'P@10': [0.08747, 0.17776, 0.11421]}),\n",
       " (5, 0.1, 10): defaultdict(list,\n",
       "             {'MRR': [0.19051, 0.406, 0.25437],\n",
       "              'MAP': [0.09258, 0.19872, 0.12401],\n",
       "              'P@1': [0.1457, 0.32957, 0.20013],\n",
       "              'P@5': [0.08939, 0.18845, 0.11872],\n",
       "              'P@10': [0.08461, 0.18195, 0.11343]}),\n",
       " (5, 1.0, 10): defaultdict(list,\n",
       "             {'MRR': [0.17847, 0.35379, 0.2304],\n",
       "              'MAP': [0.08438, 0.16902, 0.10945],\n",
       "              'P@1': [0.13907, 0.28668, 0.18279],\n",
       "              'P@5': [0.08037, 0.15899, 0.10366],\n",
       "              'P@10': [0.07639, 0.15319, 0.09914]}),\n",
       " (1, 0.0, 10): defaultdict(list,\n",
       "             {'MRR': [0.17925, 0.38111, 0.23903],\n",
       "              'MAP': [0.08344, 0.1917, 0.11549],\n",
       "              'P@1': [0.14286, 0.30926, 0.19213],\n",
       "              'P@5': [0.08004, 0.18262, 0.11041],\n",
       "              'P@10': [0.07538, 0.17624, 0.10524]}),\n",
       " (1, 0.1, 10): defaultdict(list,\n",
       "             {'MRR': [0.16909, 0.36235, 0.22631],\n",
       "              'MAP': [0.07928, 0.17291, 0.10701],\n",
       "              'P@1': [0.13813, 0.30926, 0.18879],\n",
       "              'P@5': [0.07487, 0.16151, 0.10052],\n",
       "              'P@10': [0.07121, 0.15692, 0.09659]}),\n",
       " (1, 1.0, 10): defaultdict(list,\n",
       "             {'MRR': [0.15989, 0.402, 0.23154],\n",
       "              'MAP': [0.07203, 0.18562, 0.10565],\n",
       "              'P@1': [0.12867, 0.35214, 0.1948],\n",
       "              'P@5': [0.06783, 0.17144, 0.0985],\n",
       "              'P@10': [0.06409, 0.16626, 0.09433]})}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\t0.0\t10\tMRR\t0.19424\t0.41247\t0.25870\n",
      "10\t0.0\t10\tMAP\t0.09542\t0.20048\t0.12654\n",
      "10\t0.0\t10\tP@1\t0.14286\t0.33409\t0.19947\n",
      "10\t0.0\t10\tP@5\t0.09308\t0.19195\t0.12236\n",
      "10\t0.0\t10\tP@10\t0.08757\t0.18038\t0.11504\n",
      "10\t0.1\t10\tMRR\t0.20307\t0.38316\t0.25643\n",
      "10\t0.1\t10\tMAP\t0.09780\t0.19038\t0.12523\n",
      "10\t0.1\t10\tP@1\t0.15421\t0.30023\t0.19746\n",
      "10\t0.1\t10\tP@5\t0.09399\t0.18213\t0.12010\n",
      "10\t0.1\t10\tP@10\t0.08853\t0.17363\t0.11374\n",
      "10\t1.0\t10\tMRR\t0.20497\t0.38500\t0.25831\n",
      "10\t1.0\t10\tMAP\t0.10040\t0.18851\t0.12648\n",
      "10\t1.0\t10\tP@1\t0.15043\t0.30474\t0.19613\n",
      "10\t1.0\t10\tP@5\t0.09724\t0.18134\t0.12216\n",
      "10\t1.0\t10\tP@10\t0.09173\t0.17209\t0.11552\n",
      "5\t0.0\t10\tMRR\t0.19229\t0.37878\t0.24758\n",
      "5\t0.0\t10\tMAP\t0.09421\t0.19232\t0.12328\n",
      "5\t0.0\t10\tP@1\t0.14191\t0.30248\t0.18946\n",
      "5\t0.0\t10\tP@5\t0.08988\t0.18363\t0.11765\n",
      "5\t0.0\t10\tP@10\t0.08747\t0.17776\t0.11421\n",
      "5\t0.1\t10\tMRR\t0.19051\t0.40600\t0.25437\n",
      "5\t0.1\t10\tMAP\t0.09258\t0.19872\t0.12401\n",
      "5\t0.1\t10\tP@1\t0.14570\t0.32957\t0.20013\n",
      "5\t0.1\t10\tP@5\t0.08939\t0.18845\t0.11872\n",
      "5\t0.1\t10\tP@10\t0.08461\t0.18195\t0.11343\n",
      "5\t1.0\t10\tMRR\t0.17847\t0.35379\t0.23040\n",
      "5\t1.0\t10\tMAP\t0.08438\t0.16902\t0.10945\n",
      "5\t1.0\t10\tP@1\t0.13907\t0.28668\t0.18279\n",
      "5\t1.0\t10\tP@5\t0.08037\t0.15899\t0.10366\n",
      "5\t1.0\t10\tP@10\t0.07639\t0.15319\t0.09914\n",
      "1\t0.0\t10\tMRR\t0.17925\t0.38111\t0.23903\n",
      "1\t0.0\t10\tMAP\t0.08344\t0.19170\t0.11549\n",
      "1\t0.0\t10\tP@1\t0.14286\t0.30926\t0.19213\n",
      "1\t0.0\t10\tP@5\t0.08004\t0.18262\t0.11041\n",
      "1\t0.0\t10\tP@10\t0.07538\t0.17624\t0.10524\n",
      "1\t0.1\t10\tMRR\t0.16909\t0.36235\t0.22631\n",
      "1\t0.1\t10\tMAP\t0.07928\t0.17291\t0.10701\n",
      "1\t0.1\t10\tP@1\t0.13813\t0.30926\t0.18879\n",
      "1\t0.1\t10\tP@5\t0.07487\t0.16151\t0.10052\n",
      "1\t0.1\t10\tP@10\t0.07121\t0.15692\t0.09659\n",
      "1\t1.0\t10\tMRR\t0.15989\t0.40200\t0.23154\n",
      "1\t1.0\t10\tMAP\t0.07203\t0.18562\t0.10565\n",
      "1\t1.0\t10\tP@1\t0.12867\t0.35214\t0.19480\n",
      "1\t1.0\t10\tP@5\t0.06783\t0.17144\t0.09850\n",
      "1\t1.0\t10\tP@10\t0.06409\t0.16626\t0.09433\n"
     ]
    }
   ],
   "source": [
    "for k, v in final_scores.items():    \n",
    "    cl_size = k[0]\n",
    "    lam = k[1]\n",
    "    neg = k[2]\n",
    "    for k2, v2 in v.items():        \n",
    "        print (\"%d\\t%0.1f\\t%d\\t%s\\t%0.5f\\t%0.5f\\t%0.5f\" \n",
    "               % (cl_size, lam, neg, k2, v2[0], v2[1], v2[2]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in final_scores.items():    \n",
    "    cl_size = k[0]\n",
    "    lam = k[1]\n",
    "    neg = k[2]\n",
    "    for k2, v2 in v.items():        \n",
    "        print (\"%d\\t%0.1f\\t%d\\t%s\\t%0.5f\\t%0.5f\\t%0.5f\\t%0.5f\\t%0.5f\\t%0.5f\\t%0.5f\\t%0.5f\\t%0.5f\" \n",
    "               % (cl_size, lam, neg, k2, v2[0], v2[1], v2[2], v2[3], v2[4], v2[5], v2[6], v2[7], v2[8]) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_model.load_model()\n",
    "#weights = np.load(hyp_model.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we score as per the usual way the predictions\n",
    "predictions = {}\n",
    "for w in ['Concept', 'Entity', 'Both']:\n",
    "    print (\"Doing %s\" % (w))\n",
    "    predictions[w] = hyp_model.evaluator.predict(data.test_data_token[w])        \n",
    "    test_tuples = data.token_to_words(data.test_data_token[w])\n",
    "    scorer = semeval_eval.HypernymEvaluation(test_tuples)\n",
    "    # get scores\n",
    "    score_names, all_scores = scorer.get_evaluation_scores(predictions[w])\n",
    "    # initialise scores (MRR, MAP, ...)\n",
    "    scores = {s:0.0 for s in score_names }\n",
    "    for k in range(len(score_names)):    \n",
    "        scores[score_names[k]] = float('%.5f' % (sum([score_list[k] for score_list in all_scores]) / len(all_scores)))    \n",
    "    \n",
    "    print (scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "ax = sns.regplot(x=\"Median Freq\", y=\"MAP\", data=score_freq, x_jitter=0.02, y_jitter=0.01);\n",
    "ax.set(ylim=(-0.05, 1.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DUAL Model Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_dual(dual_model, train_split, valid_split, test_split):    \n",
    "    \n",
    "    # fit model\n",
    "    # the test split is only used to measure the test loss\n",
    "    dual_model.fit(train_split, valid_split)    \n",
    "    # this step should not be required since the model is dynamically linked to the evaluator\n",
    "    dual_model.evaluator.set_model(dual_model.feature_extractor, dual_model.concept_model, dual_model.entity_model)    \n",
    "    \n",
    "    scores_all = []\n",
    "    # evaluate trained model on word in either category separately and together\n",
    "    for w in ['Concept', 'Entity', 'Both']:\n",
    "        print (\"Evaluating model on %s\" % (w) )\n",
    "        # generates predictions according to trained model\n",
    "        predictions = dual_model.evaluator.predict(test_split[w])\n",
    "        # this converts the tokens back to words for evaluation\n",
    "        test_tuples = data.token_to_words(test_split[w])\n",
    "        # here we have a scorer that will mark our effort according to this particular test split\n",
    "        scorer = semeval_eval.HypernymEvaluation(test_tuples)\n",
    "        # get scores\n",
    "        score_names, all_scores = scorer.get_evaluation_scores(predictions)\n",
    "        # initialise scores (MRR, MAP, ...)\n",
    "        scores = {s:0.0 for s in score_names }\n",
    "        for k in range(len(score_names)):    \n",
    "            scores[score_names[k]] = float('%.5f' % (sum([score_list[k] for score_list in all_scores]) / len(all_scores)))    \n",
    "        \n",
    "        scores_all.append(scores)                    \n",
    "    return scores_all\n",
    "\n",
    "def evaluate_only_dual(dual_model, test_split):\n",
    "    predictions = dual_model.evaluator.predict(test_split)\n",
    "    # this converts the tokens back to words for evaluation\n",
    "    test_tuples = data.token_to_words(test_split)\n",
    "    # here we have a scorer that will mark our effort according to this particular test split\n",
    "    scorer = semeval_eval.HypernymEvaluation(test_tuples)\n",
    "    # get scores\n",
    "    score_names, all_scores = scorer.get_evaluation_scores(predictions)\n",
    "    # initialise scores (MRR, MAP, ...)\n",
    "    scores = {s:0.0 for s in score_names }\n",
    "    for k in range(len(score_names)):    \n",
    "        scores[score_names[k]] = float('%.5f' % (sum([score_list[k] for score_list in all_scores]) / len(all_scores)))    \n",
    "\n",
    "    return predictions, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "embeddings_layer = multiprojection_model.get_embeddings_model(data.embeddings_matrix, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# standard model parameters - we won't be changing these\n",
    "args['data']              = data\n",
    "args['embeddings_layer']  = embeddings_layer\n",
    "args['epochs']            = 20\n",
    "args['batch_size']        = 64\n",
    "args['synonym_sample_n']  = 1\n",
    "args['phi_k']             = 1\n",
    "args['lambda_c']          = 0.\n",
    "args['negative_sample_n'] = 10\n",
    "args['save_path']         = 'dual_ft_semeval.npz'\n",
    "args['patience']          = 5\n",
    "args['eval_after_epoch']  = True\n",
    "args['lr']                = 0.001\n",
    "args['beta1']             = 0.9\n",
    "args['beta2']             = 0.9\n",
    "args['clip_value']        = 1.\n",
    "\n",
    "\n",
    "dual_model = multiprojection_dual.MultiProjModelDual(args)\n",
    "\n",
    "# generate parameter combinations\n",
    "#_clusters = [10, 5, 1]\n",
    "_clusters = [1, 5, 10]\n",
    "#_lambda_c = [0, 0.1, 1]\n",
    "_lambda_c = [0.]\n",
    "#_neg_count = [10, 5, 1]\n",
    "_neg_count = [10]\n",
    "\n",
    "parameters = [_clusters, _lambda_c, _neg_count]\n",
    "\n",
    "param_list = list(product(*parameters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with following parameters: phi_k: 1; lambda_c: 0.00; neg_count: 10\n",
      "Training model on Both\n",
      "Fitting model with following parameters:\n",
      " batch_size=64;\n",
      " phi_k=1;\n",
      " lambda_c=0.00;\n",
      " epochs=20;\n",
      " negative_count=10;\n",
      " synonym_count=1\n",
      "Optimizer parameters:\n",
      " lr=0.00100;\n",
      " beta1=0.900;\n",
      " beta2=0.900;\n",
      " clip=1.00\n",
      "--------------------\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch: 1; Training Loss: 0.65261; Test Loss: 0.00000; Test MAP: 0.00000; Test MRR: 0.00000\n",
      "Epoch: 2; Training Loss: 0.57333; Test Loss: 0.00000; Test MAP: 0.00000; Test MRR: 0.00000\n",
      "Epoch: 3; Training Loss: 0.50473; Test Loss: 0.00000; Test MAP: 0.00000; Test MRR: 0.00000\n",
      "Epoch: 4; Training Loss: 0.44798; Test Loss: 0.00000; Test MAP: 0.00000; Test MRR: 0.00000\n",
      "Epoch: 5; Training Loss: 0.40237; Test Loss: 0.00000; Test MAP: 0.00000; Test MRR: 0.00000\n",
      "Epoch: 6; Training Loss: 0.36658; Test Loss: 0.00000; Test MAP: 0.00000; Test MRR: 0.00000\n",
      "Epoch: 7; Training Loss: 0.33891; Test Loss: 0.00000; Test MAP: 0.00000; Test MRR: 0.00000\n",
      "Epoch: 8; Training Loss: 0.31823; Test Loss: 0.00000; Test MAP: 0.00393; Test MRR: 0.01000\n",
      "Epoch: 9; Training Loss: 0.30299; Test Loss: 0.00000; Test MAP: 0.01816; Test MRR: 0.05000\n",
      "Epoch: 10; Training Loss: 0.29143; Test Loss: 0.00000; Test MAP: 0.02585; Test MRR: 0.06622\n",
      "Epoch: 11; Training Loss: 0.28318; Test Loss: 0.00000; Test MAP: 0.03897; Test MRR: 0.07000\n",
      "Epoch: 12; Training Loss: 0.27729; Test Loss: 0.00000; Test MAP: 0.02926; Test MRR: 0.10000\n",
      "Epoch: 13; Training Loss: 0.27278; Test Loss: 0.00000; Test MAP: 0.04993; Test MRR: 0.11500\n",
      "Epoch: 14; Training Loss: 0.26960; Test Loss: 0.00000; Test MAP: 0.03793; Test MRR: 0.11000\n",
      "Epoch: 15; Training Loss: 0.26653; Test Loss: 0.00000; Test MAP: 0.03343; Test MRR: 0.08917\n",
      "Epoch: 16; Training Loss: 0.26388; Test Loss: 0.00000; Test MAP: 0.06408; Test MRR: 0.14000\n",
      "Epoch: 17; Training Loss: 0.26121; Test Loss: 0.00000; Test MAP: 0.04348; Test MRR: 0.09556\n",
      "Epoch: 18; Training Loss: 0.25851; Test Loss: 0.00000; Test MAP: 0.04637; Test MRR: 0.11167\n",
      "Epoch: 19; Training Loss: 0.25647; Test Loss: 0.00000; Test MAP: 0.05727; Test MRR: 0.13867\n",
      "Epoch: 20; Training Loss: 0.25367; Test Loss: 0.00000; Test MAP: 0.04527; Test MRR: 0.09867\n",
      "Load best model\n",
      "Done!\n",
      "Evaluating model on Concept\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Evaluating model on Entity\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Evaluating model on Both\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Done 1100\n",
      "Done 1200\n",
      "Done 1300\n",
      "Done 1400\n",
      "\n",
      "Finished 1 from 3 experiments\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 5; lambda_c: 0.00; neg_count: 10\n",
      "Training model on Both\n",
      "Fitting model with following parameters:\n",
      " batch_size=64;\n",
      " phi_k=5;\n",
      " lambda_c=0.00;\n",
      " epochs=20;\n",
      " negative_count=10;\n",
      " synonym_count=1\n",
      "Optimizer parameters:\n",
      " lr=0.00100;\n",
      " beta1=0.900;\n",
      " beta2=0.900;\n",
      " clip=1.00\n",
      "--------------------\n",
      "Epoch: 1; Training Loss: 0.60739; Test Loss: 0.00000; Test MAP: 0.00000; Test MRR: 0.00000\n",
      "Epoch: 2; Training Loss: 0.46425; Test Loss: 0.00000; Test MAP: 0.00000; Test MRR: 0.00000\n",
      "Epoch: 3; Training Loss: 0.36918; Test Loss: 0.00000; Test MAP: 0.00000; Test MRR: 0.00000\n",
      "Epoch: 4; Training Loss: 0.31038; Test Loss: 0.00000; Test MAP: 0.01570; Test MRR: 0.03000\n",
      "Epoch: 5; Training Loss: 0.27307; Test Loss: 0.00000; Test MAP: 0.06556; Test MRR: 0.10467\n",
      "Epoch: 6; Training Loss: 0.24614; Test Loss: 0.00000; Test MAP: 0.07960; Test MRR: 0.16750\n",
      "Epoch: 7; Training Loss: 0.22641; Test Loss: 0.00000; Test MAP: 0.08944; Test MRR: 0.17067\n",
      "Epoch: 8; Training Loss: 0.20939; Test Loss: 0.00000; Test MAP: 0.08779; Test MRR: 0.17483\n",
      "Epoch: 9; Training Loss: 0.19600; Test Loss: 0.00000; Test MAP: 0.09708; Test MRR: 0.18567\n",
      "Epoch: 10; Training Loss: 0.18374; Test Loss: 0.00000; Test MAP: 0.11241; Test MRR: 0.20733\n",
      "Epoch: 11; Training Loss: 0.17321; Test Loss: 0.00000; Test MAP: 0.09082; Test MRR: 0.17200\n",
      "Epoch: 12; Training Loss: 0.16333; Test Loss: 0.00000; Test MAP: 0.09406; Test MRR: 0.16883\n",
      "Epoch: 13; Training Loss: 0.15570; Test Loss: 0.00000; Test MAP: 0.11681; Test MRR: 0.21286\n",
      "Epoch: 14; Training Loss: 0.14859; Test Loss: 0.00000; Test MAP: 0.10981; Test MRR: 0.19167\n",
      "Epoch: 15; Training Loss: 0.14258; Test Loss: 0.00000; Test MAP: 0.11059; Test MRR: 0.19333\n",
      "Epoch: 16; Training Loss: 0.13647; Test Loss: 0.00000; Test MAP: 0.10255; Test MRR: 0.18133\n",
      "Epoch: 17; Training Loss: 0.13038; Test Loss: 0.00000; Test MAP: 0.10871; Test MRR: 0.19186\n",
      "Epoch: 18; Training Loss: 0.12660; Test Loss: 0.00000; Test MAP: 0.10079; Test MRR: 0.18300\n",
      "Early Stop invoked at epoch 18\n",
      "Load best model\n",
      "Done!\n",
      "Evaluating model on Concept\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Evaluating model on Entity\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Evaluating model on Both\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Done 1100\n",
      "Done 1200\n",
      "Done 1300\n",
      "Done 1400\n",
      "\n",
      "Finished 2 from 3 experiments\n",
      "------------------------------\n",
      "Running test with following parameters: phi_k: 10; lambda_c: 0.00; neg_count: 10\n",
      "Training model on Both\n",
      "Fitting model with following parameters:\n",
      " batch_size=64;\n",
      " phi_k=10;\n",
      " lambda_c=0.00;\n",
      " epochs=20;\n",
      " negative_count=10;\n",
      " synonym_count=1\n",
      "Optimizer parameters:\n",
      " lr=0.00100;\n",
      " beta1=0.900;\n",
      " beta2=0.900;\n",
      " clip=1.00\n",
      "--------------------\n",
      "Epoch: 1; Training Loss: 0.56594; Test Loss: 0.00000; Test MAP: 0.00000; Test MRR: 0.00000\n",
      "Epoch: 2; Training Loss: 0.39482; Test Loss: 0.00000; Test MAP: 0.00000; Test MRR: 0.00000\n",
      "Epoch: 3; Training Loss: 0.30418; Test Loss: 0.00000; Test MAP: 0.00664; Test MRR: 0.01333\n",
      "Epoch: 4; Training Loss: 0.25339; Test Loss: 0.00000; Test MAP: 0.04613; Test MRR: 0.10089\n",
      "Epoch: 5; Training Loss: 0.21925; Test Loss: 0.00000; Test MAP: 0.05294; Test MRR: 0.10389\n",
      "Epoch: 6; Training Loss: 0.19329; Test Loss: 0.00000; Test MAP: 0.06682; Test MRR: 0.15200\n",
      "Epoch: 7; Training Loss: 0.17339; Test Loss: 0.00000; Test MAP: 0.06363; Test MRR: 0.14488\n",
      "Epoch: 8; Training Loss: 0.15600; Test Loss: 0.00000; Test MAP: 0.08218; Test MRR: 0.19752\n",
      "Epoch: 9; Training Loss: 0.14313; Test Loss: 0.00000; Test MAP: 0.07471; Test MRR: 0.18019\n",
      "Epoch: 10; Training Loss: 0.13120; Test Loss: 0.00000; Test MAP: 0.08221; Test MRR: 0.19686\n",
      "Epoch: 11; Training Loss: 0.12210; Test Loss: 0.00000; Test MAP: 0.10808; Test MRR: 0.21400\n",
      "Epoch: 12; Training Loss: 0.11424; Test Loss: 0.00000; Test MAP: 0.08162; Test MRR: 0.18667\n",
      "Epoch: 13; Training Loss: 0.10736; Test Loss: 0.00000; Test MAP: 0.09558; Test MRR: 0.21967\n",
      "Epoch: 14; Training Loss: 0.10175; Test Loss: 0.00000; Test MAP: 0.08928; Test MRR: 0.22400\n",
      "Epoch: 15; Training Loss: 0.09616; Test Loss: 0.00000; Test MAP: 0.08712; Test MRR: 0.22067\n",
      "Epoch: 16; Training Loss: 0.09171; Test Loss: 0.00000; Test MAP: 0.08889; Test MRR: 0.20500\n",
      "Early Stop invoked at epoch 16\n",
      "Load best model\n",
      "Done!\n",
      "Evaluating model on Concept\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Evaluating model on Entity\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Evaluating model on Both\n",
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Done 1100\n",
      "Done 1200\n",
      "Done 1300\n",
      "Done 1400\n",
      "\n",
      "Finished 3 from 3 experiments\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# initialise final_scores dictionary\n",
    "final_scores = {k:defaultdict(list) for k in param_list}\n",
    "\n",
    "for idx2, _param in enumerate(param_list):\n",
    "    print (\"Running test with following parameters: phi_k: %d; lambda_c: %0.2f; neg_count: %d\" \\\n",
    "           % (_param[0], _param[1], _param[2]))\n",
    "\n",
    "    args['phi_k'] = _param[0]\n",
    "    args['lambda_c'] = _param[1]\n",
    "    args['negative_sample_n'] = _param[2]    \n",
    "\n",
    "    for w in ['Both']:    \n",
    "        # train model on three sets of data\n",
    "        print (\"Training model on %s\" % (w))\n",
    "        args['save_path'] = 'dual_ft_semeval_%s.npz' % (w)\n",
    "        dual_model.reset_model(args=args)\n",
    "\n",
    "        all_scores = train_and_evaluate_dual(dual_model, \n",
    "                                             data.train_data_token[w], \n",
    "                                             data.valid_data_token[w],\n",
    "                                             data.test_data_token)\n",
    "        # run predictions on test\n",
    "\n",
    "        for scores in all_scores:\n",
    "            for s, v  in scores.items():\n",
    "                final_scores[_param][s].append(v)\n",
    "\n",
    "    print (\"\")\n",
    "    print (\"Finished %d from %d experiments\" % (idx2+1, len(param_list)))\n",
    "    print (\"-\"*30)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t0.0\t10\tMRR\t0.10734\t0.21708\t0.13984\n",
      "1\t0.0\t10\tMAP\t0.04743\t0.10768\t0.06527\n",
      "1\t0.0\t10\tP@1\t0.08515\t0.17381\t0.11141\n",
      "1\t0.0\t10\tP@5\t0.04508\t0.10260\t0.06211\n",
      "1\t0.0\t10\tP@10\t0.04245\t0.09854\t0.05905\n",
      "5\t0.0\t10\tMRR\t0.19376\t0.40343\t0.25585\n",
      "5\t0.0\t10\tMAP\t0.09394\t0.20040\t0.12546\n",
      "5\t0.0\t10\tP@1\t0.14664\t0.32506\t0.19947\n",
      "5\t0.0\t10\tP@5\t0.09010\t0.18758\t0.11897\n",
      "5\t0.0\t10\tP@10\t0.08608\t0.18370\t0.11499\n",
      "10\t0.0\t10\tMRR\t0.21118\t0.38461\t0.26257\n",
      "10\t0.0\t10\tMAP\t0.09865\t0.19040\t0.12581\n",
      "10\t0.0\t10\tP@1\t0.16367\t0.32054\t0.21014\n",
      "10\t0.0\t10\tP@5\t0.09314\t0.18025\t0.11895\n",
      "10\t0.0\t10\tP@10\t0.08905\t0.17245\t0.11374\n"
     ]
    }
   ],
   "source": [
    "for k, v in final_scores.items():    \n",
    "    cl_size = k[0]\n",
    "    lam = k[1]\n",
    "    neg = k[2]\n",
    "    for k2, v2 in v.items():        \n",
    "        print (\"%d\\t%0.1f\\t%d\\t%s\\t%0.5f\\t%0.5f\\t%0.5f\" \n",
    "               % (cl_size, lam, neg, k2, v2[0], v2[1], v2[2]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Prediction quality vs hypernym freq in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform freq analysis of all hypernyms in training set; query terms will not features in test but test terms\n",
    "# will certainly be related to hypernyms found in training set.\n",
    "def get_hypernym_freq(dataset):\n",
    "    all_hypernyms = Counter([y for x, y in data.token_to_words(dataset)])\n",
    "    cnt_distinct_hyper = sum(all_hypernyms.values())\n",
    "    #sorted([(y, x) for x, y in all_hypernyms.items()], reverse=True)\n",
    "    hyper_freq = {w:round((cnt/cnt_distinct_hyper), 5) for w, cnt in all_hypernyms.items()}\n",
    "    return hyper_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(list(hyper_freq.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions = yummy.evaluator.predict(test_data_split[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we score as per the usual way the predictions\n",
    "test_tuples = data.token_to_words(test_data_split[4])\n",
    "scorer = semeval_eval.HypernymEvaluation(test_tuples)\n",
    "# get scores\n",
    "score_names, all_scores = scorer.get_evaluation_scores(predictions)\n",
    "# initialise scores (MRR, MAP, ...)\n",
    "scores = {s:0.0 for s in score_names }\n",
    "for k in range(len(score_names)):    \n",
    "    scores[score_names[k]] = float('%.5f' % (sum([score_list[k] for score_list in all_scores]) / len(all_scores)))    \n",
    "    \n",
    "# we create a dictionary of ground truth hypernyms for the test split of interest    \n",
    "ground_truth = defaultdict(list)\n",
    "for x, y in test_tuples:\n",
    "    ground_truth[x].append(y)\n",
    "ground_truth.default_factory = None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all MAP scores\n",
    "list(predictions.items())[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_freq_matrix(test_data, predictions, hyper_freq, jitter=False):\n",
    "    # we score as per the usual way the predictions\n",
    "    test_tuples = data.token_to_words(test_data)\n",
    "    scorer = semeval_eval.HypernymEvaluation(test_tuples)\n",
    "    # get scores\n",
    "    score_names, all_scores = scorer.get_evaluation_scores(predictions)\n",
    "    # initialise scores (MRR, MAP, ...)\n",
    "    scores = {s:0.0 for s in score_names }\n",
    "    for k in range(len(score_names)):    \n",
    "        scores[score_names[k]] = float('%.5f' % (sum([score_list[k] for score_list in all_scores]) / len(all_scores)))    \n",
    "\n",
    "    # we create a dictionary of ground truth hypernyms for the test split of interest    \n",
    "    ground_truth = defaultdict(list)\n",
    "    for x, y in test_tuples:\n",
    "        ground_truth[x].append(y)\n",
    "    ground_truth.default_factory = None    \n",
    "\n",
    "\n",
    "    # retain MAP scores only from score list of lists\n",
    "    all_map = np.round(np.asarray(all_scores)[:,1], 3)\n",
    "\n",
    "    # iterate over every query term in test set; create dataset with AP score for word, median freq of ground\n",
    "    # truth hypernym, based on appearance in training set.  Hypernyms that did not appear at all, will be assigned\n",
    "    # freq of 0.\n",
    "    score_freq_matrix = np.zeros((all_map.shape[0], 2))\n",
    "    # sort prediction keys explicityly to make sure we process in the same order processed by evaluator\n",
    "    for idx, w in enumerate(sorted(predictions.keys())):\n",
    "        # find freq of predictions\n",
    "        score_freq_matrix[idx][0] = all_map[idx]\n",
    "        gold = ground_truth[w]\n",
    "        score_freq_matrix[idx][1] = np.median(([hyper_freq[g] if g in hyper_freq else 0. for g in gold]))\n",
    "        \n",
    "    if jitter:\n",
    "        # add some jitter to the signal to make it easier to interpret in the scatterplot\n",
    "        mu, sigma = 0, 0.01 \n",
    "        # creating a noise with the same dimension as the dataset (2,2) \n",
    "        noise = np.random.normal(mu, sigma, (all_map.shape[0], 2) )\n",
    "        score_freq_matrix =  score_freq_matrix + noise\n",
    "\n",
    "    score_freq = pd.DataFrame(score_freq_matrix, columns=['AP', 'Median Freq'])\n",
    "    # add query word to data frame\n",
    "    score_freq = score_freq.assign(word=pd.Series(list(predictions.keys())).values)\n",
    "    return score_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score_freq.loc[score_freq.MAP == 1.,].sort_values('Median Freq', ascending=False)\n",
    "\n",
    "# prediction contains the generated hypernyms for YAMANE on the 5th fold of the training_data\n",
    "score_freq_yam=get_score_freq_matrix(test_data_split[4], predictions, hyper_freq)\n",
    "\n",
    "# prediction_2 contains the generated hypernyms for CRIM on the 5th fold of the training data\n",
    "score_freq=get_score_freq_matrix(test_data_split[4], predictions_2, hyper_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.relplot(x=\"Median Freq\", y=\"MAP\", data=score_freq);\n",
    "plt.figure(figsize=(12,8))\n",
    "ax = sns.regplot(x=\"Median Freq\", y=\"AP\", data=score_freq_yam, x_jitter=0.01, y_jitter=0.005, marker=\"x\");\n",
    "ax.set(ylim=(-0.05, 1.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "ax = sns.regplot(x=\"Median Freq\", y=\"AP\", data=score_freq, x_jitter=0.01, y_jitter=0.005);\n",
    "ax.set(ylim=(-0.05, 1.05))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_yummy_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze top-ranked word for term words scoring 0 MAP\n",
    "yummy_wrong_terms = score_freq_yam.loc[score_freq_yam.MAP==0, 'word'].tolist()\n",
    "crim_wrong_terms  = score_freq.loc[score_freq.MAP==0, 'word'].tolist()\n",
    "\n",
    "top_crim_wrong = []\n",
    "top_yummy_wrong = []\n",
    "\n",
    "for w in crim_wrong_terms:\n",
    "    top_crim_wrong.append(predictions_2[w][0])\n",
    "    \n",
    "for w in yummy_wrong_terms:\n",
    "    top_yummy_wrong.append(predictions[w][0])    \n",
    "    \n",
    "top_crim_wrong =  sorted([(v,k) for k, v in Counter(top_crim_wrong).items()], reverse=True)[:15]\n",
    "top_yummy_wrong =  sorted([(v,k) for k, v in Counter(top_yummy_wrong).items()], reverse=True)[:15]\n",
    "\n",
    "# multiply words according to frequency\n",
    "yummy_wrong_flat = [li for lol in list(map(lambda w: [w[1]] * w[0], top_yummy_wrong)) for li in lol  ]\n",
    "crim_wrong_flat = [li for lol in list(map(lambda w: [w[1]] * w[0], top_crim_wrong)) for li in lol  ]\n",
    "\n",
    "combined_wrong_list = list(zip(['CRIM'] * len(crim_wrong_flat), crim_wrong_flat))\n",
    "combined_wrong_list.extend(list(zip(['Yamane'] * len(yummy_wrong_flat), yummy_wrong_flat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_df =  pd.DataFrame(combined_wrong_list, columns=['Model', 'Highest Ranked Incorrect Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_df.loc[incorrect_df.Model == 'CRIM',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "g = sns.countplot(x=\"Highest Ranked Incorrect Word\", \n",
    "                  palette=sns.cubehelix_palette(15, start=2, rot=0.35, dark=0.47, light=0.85, reverse=True), \n",
    "                  data=incorrect_df.loc[incorrect_df.Model == 'CRIM',])\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in sorted([(v, k)for k, v in hyper_freq.items()], reverse=True)[:10]:\n",
    "    print (w[1], w[0])\n",
    "    \n",
    "hyper_freq['action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "g = sns.countplot(x=\"Highest Ranked Incorrect Word\", \n",
    "                  palette=sns.cubehelix_palette(15, start=2, rot=0.35, dark=0.47, light=0.85, reverse=True), \n",
    "                  data=incorrect_df.loc[incorrect_df.Model == 'Yamane',])\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for w in sorted([(v, k)for k, v in hyper_freq.items()], reverse=True)[:10]:\n",
    "    print (w[1], w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# least frequent hypernyms\n",
    "for w in [(v, k) for k, v in hyper_freq.items() if v == min(hyper_freq.values())][:10]:\n",
    "    print (w[1], w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see good scores low frequency\n",
    "good_words = score_freq.loc[(score_freq.AP >= 1.0) & (score_freq['Median Freq'] < 0.005) , 'word'].tolist()\n",
    "score_freq.loc[(score_freq.AP >= 1.0) & (score_freq['Median Freq'] < 0.005) , ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for w in good_words:\n",
    "    print (\"%s: %s\" % (w, \", \".join(predictions_2[w])))\n",
    "    print (\"%s: %s\" % (w, \", \".join(hyper_dict[w])))\n",
    "    print (\"-\"*30)\n",
    "    \n",
    "#print (hyper_dict['intercourse'])\n",
    "#predictions_2['intercourse']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(semeval_data)\n",
    "reload(multiprojection_model)\n",
    "reload(crim_evaluator)\n",
    "reload(multiprojection_dual)\n",
    "reload(crim_dual_evaluator)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Concept\n",
    "Tuples in validation set: 112; tuples in test set: 4936; tuples in training: 7184\n",
    "Unique hyponyms in validation set: 30; hyponyms in test set: 1057; hyponyms in training: 979\n",
    "------------------------------\n",
    "Entity\n",
    "Tuples in validation set: 88; tuples in test set: 2112; tuples in training: 4595\n",
    "Unique hyponyms in validation set: 20; hyponyms in test set: 443; hyponyms in training: 521\n",
    "------------------------------\n",
    "Both\n",
    "Tuples in validation set: 200; tuples in test set: 7048; tuples in training: 11779\n",
    "Unique hyponyms in validation set: 50; hyponyms in test set: 1499; hyponyms in training: 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(read_vocab(vocab_file_name)).difference(set(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove vocab term having no vector in embeddings\n",
    "def get_terms_having_vectors(w2v, dataset):\n",
    "    return [(q,h) for q, h in dataset if q in w2v and h in w2v]\n",
    "\n",
    "# remove any words which don't have corresponding embeddings \n",
    "for w in word_types:\n",
    "    validation[w] = get_terms_having_vectors(model, validation[w])\n",
    "    test[w] = get_terms_having_vectors(model, test[w])\n",
    "    training[w] = get_terms_having_vectors(model, training[w])\n",
    "\n",
    "vocabulary = list(filter(lambda w: w in model, vocabulary))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
